{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第6章 Function Calling与Agent 智能体"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💡 学习目标\n",
    "\n",
    "1. 理解Function Calling的概念\n",
    "2. 理解Function Calling的工作原理\n",
    "3. 实战使用OpenAI提供的Function Calling接口（基础请求及优化）\n",
    "4. 探讨自定义Function的提供的可能性\n",
    "5. 探讨Function Calling在大模型应用场景中带来的“质变”\n",
    "6. 智能体/LLM应用的定义与作用\n",
    "   - 什么是智能体？智能体与大模型的关系是什么样的\n",
    "   - 智能体概念的演进过程，基本架构与功能\n",
    "7. 智能体开发框架smolagents\n",
    "8. Agentic RAG 实战\n",
    "9. 智能体的应用场景\n",
    "   - 探讨智能体在行业场景中的落地情况 \n",
    "   - 探讨智能体系统/LLM应用的常见分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Function Calling的概念\n",
    "\n",
    "Function Calling（函数调用），顾名思义，为模型提供了一种调用函数的方法/能力。\n",
    "\n",
    "- Function Calling成立的模型能力基础：\n",
    "    - 问题理解和行动规划\n",
    "    - 结构化数据输出\n",
    "    - 上下文学习 In-Context Learning\n",
    "\n",
    "Function Calling让模型输出不再局限于自身推理输出，而是可以与外部系统交互，完成更复杂的任务\n",
    "\n",
    "- 常见Function Calling应用场景包括：\n",
    "    - 查询检索，补充额外信息（如RAG、搜索）\n",
    "    - 理解用户输入，向外部系统写入信息（如表单填写）\n",
    "    - 调用外部系统能力，完成实际行为动作（如下订单）\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Function Calling的工作原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 OpenAI官方定义\n",
    "\n",
    "OpenAI官方说明文档：https://platform.openai.com/docs/guides/function-calling\n",
    "\n",
    "<img src=\"./assets/function-calling-diagram-steps.png\" width=\"800px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 描述Function Calling的另一个流程图\n",
    "\n",
    "<img src=\"./assets/function_calling.drawio.png\" width=\"721px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Calling是结果，理解和选择才是第一步\n",
    "\n",
    "- 除了代表用户诉求的Prompt之外，Function Calling还需要将可用的工具信息（Function Definitions）也提供给模型\n",
    "- 在第一次请求时，模型的核心工作如下：\n",
    "    1. 理解Prompt所代表的“诉求”和Definitions所代表的“行动可能性”\n",
    "    2. “选择”完成“诉求”所需要进行的“行动”（从“行动可能性”中获得）\n",
    "    3. 根据所选择的“行动”，给出执行“行动”所需的“行动参数”（Parameters）\n",
    "- 那么想一想：\n",
    "    1. 什么影响“选择”的效果？\n",
    "    2. 什么影响“行动”的可执行性和效果？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 作为可选项的结果回调和最终回复输出\n",
    "\n",
    "- 在对话流中，将Function Calling的结果（Function Result）与初始的Prompt诉求再次组合，提供给模型以获得最终的回复输出，是常见的流程（RAG就是一个典型的例子）\n",
    "- 但如果我们将Function Calling用于非对话流场景，最终回复输出就不一定是必选项了，例如：\n",
    "    1. 【只需要完成Calling动作】我们只是希望通过Function Calling完成行动选择和发起，接下来就进入业务处理流程，例如：理解用户表达并代替用户下单\n",
    "    2. 【只需要完成行动参数Parameters生成】我们只是希望将Function Calling做好工具使用决策，并完成部分请求参数的生成，接下来需要走业务流程补全其他参数（比如鉴权信息），例如：敏感数据查询\n",
    "\n",
    "在实际生产中，不给出最终回复输出，而只是使用Function Calling返回的调用方法数据，是很常见的用法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 实际调用Function Calling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从官方案例开始"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 第一步：工具决策和调用信息生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function(arguments='{\"latitude\":31.2304,\"longitude\":121.4737}', name='get_weather', parameters=None)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    base_url=os.getenv(\"OPENAI_BASE_URL\"),\n",
    ")\n",
    "\n",
    "# 给出工具定义\n",
    "tools = [\n",
    "    # 每一个列表元素项，就是一个工具定义\n",
    "    {\n",
    "        # 类型标注（固定格式）\n",
    "        \"type\": \"function\",\n",
    "        # 函数定义\n",
    "        \"function\": {\n",
    "            # 函数名称（帮助我们去查找本地的函数在哪里，函数映射ID）\n",
    "            \"name\": \"get_weather\",\n",
    "            # 函数描述（帮助模型去理解函数的作用，适用场景，可以理解为Prompt的一部分）\n",
    "            \"description\": \"Get current temperature for provided coordinates in celsius.\",\n",
    "            # 函数依赖参数的定义（帮助模型去理解如果要做参数生成，应该怎么生成）\n",
    "            \"parameters\": {\n",
    "                # 参数形式\n",
    "                \"type\": \"object\", # 对应输出JSON String\n",
    "                # 参数结构\n",
    "                \"properties\": {\n",
    "                    # 参数名，参数类型\n",
    "                    \"latitude\": {\"type\": \"number\"},\n",
    "                    # 参数名，参数类型\n",
    "                    \"longitude\": {\"type\": \"number\"}\n",
    "                },\n",
    "                # 必须保证生成的参数列表（每个元素对应上面properties的参数名）\n",
    "                \"required\": [\"latitude\", \"longitude\"],\n",
    "                \"additionalProperties\": False\n",
    "            },\n",
    "            # 格式是否严格（默认为True）\n",
    "            \"strict\": True\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# 给出诉求表达\n",
    "messages = [{\"role\": \"user\", \"content\": \"What's the weather like in Shanghai today?\"}]\n",
    "#messages = [{\"role\": \"user\", \"content\": \"How are you today?\"}]\n",
    "#messages = [{\"role\": \"user\", \"content\": \"请告诉我北京的经纬度\"}]\n",
    "#messages = [{\"role\": \"user\", \"content\": \"What's the weather like today?\"}]\n",
    "\n",
    "# 发起请求\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=messages,\n",
    "    # 把工具定义提交给模型，就已经默认启用了Function Calling\n",
    "    tools=tools,\n",
    ")\n",
    "# print(completion.choices[0].message)\n",
    "# print(completion.choices[0].message.tool_calls[0].function)\n",
    "if completion.choices[0].message.tool_calls:\n",
    "    print(completion.choices[0].message.tool_calls[0].function)\n",
    "else:\n",
    "    print(\"No function is called.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 情况1：问与工具的无关的问题会发生什么？\n",
    "    - `messages = [{\"role\": \"user\", \"content\": \"How are you today?\"}]`\n",
    "- 情况2：问无法获得确切行动参数的问题会发生什么？\n",
    "    - `messages = [{\"role\": \"user\", \"content\": \"What's the weather like today?\"}]`\n",
    "\n",
    "- 如何容错：\n",
    "\n",
    "    ```python\n",
    "    if completion.choices[0].message.tool_calls:\n",
    "        print(completion.choices[0].message.tool_calls[0].function)\n",
    "    else:\n",
    "        print(\"No function is called.\")\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 第二步：实际调用工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call Function Name: get_weather\n",
      "Call Function Arguments: {\"latitude\":31.2304,\"longitude\":121.4737}\n"
     ]
    }
   ],
   "source": [
    "function_calling_message = completion.choices[0].message\n",
    "function_calling = completion.choices[0].message.tool_calls[0]\n",
    "\n",
    "print(\"Call Function Name:\", function_calling.function.name)\n",
    "print(\"Call Function Arguments:\", function_calling.function.arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'success: 1, 2, 3, 4'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Python基础：函数参数定义及解析\n",
    "def test(arg1, arg2, *, arg3, arg4):\n",
    "    return f\"success: {arg1}, {arg2}, {arg3}, {arg4}\"\n",
    "\n",
    "args = (1, 2) # 列表型数据展开为位置参数\n",
    "kwargs = { # 字典型数据展开为具名参数（key）\n",
    "    \"arg3\": 3,\n",
    "    \"arg4\": 4\n",
    "}\n",
    "\n",
    "test(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'temperature': 23, 'weather': 'Sunny', 'wind_direction': 'South', 'windy': 2}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def get_weather(*, latitude:float, longitude:float):\n",
    "    return {\n",
    "        \"temperature\": 23,\n",
    "        \"weather\": \"Sunny\",\n",
    "        \"wind_direction\": \"South\",\n",
    "        \"windy\": 2,\n",
    "    }\n",
    "\n",
    "functions = {\n",
    "    \"get_weather\": get_weather\n",
    "}\n",
    "\n",
    "function_result = functions[function_calling.function.name](**json.loads(function_calling.function.arguments))\n",
    "print(function_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 第三步：将结果返回给模型获取最终结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': \"What's the weather like in Shanghai today?\"}]\n"
     ]
    }
   ],
   "source": [
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': \"What's the weather like in Shanghai today?\"}, ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageFunctionToolCall(id='call_R0bSycZHkVb4wK7dcQVG5iH4', function=Function(arguments='{\"latitude\":31.2304,\"longitude\":121.4737}', name='get_weather', parameters=None), type='function')]), {'role': 'tool', 'tool_call_id': 'call_R0bSycZHkVb4wK7dcQVG5iH4', 'content': \"{'temperature': 23, 'weather': 'Sunny', 'wind_direction': 'South', 'windy': 2}\"}]\n"
     ]
    }
   ],
   "source": [
    "# 必须：让模型知道自己之前给了一个什么指令（包含tool_call_id）\n",
    "messages.append(function_calling_message)\n",
    "# 包含了tool_call_id的结果加入消息列\n",
    "messages.append({\n",
    "    \"role\": \"tool\",\n",
    "    \"tool_call_id\": function_calling.id,\n",
    "    \"content\": str(function_result),\n",
    "})\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weather in Shanghai today is sunny with a temperature of 23°C. There is a light wind blowing from the south.\n"
     ]
    }
   ],
   "source": [
    "final_result = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    ")\n",
    "print(final_result.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 如果获得错误信息会怎么样？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': \"What's the weather like in Shanghai today?\"}, ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageFunctionToolCall(id='call_R0bSycZHkVb4wK7dcQVG5iH4', function=Function(arguments='{\"latitude\":31.2304,\"longitude\":121.4737}', name='get_weather', parameters=None), type='function')]), {'role': 'tool', 'tool_call_id': 'call_R0bSycZHkVb4wK7dcQVG5iH4', 'content': \"Key 'latitude' can not be supported any more, please use 'lat' instead.\"}]\n",
      "Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageFunctionToolCall(id='call_sesSJqg1SmyQZj8mrmJ0YzfH', function=Function(arguments='{\"lat\":31.2304,\"longitude\":121.4737}', name='get_weather', parameters=None), type='function')]))\n"
     ]
    }
   ],
   "source": [
    "error_messages = messages[:1]\n",
    "error_messages.append(function_calling_message)\n",
    "error_messages.append({\n",
    "    \"role\": \"tool\",\n",
    "    \"tool_call_id\": function_calling.id,\n",
    "    \"content\": str(TypeError(\"Key 'latitude' can not be supported any more, please use 'lat' instead.\")),\n",
    "})\n",
    "print(error_messages)\n",
    "\n",
    "final_result = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=error_messages,\n",
    "    tools=tools,\n",
    ")\n",
    "print(final_result.choices[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 答：会重试，但不多..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 在实际应用场景中的一些案例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 封装基本方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import TypedDict\n",
    "from openai import OpenAI\n",
    "\n",
    "class FunctionCallingResult(TypedDict):\n",
    "    name: str\n",
    "    arguments: str\n",
    "\n",
    "class ModelRequestWithFunctionCalling:\n",
    "    def __init__(self):\n",
    "        self._client = OpenAI(\n",
    "            api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "            base_url=os.getenv(\"OPENAI_BASE_URL\"),\n",
    "        )\n",
    "        self._function_infos = {}\n",
    "        self._function_mappings = {}\n",
    "        self._messages = []\n",
    "    \n",
    "    def register_function(self, *, name, description, parameters, function, **kwargs):\n",
    "        self._function_infos.update({\n",
    "            name: {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": name,\n",
    "                    \"description\": description,\n",
    "                    \"parameters\": parameters,\n",
    "                    **kwargs\n",
    "                }\n",
    "            }\n",
    "        })\n",
    "        self._function_mappings.update({ name: function })\n",
    "        return self\n",
    "\n",
    "    def reset_messages(self):\n",
    "        self._messages = []\n",
    "        return self\n",
    "    \n",
    "    def append_message(self, role, content, **kwargs):\n",
    "        self._messages.append({ \"role\": role, \"content\": content, **kwargs })\n",
    "        print(\"[Processing Messages]:\", self._messages[-1])\n",
    "        return self\n",
    "    \n",
    "    def _call(self, function_calling_result:FunctionCallingResult):\n",
    "        function = self._function_mappings[function_calling_result.name]\n",
    "        arguments = json.loads(function_calling_result.arguments)\n",
    "        return function(**arguments)\n",
    "\n",
    "    def request(self, *, role=\"user\", content=None):\n",
    "        if role and content:\n",
    "            self._messages.append({ \"role\": role, \"content\": content })\n",
    "        result = self._client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=self._messages,\n",
    "            tools=self._function_infos.values(),\n",
    "        )\n",
    "        self.append_message(**dict(result.choices[0].message))\n",
    "        if result.choices[0].message.tool_calls:\n",
    "            for tool_call in result.choices[0].message.tool_calls:\n",
    "                call_result = self._call(tool_call.function)\n",
    "                self.append_message(\"tool\", str(call_result), tool_call_id=tool_call.id)\n",
    "            return self.request()\n",
    "        else:\n",
    "            self.append_message(\"assistant\", result.choices[0].message.content)\n",
    "            return result.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 联网检索现实场景"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Processing Messages]: {'role': 'assistant', 'content': None, 'refusal': None, 'annotations': [], 'audio': None, 'function_call': None, 'tool_calls': [ChatCompletionMessageToolCall(id='call_CgPWqMF0iTsB3y7UDchAPE3e', function=Function(arguments='{\"city\":\"北京\",\"location\":\"五道口\"}', name='get_location_coordinate'), type='function')]}\n",
      "[Processing Messages]: {'role': 'tool', 'content': \"{'parent': '', 'address': '海淀区', 'distance': '', 'pcode': '110000', 'adcode': '110108', 'pname': '北京市', 'cityname': '北京市', 'type': '地名地址信息;热点地名;热点地名', 'typecode': '190700', 'adname': '海淀区', 'citycode': '010', 'name': '五道口', 'location': '116.338611,39.992552', 'id': 'B000A8WSBH'}\", 'tool_call_id': 'call_CgPWqMF0iTsB3y7UDchAPE3e'}\n",
      "[Processing Messages]: {'role': 'assistant', 'content': None, 'refusal': None, 'annotations': [], 'audio': None, 'function_call': None, 'tool_calls': [ChatCompletionMessageToolCall(id='call_80oV9YDrCDQU4axygLZazpAE', function=Function(arguments='{\"keyword\":\"咖啡馆\",\"latitude\":39.992552,\"longitude\":116.338611}', name='search_nearby_pois'), type='function')]}\n",
      "[Processing Messages]: {'role': 'tool', 'content': '星巴克(北京五道口购物中心店)\\n海淀街道府路28号五道口商城1层\\n距离：44米\\n\\n瑞幸咖啡(五道口购物中心店)\\n成府路28号五道口购物中心负一层101号\\n距离：67米\\n\\nManner Coffee(五道口购物中心店)\\n成府路28号五道口购物中心1F层L1-04\\n距离：88米\\n\\n', 'tool_call_id': 'call_80oV9YDrCDQU4axygLZazpAE'}\n",
      "[Processing Messages]: {'role': 'assistant', 'content': '在五道口附近，你可以找到以下咖啡馆：\\n\\n1. **星巴克(北京五道口购物中心店)**  \\n   地址：海淀街道府路28号五道口商城1层  \\n   距离五道口约44米。\\n\\n2. **瑞幸咖啡(五道口购物中心店)**  \\n   地址：成府路28号五道口购物中心负一层101号  \\n   距离五道口约67米。\\n\\n3. **Manner Coffee(五道口购物中心店)**  \\n   地址：成府路28号五道口购物中心1F层L1-04  \\n   距离五道口约88米。\\n\\n这些咖啡馆都位于五道口购物中心内，非常方便。希望你能找到合适的休闲或工作的地方！', 'refusal': None, 'annotations': [], 'audio': None, 'function_call': None, 'tool_calls': None}\n",
      "[Processing Messages]: {'role': 'assistant', 'content': '在五道口附近，你可以找到以下咖啡馆：\\n\\n1. **星巴克(北京五道口购物中心店)**  \\n   地址：海淀街道府路28号五道口商城1层  \\n   距离五道口约44米。\\n\\n2. **瑞幸咖啡(五道口购物中心店)**  \\n   地址：成府路28号五道口购物中心负一层101号  \\n   距离五道口约67米。\\n\\n3. **Manner Coffee(五道口购物中心店)**  \\n   地址：成府路28号五道口购物中心1F层L1-04  \\n   距离五道口约88米。\\n\\n这些咖啡馆都位于五道口购物中心内，非常方便。希望你能找到合适的休闲或工作的地方！'}\n",
      "----------------------\n",
      "\n",
      " 在五道口附近，你可以找到以下咖啡馆：\n",
      "\n",
      "1. **星巴克(北京五道口购物中心店)**  \n",
      "   地址：海淀街道府路28号五道口商城1层  \n",
      "   距离五道口约44米。\n",
      "\n",
      "2. **瑞幸咖啡(五道口购物中心店)**  \n",
      "   地址：成府路28号五道口购物中心负一层101号  \n",
      "   距离五道口约67米。\n",
      "\n",
      "3. **Manner Coffee(五道口购物中心店)**  \n",
      "   地址：成府路28号五道口购物中心1F层L1-04  \n",
      "   距离五道口约88米。\n",
      "\n",
      "这些咖啡馆都位于五道口购物中心内，非常方便。希望你能找到合适的休闲或工作的地方！\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "\n",
    "amap_key = os.getenv(\"AMAP_MAPS_API_KEY\")\n",
    "amap_base_url = \"https://restapi.amap.com/v5\" # 默认是 https://restapi.amap.com/v5\n",
    "\n",
    "\n",
    "def get_location_coordinate(location, city):\n",
    "    url = f\"{amap_base_url}/place/text?key={amap_key}&keywords={location}&region={city}\"\n",
    "    r = requests.get(url)\n",
    "    result = r.json()\n",
    "    if \"pois\" in result and result[\"pois\"]:\n",
    "        return result[\"pois\"][0]\n",
    "    return None\n",
    "\n",
    "\n",
    "def search_nearby_pois(longitude, latitude, keyword):\n",
    "    url = f\"{amap_base_url}/place/around?key={amap_key}&keywords={keyword}&location={longitude},{latitude}\"\n",
    "    r = requests.get(url)\n",
    "    result = r.json()\n",
    "    ans = \"\"\n",
    "    if \"pois\" in result and result[\"pois\"]:\n",
    "        for i in range(min(3, len(result[\"pois\"]))):\n",
    "            name = result[\"pois\"][i][\"name\"]\n",
    "            address = result[\"pois\"][i][\"address\"]\n",
    "            distance = result[\"pois\"][i][\"distance\"]\n",
    "            ans += f\"{name}\\n{address}\\n距离：{distance}米\\n\\n\"\n",
    "    return ans\n",
    "\n",
    "function_calling_request = ModelRequestWithFunctionCalling()\n",
    "\n",
    "(\n",
    "    function_calling_request\n",
    "        .register_function(\n",
    "            name=\"get_location_coordinate\",\n",
    "            description=\"根据POI名称，获得POI的经纬度坐标\",\n",
    "            parameters={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"POI名称，必须是中文\",\n",
    "                    },\n",
    "                    \"city\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"POI所在的城市名，必须是中文\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"location\", \"city\"],\n",
    "            },\n",
    "            function=get_location_coordinate,\n",
    "        )\n",
    "        .register_function(\n",
    "            name=\"search_nearby_pois\",\n",
    "            description=\"搜索给定坐标附近的poi\",\n",
    "            parameters={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"longitude\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"中心点的经度\",\n",
    "                    },\n",
    "                    \"latitude\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"中心点的纬度\",\n",
    "                    },\n",
    "                    \"keyword\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"目标poi的关键字\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"longitude\", \"latitude\", \"keyword\"],\n",
    "            },\n",
    "            function=search_nearby_pois,\n",
    "        )\n",
    ")\n",
    "result = function_calling_request.request(content=\"五道口附近的咖啡馆\")\n",
    "print(\"----------------------\\n\\n\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 本地数据库查询"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "database_schema_string = \"\"\"\n",
    "CREATE TABLE orders (\n",
    "    id INT PRIMARY KEY NOT NULL, -- 主键，不允许为空\n",
    "    customer_id INT NOT NULL, -- 客户ID，不允许为空\n",
    "    product_id STR NOT NULL, -- 产品ID，不允许为空\n",
    "    price DECIMAL(10,2) NOT NULL, -- 价格，不允许为空\n",
    "    status INT NOT NULL, -- 订单状态，整数类型，不允许为空。0代表待支付，1代表已支付，2代表已退款\n",
    "    create_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP, -- 创建时间，默认为当前时间\n",
    "    pay_time TIMESTAMP -- 支付时间，可以为空\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "conn = sqlite3.connect(':memory:')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(database_schema_string)\n",
    "\n",
    "mock_data = [\n",
    "    (1, 1001, 'TSHIRT_1', 50.00, 0, '2023-09-12 10:00:00', None),\n",
    "    (2, 1001, 'TSHIRT_2', 75.50, 1, '2023-09-16 11:00:00', '2023-08-16 12:00:00'),\n",
    "    (3, 1002, 'SHOES_X2', 25.25, 2, '2023-10-17 12:30:00', '2023-08-17 13:00:00'),\n",
    "    (4, 1003, 'SHOES_X2', 25.25, 1, '2023-10-17 12:30:00', '2023-08-17 13:00:00'),\n",
    "    (5, 1003, 'HAT_Z112', 60.75, 1, '2023-10-20 14:00:00', '2023-08-20 15:00:00'),\n",
    "    (6, 1002, 'WATCH_X001', 90.00, 0, '2023-10-28 16:00:00', None)\n",
    "]\n",
    "\n",
    "for record in mock_data:\n",
    "    cursor.execute('''\n",
    "    INSERT INTO orders (id, customer_id, product_id, price, status, create_time, pay_time)\n",
    "    VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "    ''', record)\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "def query_db(query):\n",
    "    cursor.execute(query)\n",
    "    return cursor.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2 调用执行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Processing Messages]: {'role': 'assistant', 'content': None, 'refusal': None, 'annotations': None, 'audio': None, 'function_call': None, 'tool_calls': [ChatCompletionMessageFunctionToolCall(id='call_xa3OMixL6SBGCliUkQJY7IvL', function=Function(arguments='{\"query\":\"SELECT COUNT(*) AS total_orders FROM orders WHERE strftime(\\'%Y-%m\\', create_time) = \\'2023-10\\';\"}', name='query_db', parameters=None), type='function')]}\n",
      "[Processing Messages]: {'role': 'tool', 'content': '[(4,)]', 'tool_call_id': 'call_xa3OMixL6SBGCliUkQJY7IvL'}\n",
      "[Processing Messages]: {'role': 'assistant', 'content': '2023年10月总共成交了 4 笔订单。', 'refusal': None, 'annotations': None, 'audio': None, 'function_call': None, 'tool_calls': None}\n",
      "[Processing Messages]: {'role': 'assistant', 'content': '2023年10月总共成交了 4 笔订单。'}\n"
     ]
    }
   ],
   "source": [
    "function_calling_request = ModelRequestWithFunctionCalling()\n",
    "\n",
    "(\n",
    "    function_calling_request\n",
    "        .register_function(\n",
    "            name=\"query_db\",\n",
    "            description=\"使用此函数查询业务数据库获取结果，输出的SQL需要能够在Python的sqlite3中执行\",\n",
    "            parameters={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": f\"\"\"\n",
    "                        SQL query extracting info to answer the user's question.\n",
    "                        The query should be returned in plain text, not in JSON.\n",
    "                        The query should only contain grammars supported by SQLite.\n",
    "                        \"\"\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"query\"],\n",
    "            },\n",
    "            function=query_db,\n",
    "        )\n",
    ")\n",
    "\n",
    "question = \"2023年10月总共成交了几笔订单？\"\n",
    "\n",
    "result = function_calling_request.request(\n",
    "    content=f\"\"\"\n",
    "    问题：{ question },\n",
    "    数据库元数据信息：{ database_schema_string },\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 跨模型协作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 利用文心4.0以上模型作为搜索工具"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 安装文心调用SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install erniebot "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**GPT-5发布会的亮点如下**：\n",
      "1. **集成模型架构**：GPT-5是集成模型，融合了大语言模型GPT系列和推理模型o系列，采用内嵌式三位一体集成架构。系统由处理常规问题的GPT-5-main模型、解决复杂任务的GPT-5-thinking深度思考模型、实时决策的路由机制以及额度使用完后启动的mini版本组成。用户无需手动切换模型，系统会根据问题复杂度自动决定是否进入深度思考模式。\n",
      "2. **性能显著提升**：在数学、编程、视觉感知和健康等领域表现出顶尖性能，大幅超越前代模型。例如在2025年AIME测试中无工具辅助达到94.6%；在健康领域表现大幅超越所有前代模型；在大模型竞技场LMArena上，在所有细分类目中都位列第一。\n",
      "3. **幻觉率大幅降低**：与GPT-4o相比，GPT-5的事实错误率降低了45%；在深度思考模式下，事实错误率比o3降低80%。在开放性事实准确性基准LongFact和FActScore测试中，幻觉率比o3减少大约六倍，长篇内容生成的准确性显著提升。\n",
      "4. **多版本与定价策略**：拥有GPT-5、GPT-5-mini、GPT-5-nano和GPT-5-pro四个版本。免费用户用量有限，超过限额后自动转至GPT-5-mini，GPT-5-pro仅供Pro订阅用户使用。面向开发者提供API服务，输入、输出价格分别为每百万token 1.25美元/10美元、每百万token 0.5美元/5美元、每百万token 0.15美元/1.5美元，成本相当甚至低于主要竞争对手。\n",
      "5. **应用场景丰富**：编码能力突出，能几句话创建游戏，几分钟开发学习法语的网络应用、复杂的财务分析模版等。在写作方面从“工具”进化为“缪斯”，能理解并创作具有文学深度和韵律的作品。在健康问题上，像一个思维伙伴，会主动提问、标记潜在风险，帮用户更好地与医生沟通。官方还推出四种预设性格，用户可以随时切换。\n",
      "6. **API调用优化**：实现了更加灵活的模型和思考强度调节，提供三个级别的模型选择，每个版本都可以调节推理工作量。通过“详细程度”参数，用户可以控制输出更简洁或更详细，其函数调用能用自然语言来配置，而不需要用JSON格式。还将发布一个结构化输出的扩展，用户可以提供正则表达式，甚至是上下文无关语法，并将模型的输出约束到这些语法上。\n",
      "7. **上下文长度提升**：上下文长度从o3 pro的200K提升到了400K，是Claude 4的两倍，虽然不如Gemini 2.5 pro的1000K，但对于开发者而言也是极大的提升。\n",
      "8. **智能路由系统**：引入全新的“智能路由”系统，能秒级判断问题难度，自动调用合适的模型来回答，普通问题用“高效大脑”快速响应，复杂问题则自动切换到“深度思考大脑”。\n",
      "9. **Pro版优势明显**：GPT-5 Pro版相当于顶尖人类专家，拥有“扩展推理”能力，会花更长时间进行并行计算，提供最全面、最精准的答案。在超高难度的科学问题（GPQA）上，刷新了世界纪录，在与人类专家的“盲测”中，10次里有近7次被认为更优，Pro订阅者专属，每月费用200美元。"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"AISTUDIO_ACCESS_TOKEN\"),  # Access Token属于个人账户的重要隐私信息，请谨慎管理，切忌随意对外公开,\n",
    "    base_url=\"https://aistudio.baidu.com/llm/lmapi/v3\",  # aistudio 大模型 api 服务域名\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    model=\"ernie-4.5-turbo-32k\",\n",
    "    messages=[\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"GPT-5发布会有哪些亮点？\"\n",
    "    }\n",
    "],\n",
    "    stream=True,\n",
    "    extra_body={\n",
    "        \"penalty_score\": 1,\n",
    "        \"web_search\": {\n",
    "            \"enable\": True\n",
    "        }\n",
    "    },\n",
    "    max_completion_tokens=12288,\n",
    "    temperature=0.95,\n",
    "    top_p=0.7\n",
    ")\n",
    "\n",
    "for chunk in chat_completion:\n",
    "    if not chunk.choices or len(chunk.choices) == 0:\n",
    "        continue\n",
    "    if hasattr(chunk.choices[0].delta, \"reasoning_content\") and chunk.choices[0].delta.reasoning_content:\n",
    "        print(chunk.choices[0].delta.reasoning_content, end=\"\", flush=True)\n",
    "    else:\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**GPT-5发布会的亮点如下**：\n",
      "1. **集成模型架构**：GPT-5是集成模型，融合了大语言模型GPT系列和推理模型o系列，采用内嵌式三位一体集成架构。系统由处理常规问题的GPT-5-main模型、解决复杂任务的GPT-5-thinking深度思考模型、实时决策的路由机制以及额度使用完后启动的mini版本组成。用户无需手动切换不同模型，路由机制会根据对话类型、复杂程度、工具需求以及用户意图，快速决定使用哪个模型，自主决定是否进入深度思考模式。\n",
      "2. **性能显著提升**：在数学、编程、视觉感知和健康等领域都表现出顶尖性能，大幅超越前代。例如在数学领域，GPT-5在2025年AIME测试中无工具辅助达到94.6%，远超o3模型；在健康领域，表现大幅超越所有前代模型；在大模型竞技场LMArena上，凭借极强性能在所有细分类目中都位列第一。\n",
      "3. **幻觉率大幅降低**：与GPT-4o相比，GPT-5的事实错误率降低了45%；在深度思考模式下，事实错误率比o3降低80%。在开放性事实准确性基准LongFact和FActScore测试中，GPT-5的幻觉率比o3减少大约六倍，长篇内容生成的准确性显著提升。\n",
      "4. **多版本与定价策略**：拥有GPT-5、GPT-5-mini、GPT-5-nano和GPT-5-pro四个版本。免费用户GPT-5用量有限，超过限额后自动转至GPT-5-mini，GPT-5-pro仅供Pro订阅用户使用。GPT-5、GPT-5-mini、GPT-5-nano三款模型可面向开发者提供API服务，输入、输出价格分别为每百万token 1.25美元/10美元、每百万token 0.5美元/5美元、每百万token 0.15美元/1.5美元，与主要竞争对手相比成本相当甚至更低。\n",
      "5. **应用场景丰富**：编码能力突出，能几句话创建游戏，几分钟开发学习法语的网络应用、复杂的财务分析模版等。在写作方面从“工具”进化为“缪斯”，能理解并创作具有文学深度和韵律的作品；在健康问题上，像一个思维伙伴，会主动提问、标记潜在风险，帮用户更好地与医生沟通；官方还推出四种预设性格，用户可以随时切换，让AI的沟通风格更符合偏好。\n",
      "6. **API调用优化**：实现了更加灵活的模型和思考强度调节，提供三个级别的模型选择，每个版本都可以调节推理工作量。通过“详细程度”参数，用户可以控制输出更简洁或更详细，其函数调用能用自然语言来配置，而不需要用JSON格式。此外，还将发布一个结构化输出的扩展，用户可以提供正则表达式，甚至是上下文无关语法，并将模型的输出约束到这些语法上。\n",
      "7. **上下文长度提升**：GPT-5的上下文长度从o3 pro的200K提升到了400K，是Claude 4的两倍，虽然不如Gemini 2.5 pro的1000K，但对于开发者而言也是极大的提升。\n",
      "8. **智能路由系统**：引入全新的“智能路由”系统，能秒级判断问题难度，自动调用合适的模型来回答，普通问题用“高效大脑”快速响应，复杂问题则自动切换到“深度思考大脑”。\n",
      "9. **Pro版优势明显**：GPT-5 Pro版相当于顶尖人类专家，拥有“扩展推理”能力，会花更长时间进行并行计算，提供最全面、最精准的答案。在超高难度的科学问题（GPQA）上，刷新了世界纪录，在与人类专家的“盲测”中，10次里有近7次被认为更优，Pro订阅者专属，每月费用200美元。"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"AISTUDIO_ACCESS_TOKEN\"),  # Access Token属于个人账户的重要隐私信息，请谨慎管理，切忌随意对外公开,\n",
    "    base_url=\"https://aistudio.baidu.com/llm/lmapi/v3\",  # aistudio 大模型 api 服务域名\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    model=\"ernie-4.5-turbo-32k\",\n",
    "    messages=[\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"GPT-5发布会有哪些亮点？\"\n",
    "    }\n",
    "],\n",
    "    stream=True,\n",
    "    extra_body={\n",
    "        \"penalty_score\": 1,\n",
    "        \"web_search\": {\n",
    "            \"enable\": True\n",
    "        }\n",
    "    },\n",
    "    max_completion_tokens=12288,\n",
    "    temperature=0.95,\n",
    "    top_p=0.7\n",
    ")\n",
    "\n",
    "for chunk in chat_completion:\n",
    "    if hasattr(chunk.choices[0].delta, \"reasoning_content\") and chunk.choices[0].delta.reasoning_content:\n",
    "        print(chunk.choices[0].delta.reasoning_content, end=\"\", flush=True)\n",
    "    else:\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 封装工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import erniebot\n",
    "\n",
    "erniebot.api_type = \"aistudio\"\n",
    "erniebot.access_token = os.environ.get(\"AISTUDIO_ACCESS_TOKEN\")\n",
    "\n",
    "def nl_search(question:str):\n",
    "    prompt = f\"\"\"\n",
    "基于联网搜索结果回答此问题：{ question }\n",
    "其他输出要求：答案中的关键信息必须标注精确到内容页面的来源链接\n",
    "你的回答：\n",
    "\"\"\"\n",
    "    response = erniebot.ChatCompletion.create(\n",
    "    model=\"ernie-4.5-turbo-32k\",\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt,\n",
    "    }])\n",
    "    return response.get_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 调用执行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Processing Messages]: {'role': 'assistant', 'content': None, 'refusal': None, 'annotations': None, 'audio': None, 'function_call': None, 'tool_calls': [ChatCompletionMessageFunctionToolCall(id='call_9T1bf5TuVJc9hcSFjDgx7cxo', function=Function(arguments='{\"question\":\"GPT-5发布会亮点\"}', name='nl_search', parameters=None), type='function')]}\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "'ernie-4.5-turbo-32k' is not a supported model.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidArgumentError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m      3\u001b[39m (\n\u001b[32m      4\u001b[39m     function_calling_request\n\u001b[32m      5\u001b[39m         .register_function(\n\u001b[32m   (...)\u001b[39m\u001b[32m     19\u001b[39m         )\n\u001b[32m     20\u001b[39m )\n\u001b[32m     22\u001b[39m question = \u001b[33m\"\u001b[39m\u001b[33mGPT-5发布会有哪些亮点？\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m result = \u001b[43mfunction_calling_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 60\u001b[39m, in \u001b[36mModelRequestWithFunctionCalling.request\u001b[39m\u001b[34m(self, role, content)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result.choices[\u001b[32m0\u001b[39m].message.tool_calls:\n\u001b[32m     59\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m tool_call \u001b[38;5;129;01min\u001b[39;00m result.choices[\u001b[32m0\u001b[39m].message.tool_calls:\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m         call_result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool_call\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m         \u001b[38;5;28mself\u001b[39m.append_message(\u001b[33m\"\u001b[39m\u001b[33mtool\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(call_result), tool_call_id=tool_call.id)\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 47\u001b[39m, in \u001b[36mModelRequestWithFunctionCalling._call\u001b[39m\u001b[34m(self, function_calling_result)\u001b[39m\n\u001b[32m     45\u001b[39m function = \u001b[38;5;28mself\u001b[39m._function_mappings[function_calling_result.name]\n\u001b[32m     46\u001b[39m arguments = json.loads(function_calling_result.arguments)\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43marguments\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mnl_search\u001b[39m\u001b[34m(question)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnl_search\u001b[39m(question:\u001b[38;5;28mstr\u001b[39m):\n\u001b[32m      7\u001b[39m     prompt = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[33m基于联网搜索结果回答此问题：\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;250m \u001b[39mquestion\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33m其他输出要求：答案中的关键信息必须标注精确到内容页面的来源链接\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[33m你的回答：\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     response = \u001b[43merniebot\u001b[49m\u001b[43m.\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mernie-4.5-turbo-32k\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.get_result()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\jukeai-chapter6\\Lib\\site-packages\\erniebot\\resources\\chat_completion.py:318\u001b[39m, in \u001b[36mChatCompletion.create\u001b[39m\u001b[34m(cls, model, messages, functions, temperature, top_p, penalty_score, system, stop, disable_search, enable_citation, user_id, tool_choice, stream, validate_functions, extra_params, headers, request_timeout, response_format, max_output_tokens, _config_)\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    316\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m] = response_format\n\u001b[32m--> \u001b[39m\u001b[32m318\u001b[39m resp = \u001b[43mresource\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_resource\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m transform(ChatCompletionResponse.from_mapping, resp)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\jukeai-chapter6\\Lib\\site-packages\\erniebot\\resources\\abc\\creatable.py:66\u001b[39m, in \u001b[36mCreatableWithStreaming.create_resource\u001b[39m\u001b[34m(self, **create_kwargs)\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_resource\u001b[39m(\u001b[38;5;28mself\u001b[39m, **create_kwargs: Any) -> Union[EBResponse, Iterator[EBResponse]]:\n\u001b[32m     65\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Creates a resource.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     req = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_create\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcreate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m     resp = \u001b[38;5;28mself\u001b[39m.request(\n\u001b[32m     68\u001b[39m         method=req.method,\n\u001b[32m     69\u001b[39m         path=req.path,\n\u001b[32m   (...)\u001b[39m\u001b[32m     73\u001b[39m         request_timeout=req.timeout,\n\u001b[32m     74\u001b[39m     )\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp, EBResponse):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\jukeai-chapter6\\Lib\\site-packages\\erniebot\\resources\\chat_completion.py:564\u001b[39m, in \u001b[36mChatCompletion._prepare_create\u001b[39m\u001b[34m(self, kwargs)\u001b[39m\n\u001b[32m    562\u001b[39m     api_info = \u001b[38;5;28mself\u001b[39m._API_INFO_DICT[\u001b[38;5;28mself\u001b[39m.api_type]\n\u001b[32m    563\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m api_info[\u001b[33m\"\u001b[39m\u001b[33mmodels\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m564\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m errors.InvalidArgumentError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(model)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not a supported model.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    565\u001b[39m     path = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mapi_info[\u001b[33m'\u001b[39m\u001b[33mresource_id\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mapi_info[\u001b[33m'\u001b[39m\u001b[33mmodels\u001b[39m\u001b[33m'\u001b[39m][model][\u001b[33m'\u001b[39m\u001b[33mmodel_id\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    566\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mInvalidArgumentError\u001b[39m: 'ernie-4.5-turbo-32k' is not a supported model."
     ]
    }
   ],
   "source": [
    "function_calling_request = ModelRequestWithFunctionCalling()\n",
    "\n",
    "(\n",
    "    function_calling_request\n",
    "        .register_function(\n",
    "            name=\"nl_search\",\n",
    "            description=\"使用此工具，可以用自然语言输入，获得基于网络搜索的事实性结果总结\",\n",
    "            parameters={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"question\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"使用自然语言总结用户关注的关键问题\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"question\"],\n",
    "            },\n",
    "            function=nl_search,\n",
    "        )\n",
    ")\n",
    "\n",
    "question = \"GPT-5发布会有哪些亮点？\"\n",
    "\n",
    "result = function_calling_request.request(\n",
    "    content=question,\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Function Calling在大模型应用场景中带来的“质变”\n",
    "\n",
    "- 知识层面：从模型自身知识（来源于训练语料）扩展到真实世界知识\n",
    "- 行为层面：从“思考模拟器”、“问题应答”扩展到“理解问题-选择行动-发起请求-理解结果-给出回应”\n",
    "- 架构层面：让模型不再是一个孤立模块，而是可以融入现有信息系统之中\n",
    "\n",
    "给软件开发思想带来的冲击：\n",
    "\n",
    "- 不是基于“规则”而是基于“世界理解”的调用\n",
    "- 接纳没有明确的处理过程带来的输出不确定性（如数据查询）\n",
    "- 不走极端：“全盘拒绝”和“全盘接受”都不可取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 智能体/LLM应用的定义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Agent智能体的概念存在“过度炒作“的现象，部分媒体使用“智能体“这个词指代任何基于LLM能力构建的应用\n",
    "- 不给出清晰定义的概念讨论甚至衍生讨论都是耍流氓"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 机器学习概念中的Agent\n",
    "\n",
    "在机器学习领域，智能体（Agent）通常指能够感知环境、做出决策并采取行动以实现特定目标的实体。这些智能体具备自主性，能够通过传感器获取环境信息，经过内部处理后，通过执行器对环境施加影响。这种架构使智能体能够在复杂、多变的环境中自主运作。\n",
    "\n",
    "- 例如：\n",
    "    - Alpha Go\n",
    "    - 星际争霸/Dota 2对战AI\n",
    "    - 学踢足球的AI [点击观看](https://www.bilibili.com/video/BV11e4y1V7US/?share_source=copy_web&vd_source=b44770b90404f8e657d344a39ac5f758)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 由各类开源项目实践并由Lilian Weng总结的LLM-Powered Autonomous Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 原文地址：https://lilianweng.github.io/posts/2023-06-23-agent/\n",
    "\n",
    "- 重要意义：给基于LLM驱动的智能体讨论提供了共识性的基础定义\n",
    "\n",
    "- 架构图：\n",
    "\n",
    "    <img src = \"./assets/agent-overview.png\" width=\"800\"/>\n",
    "\n",
    "- 核心概念：\n",
    "    - 核心驱动：LLM（提供基础智力、通识、逻辑、上下文内学习等基础能力）\n",
    "    - 关键组件：\n",
    "        - **规划（Planning）**：将复杂任务分解为可管理的子目标(Task Decomposition)，并通过自我反思（Self-Reflection）来提高结果质量\n",
    "        - **记忆（Memory）**：包括短期记忆（对话记录）和<u>长期记忆（通过外部向量存储和快速检索来保留和回忆信息）</u>（这部分突破项目不多，去年有一个叫Mem0的项目刷过一次屏）\n",
    "        - **工具使用（Tool Use）**：学习调用外部工具，补充额外信息或完成环境交互\n",
    "\n",
    "- Inspiration：\n",
    "    - [ReAct论文](https://arxiv.org/abs/2210.03629）\n",
    "    - [AutoGPT](https://github.com/Significant-Gravitas/Auto-GPT)\n",
    "    - [GPT-Engineer](https://github.com/AntonOsika/gpt-engineer)\n",
    "    - [BabyAGI](https://github.com/yoheinakajima/babyagi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 多智能体协同\n",
    "\n",
    "- 智能体概念的提出让一批使用类似上述结构（通常是简化的结构，比如只使用Role设定，或是ReAct Prompt）尝试进行多次模型请求协同的项目被关注，核心思想是通过不同的智能体分工协作，组成更大的协作网络\n",
    "\n",
    "- 代表项目：\n",
    "    - Camel.ai\n",
    "    - MetaGPT\n",
    "    - Microsoft AutoGen\n",
    "    - OpenAI Swarm(现在的Agent SDK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 智能体开发框架smolagents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 smolagents 介绍\n",
    "\n",
    "Github：https://github.com/huggingface/smolagents\n",
    "\n",
    "官方文档：https://huggingface.co/docs/smolagents/index\n",
    "\n",
    "smolagents是HuggingFace官方推出的Agent开发库，构建强大 agent 的最简单框架！。\n",
    "\n",
    "首先来介绍一下smolagents吧，smol是small的俏皮用法，故smolagents的含义是“轻量的agent工具”。smolagents库提供：\n",
    "\n",
    "✨ **简洁性**：Agent 逻辑仅需约千行代码。我们将抽象保持在原始代码之上的最小形态！\n",
    "\n",
    "🌐 **支持任何 LLM**：支持通过 Hub 托管的模型，使用其 `transformers` 版本或通过我们的推理 API 加载，也支持 OpenAI、Anthropic 等模型。使用任何 LLM 为 agent 提供动力都非常容易。\n",
    "\n",
    "🧑‍💻 **一流的代码 agent 支持**，即编写代码作为其操作的 agent（与\"用于编写代码的 agent\"相对），[在此了解更多](tutorials/secure_code_execution)。\n",
    "\n",
    "🤗 **Hub 集成**：您可以在 Hub 上共享和加载工具，更多功能即将推出！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 实战：Agentic RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 传统 RAG 的局限性\n",
    "\n",
    "尽管传统 RAG 方法有诸多优势，但它也面临一些挑战：\n",
    "\n",
    "1. 单次检索步骤：如果初始检索结果较差，则最终生成的结果将受到影响\n",
    "2. 查询文档不匹配：用户查询（通常是问题）可能与包含答案（通常是陈述）的文档不太匹配\n",
    "3. 推理能力有限：简单的 RAG 流程无法进行多步推理或查询细化\n",
    "4. 上下文窗口约束：检索到的文档必须适合模型的上下文窗口"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Agentic RAG 的主要优势\n",
    "\n",
    "拥有检索工具的代理可以：\n",
    "\n",
    "1. ✅制定优化查询：代理可以将用户问题转换为易于检索的查询\n",
    "\n",
    "2. ✅执行多次检索：代理可以根据需要迭代检索信息\n",
    "\n",
    "3. ✅对检索到的内容进行推理：代理可以从多个来源进行分析、综合并得出结论\n",
    "\n",
    "4. ✅自我批评和改进：代理可以评估检索结果并调整其方法\n",
    "\n",
    "这种方法自然地实现了先进的 RAG 技术：\n",
    "\n",
    "- Hypothetical Document Embedding (HyDE)：代理不直接使用用户查询，而是制定检索优化查询\n",
    "- Self-Query Refinement：代理可以分析初始结果，并使用细化查询执行后续检索"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 构建 Agentic RAG 系统\n",
    "\n",
    "<img src=\"./assets/agentic-rag.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install smolagents pandas langchain langchain-community sentence-transformers datasets rank_bm25 hf_xet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你需要一个有效的 token 作为环境变量 `HF_TOKEN` 来调用 Inference Providers。\n",
    "\n",
    "Hugging Face 注册 - 登录 - 创建 Access Tokens - 系统环境变量配置 HF_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们首先加载一个知识库以在其上执行 RAG：此数据集是许多 Hugging Face 库的文档页面的汇编，存储为 markdown 格式。我们将仅保留 `transformers` 库的文档。然后通过处理数据集并将其存储到向量数据库中，为检索器准备知识库。我们将使用 [LangChain](https://python.langchain.com/docs/introduction/) 来利用其出色的向量数据库工具。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\.conda\\envs\\jukeai-chapter6\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\HP\\.cache\\huggingface\\hub\\datasets--m-ric--huggingface_doc. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Generating train split: 100%|█████████████████████████████████████████████| 2647/2647 [00:00<00:00, 7088.26 examples/s]\n",
      "Filter: 100%|████████████████████████████████████████████████████████████| 2647/2647 [00:00<00:00, 45892.35 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "knowledge_base = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\")\n",
    "knowledge_base = knowledge_base.filter(lambda row: row[\"source\"].startswith(\"huggingface/transformers\"))\n",
    "\n",
    "source_docs = [\n",
    "    Document(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"].split(\"/\")[1]})\n",
    "    for doc in knowledge_base\n",
    "]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    add_start_index=True,\n",
    "    strip_whitespace=True,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    ")\n",
    "docs_processed = text_splitter.split_documents(source_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在文档已准备好。我们来一起构建我们的 agent RAG 系统！\n",
    "👉 我们只需要一个 RetrieverTool，我们的 agent 可以利用它从知识库中检索信息。\n",
    "\n",
    "由于我们需要将 vectordb 添加为工具的属性，我们不能简单地使用带有 `@tool` 装饰器的简单工具构造函数：因此我们将遵循 [tools 教程](../tutorials/tools) 中突出显示的高级设置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import Tool\n",
    "\n",
    "class RetrieverTool(Tool):\n",
    "    name = \"retriever\"\n",
    "    description = \"Uses semantic search to retrieve the parts of transformers documentation that could be most relevant to answer your query.\"\n",
    "    inputs = {\n",
    "        \"query\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The query to perform. This should be semantically close to your target documents. Use the affirmative form rather than a question.\",\n",
    "        }\n",
    "    }\n",
    "    output_type = \"string\"\n",
    "\n",
    "    def __init__(self, docs, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.retriever = BM25Retriever.from_documents(\n",
    "            docs, k=10\n",
    "        )\n",
    "\n",
    "    def forward(self, query: str) -> str:\n",
    "        assert isinstance(query, str), \"Your search query must be a string\"\n",
    "\n",
    "        docs = self.retriever.invoke(\n",
    "            query,\n",
    "        )\n",
    "        return \"\\nRetrieved documents:\\n\" + \"\".join(\n",
    "            [\n",
    "                f\"\\n\\n===== Document {str(i)} =====\\n\" + doc.page_content\n",
    "                for i, doc in enumerate(docs)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "retriever_tool = RetrieverTool(docs_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BM25 检索方法是一个经典的检索方法，因为它的设置速度非常快。为了提高检索准确性，你可以使用语义搜索，使用文档的向量表示替换 BM25：因此你可以前往 [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard) 选择一个好的嵌入模型。\n",
    "\n",
    "现在我们已经创建了一个可以从知识库中检索信息的工具，现在我们可以很容易地创建一个利用这个\n",
    "`retriever_tool` 的 agent！此 agent 将使用如下参数初始化：\n",
    "\n",
    "- `tools`：代理将能够调用的工具列表。\n",
    "- `model`：为代理提供动力的 LLM。\n",
    "\n",
    "我们的 `model` 必须是一个可调用对象，它接受一个消息的 list 作为输入，并返回文本。它还需要接受一个 stop_sequences 参数，指示何时停止生成。为了方便起见，我们直接使用包中提供的 `HfEngine` 类来获取调用 Hugging Face 的 Inference API 的 LLM 引擎。\n",
    "\n",
    "接着，我们将使用 [meta-llama/Llama-3.3-70B-Instruct](meta-llama/Llama-3.3-70B-Instruct) 作为 llm 引\n",
    "擎，因为：\n",
    "\n",
    "- 它有一个长 128k 上下文，这对处理长源文档很有用。\n",
    "- 它在 HF 的 Inference API 上始终免费提供！\n",
    "\n",
    "_Note:_ 此 Inference API 托管基于各种标准的模型，部署的模型可能会在没有事先通知的情况下进行更新或替换。了解更多信息，请点击[这里](https://huggingface.co/docs/api-inference/supported-models)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import InferenceClientModel, CodeAgent\n",
    "\n",
    "agent = CodeAgent(\n",
    "    tools=[retriever_tool],\n",
    "    model=InferenceClientModel(model_id=\"meta-llama/Llama-3.3-70B-Instruct\"),\n",
    "    max_steps=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当我们初始化 CodeAgent 时，它已经自动获得了一个默认的系统提示，告诉 LLM 引擎按步骤处理并生成工具调用作为代码片段，但你可以根据需要替换此提示模板。接着，当其 `.run()` 方法被调用时，代理将负责调用 LLM 引擎，并在循环中执行工具调用，直到工具 `final_answer` 被调用，而其参数为最终答案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">For a transformers model training, which is slower, the forward or the backward pass?</span>                           <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ InferenceClientModel - meta-llama/Llama-3.3-70B-Instruct ──────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mFor a transformers model training, which is slower, the forward or the backward pass?\u001b[0m                           \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m InferenceClientModel - meta-llama/Llama-3.3-70B-Instruct \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">Step 1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1;37mStep 1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing parsed code:</span> ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">info </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> retriever(query</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"transformers model training forward and backward pass computational complexity\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">       </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(info)</span><span style=\"background-color: #272822\">                                                                                                    </span>  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "</pre>\n"
      ],
      "text/plain": [
       " ─ \u001b[1mExecuting parsed code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34minfo\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mretriever\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mquery\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mtransformers model training forward and backward pass computational complexity\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m       \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34minfo\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                                    \u001b[0m  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
       "\n",
       "Retrieved documents:\n",
       "\n",
       "\n",
       "===== Document 0 =====\n",
       "Saving all activations from the forward pass in order to compute the gradients during the backward pass can result \n",
       "in \n",
       "significant memory overhead. The alternative approach of discarding the activations and recalculating them when \n",
       "needed \n",
       "during the backward pass, would introduce a considerable computational overhead and slow down the training process.\n",
       "\n",
       "===== Document 1 =====\n",
       "- A train step function which combines the loss function and optimizer update, does the forward and backward pass \n",
       "and returns the updated parameters.\n",
       "\n",
       "===== Document 2 =====\n",
       "overhead. This is super helpful when you have activation checkpointing enabled, where we do a forward recompute and\n",
       "backward passes a single layer granularity and want to keep the parameter in the forward recompute till the \n",
       "backward\n",
       "\n",
       "===== Document 3 =====\n",
       "For convolutions and linear layers there are 2x flops in the backward compared to the forward, which generally \n",
       "translates \n",
       "into ~2x slower (sometimes more, because sizes in the backward tend to be more awkward). Activations are usually \n",
       "bandwidth-limited, and it’s typical for an activation to have to read more data in the backward than in the forward\n",
       "(e.g. activation forward reads once, writes once, activation backward reads twice, gradOutput and output of the \n",
       "forward,\n",
       "\n",
       "===== Document 4 =====\n",
       "## How to benchmark 🤗 Transformers models\n",
       "\n",
       "The classes [`PyTorchBenchmark`] and [`TensorFlowBenchmark`] allow to flexibly benchmark 🤗 Transformers models. \n",
       "The benchmark classes allow us to measure the _peak memory usage_ and _required time_ for both _inference_ and \n",
       "_training_.\n",
       "\n",
       "&lt;Tip&gt;\n",
       "\n",
       "Hereby, _inference_ is defined by a single forward pass, and _training_ is defined by a single forward pass and\n",
       "backward pass.\n",
       "\n",
       "&lt;/Tip&gt;\n",
       "\n",
       "===== Document 5 =====\n",
       "becomes possible to increase the **effective batch size** beyond the limitations imposed by the GPU's memory \n",
       "capacity. \n",
       "However, it is important to note that the additional forward and backward passes introduced by gradient \n",
       "accumulation can \n",
       "slow down the training process.\n",
       "\n",
       "===== Document 6 =====\n",
       "*Self-attention has become a defacto choice for capturing global context in various vision applications. However, \n",
       "its quadratic computational complexity with respect to image resolution limits its use in real-time applications, \n",
       "especially for deployment on resource-constrained mobile devices\n",
       "\n",
       "===== Document 7 =====\n",
       "\\bold{win}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to \n",
       "non-overlapping\n",
       "local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility \n",
       "to model at\n",
       "various scales and has linear computational complexity with respect to image size. These qualities of Swin \n",
       "Transformer make it\n",
       "\n",
       "===== Document 8 =====\n",
       "```python\n",
       "# transform the loss function to get the gradients\n",
       "grad_fn = jax.value_and_grad(compute_loss)\n",
       "```\n",
       "\n",
       "We use the [optax](https://github.com/deepmind/optax) library to Initialize the optimizer. \n",
       "\n",
       "```python\n",
       "import optax\n",
       "\n",
       "params = model.params\n",
       "tx = optax.sgd(learning_rate=3e-3)\n",
       "opt_state = tx.init(params)\n",
       "```\n",
       "\n",
       "Now we define a single training step which will do a forward and a backward pass.\n",
       "\n",
       "===== Document 9 =====\n",
       "**Forward Activations**\n",
       "\n",
       "- size depends on many factors, the key ones being sequence length, hidden size and batch size.\n",
       "\n",
       "There are the input and output that are being passed and returned by the forward and the backward functions and the\n",
       "forward activations saved for gradient computation.\n",
       "\n",
       "**Temporary Memory**\n",
       "\n",
       "Out: None\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mExecution logs:\u001b[0m\n",
       "\n",
       "Retrieved documents:\n",
       "\n",
       "\n",
       "===== Document 0 =====\n",
       "Saving all activations from the forward pass in order to compute the gradients during the backward pass can result \n",
       "in \n",
       "significant memory overhead. The alternative approach of discarding the activations and recalculating them when \n",
       "needed \n",
       "during the backward pass, would introduce a considerable computational overhead and slow down the training process.\n",
       "\n",
       "===== Document 1 =====\n",
       "- A train step function which combines the loss function and optimizer update, does the forward and backward pass \n",
       "and returns the updated parameters.\n",
       "\n",
       "===== Document 2 =====\n",
       "overhead. This is super helpful when you have activation checkpointing enabled, where we do a forward recompute and\n",
       "backward passes a single layer granularity and want to keep the parameter in the forward recompute till the \n",
       "backward\n",
       "\n",
       "===== Document 3 =====\n",
       "For convolutions and linear layers there are 2x flops in the backward compared to the forward, which generally \n",
       "translates \n",
       "into ~2x slower (sometimes more, because sizes in the backward tend to be more awkward). Activations are usually \n",
       "bandwidth-limited, and it’s typical for an activation to have to read more data in the backward than in the forward\n",
       "(e.g. activation forward reads once, writes once, activation backward reads twice, gradOutput and output of the \n",
       "forward,\n",
       "\n",
       "===== Document 4 =====\n",
       "## How to benchmark 🤗 Transformers models\n",
       "\n",
       "The classes [`PyTorchBenchmark`] and [`TensorFlowBenchmark`] allow to flexibly benchmark 🤗 Transformers models. \n",
       "The benchmark classes allow us to measure the _peak memory usage_ and _required time_ for both _inference_ and \n",
       "_training_.\n",
       "\n",
       "<Tip>\n",
       "\n",
       "Hereby, _inference_ is defined by a single forward pass, and _training_ is defined by a single forward pass and\n",
       "backward pass.\n",
       "\n",
       "</Tip>\n",
       "\n",
       "===== Document 5 =====\n",
       "becomes possible to increase the **effective batch size** beyond the limitations imposed by the GPU's memory \n",
       "capacity. \n",
       "However, it is important to note that the additional forward and backward passes introduced by gradient \n",
       "accumulation can \n",
       "slow down the training process.\n",
       "\n",
       "===== Document 6 =====\n",
       "*Self-attention has become a defacto choice for capturing global context in various vision applications. However, \n",
       "its quadratic computational complexity with respect to image resolution limits its use in real-time applications, \n",
       "especially for deployment on resource-constrained mobile devices\n",
       "\n",
       "===== Document 7 =====\n",
       "\\bold{win}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to \n",
       "non-overlapping\n",
       "local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility \n",
       "to model at\n",
       "various scales and has linear computational complexity with respect to image size. These qualities of Swin \n",
       "Transformer make it\n",
       "\n",
       "===== Document 8 =====\n",
       "```python\n",
       "# transform the loss function to get the gradients\n",
       "grad_fn = jax.value_and_grad(compute_loss)\n",
       "```\n",
       "\n",
       "We use the [optax](https://github.com/deepmind/optax) library to Initialize the optimizer. \n",
       "\n",
       "```python\n",
       "import optax\n",
       "\n",
       "params = model.params\n",
       "tx = optax.sgd(learning_rate=3e-3)\n",
       "opt_state = tx.init(params)\n",
       "```\n",
       "\n",
       "Now we define a single training step which will do a forward and a backward pass.\n",
       "\n",
       "===== Document 9 =====\n",
       "**Forward Activations**\n",
       "\n",
       "- size depends on many factors, the key ones being sequence length, hidden size and batch size.\n",
       "\n",
       "There are the input and output that are being passed and returned by the forward and the backward functions and the\n",
       "forward activations saved for gradient computation.\n",
       "\n",
       "**Temporary Memory**\n",
       "\n",
       "Out: None\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 1.48 seconds| Input tokens: 2,088 | Output tokens: 108]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 1.48 seconds| Input tokens: 2,088 | Output tokens: 108]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">Step 2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1;37mStep 2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing parsed code:</span> ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">backward_pass_info </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> retriever(query</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"transformers model training backward pass computational complexity\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">     </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(backward_pass_info)</span><span style=\"background-color: #272822\">                                                                                      </span>  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "</pre>\n"
      ],
      "text/plain": [
       " ─ \u001b[1mExecuting parsed code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mbackward_pass_info\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mretriever\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mquery\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mtransformers model training backward pass computational complexity\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m     \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mbackward_pass_info\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                      \u001b[0m  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
       "\n",
       "Retrieved documents:\n",
       "\n",
       "\n",
       "===== Document 0 =====\n",
       "Saving all activations from the forward pass in order to compute the gradients during the backward pass can result \n",
       "in \n",
       "significant memory overhead. The alternative approach of discarding the activations and recalculating them when \n",
       "needed \n",
       "during the backward pass, would introduce a considerable computational overhead and slow down the training process.\n",
       "\n",
       "===== Document 1 =====\n",
       "- A train step function which combines the loss function and optimizer update, does the forward and backward pass \n",
       "and returns the updated parameters.\n",
       "\n",
       "===== Document 2 =====\n",
       "*Self-attention has become a defacto choice for capturing global context in various vision applications. However, \n",
       "its quadratic computational complexity with respect to image resolution limits its use in real-time applications, \n",
       "especially for deployment on resource-constrained mobile devices\n",
       "\n",
       "===== Document 3 =====\n",
       "\\bold{win}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to \n",
       "non-overlapping\n",
       "local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility \n",
       "to model at\n",
       "various scales and has linear computational complexity with respect to image size. These qualities of Swin \n",
       "Transformer make it\n",
       "\n",
       "===== Document 4 =====\n",
       "```python\n",
       "from transformers import Tool\n",
       "\n",
       "\n",
       "class HFModelDownloadsTool(Tool):\n",
       "    pass\n",
       "```\n",
       "\n",
       "===== Document 5 =====\n",
       "For convolutions and linear layers there are 2x flops in the backward compared to the forward, which generally \n",
       "translates \n",
       "into ~2x slower (sometimes more, because sizes in the backward tend to be more awkward). Activations are usually \n",
       "bandwidth-limited, and it’s typical for an activation to have to read more data in the backward than in the forward\n",
       "(e.g. activation forward reads once, writes once, activation backward reads twice, gradOutput and output of the \n",
       "forward,\n",
       "\n",
       "===== Document 6 =====\n",
       "### Train\n",
       "&lt;frameworkcontent&gt;\n",
       "&lt;pt&gt;\n",
       "&lt;Tip&gt;\n",
       "\n",
       "If you aren't familiar with finetuning a model with the [`Trainer`], take a look at the basic tutorial \n",
       "[here](../training#finetune-with-trainer)!\n",
       "\n",
       "&lt;/Tip&gt;\n",
       "\n",
       "You're ready to start training your model now! Load SegFormer with [`AutoModelForSemanticSegmentation`], and pass \n",
       "the model the mapping between label ids and label classes:\n",
       "\n",
       "```py\n",
       "&gt;&gt;&gt; from transformers import AutoModelForSemanticSegmentation, TrainingArguments, Trainer\n",
       "\n",
       "===== Document 7 =====\n",
       "Perceiver IO is a generalization of [Perceiver](https://arxiv.org/abs/2103.03206) to handle arbitrary outputs in\n",
       "addition to arbitrary inputs. The original Perceiver only produced a single classification label. In addition to\n",
       "classification labels, Perceiver IO can produce (for example) language, optical flow, and multimodal videos with \n",
       "audio.\n",
       "This is done using the same building blocks as the original Perceiver. The computational complexity of Perceiver IO\n",
       "is\n",
       "\n",
       "===== Document 8 =====\n",
       "overhead. This is super helpful when you have activation checkpointing enabled, where we do a forward recompute and\n",
       "backward passes a single layer granularity and want to keep the parameter in the forward recompute till the \n",
       "backward\n",
       "\n",
       "===== Document 9 =====\n",
       "becomes possible to increase the **effective batch size** beyond the limitations imposed by the GPU's memory \n",
       "capacity. \n",
       "However, it is important to note that the additional forward and backward passes introduced by gradient \n",
       "accumulation can \n",
       "slow down the training process.\n",
       "\n",
       "Out: None\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mExecution logs:\u001b[0m\n",
       "\n",
       "Retrieved documents:\n",
       "\n",
       "\n",
       "===== Document 0 =====\n",
       "Saving all activations from the forward pass in order to compute the gradients during the backward pass can result \n",
       "in \n",
       "significant memory overhead. The alternative approach of discarding the activations and recalculating them when \n",
       "needed \n",
       "during the backward pass, would introduce a considerable computational overhead and slow down the training process.\n",
       "\n",
       "===== Document 1 =====\n",
       "- A train step function which combines the loss function and optimizer update, does the forward and backward pass \n",
       "and returns the updated parameters.\n",
       "\n",
       "===== Document 2 =====\n",
       "*Self-attention has become a defacto choice for capturing global context in various vision applications. However, \n",
       "its quadratic computational complexity with respect to image resolution limits its use in real-time applications, \n",
       "especially for deployment on resource-constrained mobile devices\n",
       "\n",
       "===== Document 3 =====\n",
       "\\bold{win}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to \n",
       "non-overlapping\n",
       "local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility \n",
       "to model at\n",
       "various scales and has linear computational complexity with respect to image size. These qualities of Swin \n",
       "Transformer make it\n",
       "\n",
       "===== Document 4 =====\n",
       "```python\n",
       "from transformers import Tool\n",
       "\n",
       "\n",
       "class HFModelDownloadsTool(Tool):\n",
       "    pass\n",
       "```\n",
       "\n",
       "===== Document 5 =====\n",
       "For convolutions and linear layers there are 2x flops in the backward compared to the forward, which generally \n",
       "translates \n",
       "into ~2x slower (sometimes more, because sizes in the backward tend to be more awkward). Activations are usually \n",
       "bandwidth-limited, and it’s typical for an activation to have to read more data in the backward than in the forward\n",
       "(e.g. activation forward reads once, writes once, activation backward reads twice, gradOutput and output of the \n",
       "forward,\n",
       "\n",
       "===== Document 6 =====\n",
       "### Train\n",
       "<frameworkcontent>\n",
       "<pt>\n",
       "<Tip>\n",
       "\n",
       "If you aren't familiar with finetuning a model with the [`Trainer`], take a look at the basic tutorial \n",
       "[here](../training#finetune-with-trainer)!\n",
       "\n",
       "</Tip>\n",
       "\n",
       "You're ready to start training your model now! Load SegFormer with [`AutoModelForSemanticSegmentation`], and pass \n",
       "the model the mapping between label ids and label classes:\n",
       "\n",
       "```py\n",
       ">>> from transformers import AutoModelForSemanticSegmentation, TrainingArguments, Trainer\n",
       "\n",
       "===== Document 7 =====\n",
       "Perceiver IO is a generalization of [Perceiver](https://arxiv.org/abs/2103.03206) to handle arbitrary outputs in\n",
       "addition to arbitrary inputs. The original Perceiver only produced a single classification label. In addition to\n",
       "classification labels, Perceiver IO can produce (for example) language, optical flow, and multimodal videos with \n",
       "audio.\n",
       "This is done using the same building blocks as the original Perceiver. The computational complexity of Perceiver IO\n",
       "is\n",
       "\n",
       "===== Document 8 =====\n",
       "overhead. This is super helpful when you have activation checkpointing enabled, where we do a forward recompute and\n",
       "backward passes a single layer granularity and want to keep the parameter in the forward recompute till the \n",
       "backward\n",
       "\n",
       "===== Document 9 =====\n",
       "becomes possible to increase the **effective batch size** beyond the limitations imposed by the GPU's memory \n",
       "capacity. \n",
       "However, it is important to note that the additional forward and backward passes introduced by gradient \n",
       "accumulation can \n",
       "slow down the training process.\n",
       "\n",
       "Out: None\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 1.23 seconds| Input tokens: 5,088 | Output tokens: 239]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 1.23 seconds| Input tokens: 5,088 | Output tokens: 239]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">Step 3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1;37mStep 3\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing parsed code:</span> ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">final_answer(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"backward\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                                                       </span>  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "</pre>\n"
      ],
      "text/plain": [
       " ─ \u001b[1mExecuting parsed code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mfinal_answer\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mbackward\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                       \u001b[0m  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: backward</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: backward\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 1.16 seconds| Input tokens: 8,994 | Output tokens: 338]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 3: Duration 1.16 seconds| Input tokens: 8,994 | Output tokens: 338]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final output:\n",
      "backward\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"HF_TOKEN\"] = \"\"\n",
    "\n",
    "agent_output = agent.run(\"For a transformers model training, which is slower, the forward or the backward pass?\")\n",
    "\n",
    "print(\"Final output:\")\n",
    "print(agent_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 智能体系统/LLM应用的应用场景"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "项目举例：\n",
    "\n",
    "- DeepResearch\n",
    "\n",
    "应用场景举例：\n",
    "\n",
    "- 文案生成\n",
    "- Auto Coder\n",
    "- NL2DB\n",
    "- 智能家居\n",
    "- 智能座舱"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 智能体系统/LLM应用核心逻辑的几种分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CoT/简单工作流\n",
    "- 附加Function Calling的单次请求\n",
    "- 基于SOP的复杂工作流\n",
    "- 自规划\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 学习打卡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 掌握Function Calling核心概念和工作原理\n",
    "2. 掌握智能体Agent核心概念和工作原理\n",
    "3. 掌握智能体开发框架smolagents\n",
    "4. 基于smolagents开发Agentic RAG应用，参考 smolagents\\examples 目录下 `rag.py` 和 `rag_using_chromadb.py`\n",
    "5. 基于smolagents开发text_to_sql应用，参考 smolagents\\examples 目录下 `text_to_sql.py`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
