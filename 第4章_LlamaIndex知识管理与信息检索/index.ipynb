{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35d0faef-0ef0-4a1b-a7d8-99f59cd72a09",
   "metadata": {},
   "source": [
    "# ç¬¬4ç«  LlamaIndexçŸ¥è¯†ç®¡ç†ä¸ä¿¡æ¯æ£€ç´¢\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec8c5bb",
   "metadata": {},
   "source": [
    "## ğŸ’¡ å­¦ä¹ ç›®æ ‡\n",
    "\n",
    "1. æŒæ¡ LlamaIndex çš„ç‰¹ç‚¹å’ŒåŸºæœ¬ç”¨æ³•\n",
    "2. æŒæ¡ LlamaIndex å†…ç½®çš„å·¥å…·\n",
    "3. å¦‚ä½•ç”¨å¥½ SDK ç®€åŒ–åŸºäº LLM çš„åº”ç”¨å¼€å‘\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194427bf-5807-49d4-ab25-fa0556f835f3",
   "metadata": {},
   "source": [
    "## 1. å¤§è¯­è¨€æ¨¡å‹å¼€å‘æ¡†æ¶çš„ä»·å€¼æ˜¯ä»€ä¹ˆï¼Ÿ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb60e6e5-8f2a-4cd5-a4b5-824259afc229",
   "metadata": {},
   "source": [
    "_SDKï¼šSoftware Development Kitï¼Œå®ƒæ˜¯ä¸€ç»„è½¯ä»¶å·¥å…·å’Œèµ„æºçš„é›†åˆï¼Œæ—¨åœ¨å¸®åŠ©å¼€å‘è€…åˆ›å»ºã€æµ‹è¯•ã€éƒ¨ç½²å’Œç»´æŠ¤åº”ç”¨ç¨‹åºæˆ–è½¯ä»¶ã€‚_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d08668-4981-4b84-ae07-f74cfa191309",
   "metadata": {},
   "source": [
    "æ‰€æœ‰å¼€å‘æ¡†æ¶ï¼ˆSDKï¼‰çš„æ ¸å¿ƒä»·å€¼ï¼Œéƒ½æ˜¯é™ä½å¼€å‘ã€ç»´æŠ¤æˆæœ¬ã€‚\n",
    "\n",
    "å¤§è¯­è¨€æ¨¡å‹å¼€å‘æ¡†æ¶çš„ä»·å€¼ï¼Œæ˜¯è®©å¼€å‘è€…å¯ä»¥æ›´æ–¹ä¾¿åœ°å¼€å‘åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„åº”ç”¨ã€‚ä¸»è¦æä¾›ä¸¤ç±»å¸®åŠ©ï¼š\n",
    "\n",
    "1. ç¬¬ä¸‰æ–¹èƒ½åŠ›æŠ½è±¡ã€‚æ¯”å¦‚ LLMã€å‘é‡æ•°æ®åº“ã€æœç´¢æ¥å£ç­‰\n",
    "2. å¸¸ç”¨å·¥å…·ã€æ–¹æ¡ˆå°è£…\n",
    "3. åº•å±‚å®ç°å°è£…ã€‚æ¯”å¦‚æµå¼æ¥å£ã€è¶…æ—¶é‡è¿ã€å¼‚æ­¥ä¸å¹¶è¡Œç­‰\n",
    "\n",
    "å¥½çš„å¼€å‘æ¡†æ¶ï¼Œéœ€è¦å…·å¤‡ä»¥ä¸‹ç‰¹ç‚¹ï¼š\n",
    "\n",
    "1. å¯é æ€§ã€é²æ£’æ€§é«˜\n",
    "2. å¯ç»´æŠ¤æ€§é«˜\n",
    "3. å¯æ‰©å±•æ€§é«˜\n",
    "4. å­¦ä¹ æˆæœ¬ä½\n",
    "\n",
    "ä¸¾äº›é€šä¿—çš„ä¾‹å­ï¼š\n",
    "\n",
    "- ä¸å¤–éƒ¨åŠŸèƒ½è§£ä¾èµ–\n",
    "  - æ¯”å¦‚å¯ä»¥éšæ„æ›´æ¢ LLM è€Œä¸ç”¨å¤§é‡é‡æ„ä»£ç \n",
    "  - æ›´æ¢ä¸‰æ–¹å·¥å…·ä¹ŸåŒç†\n",
    "- ç»å¸¸å˜çš„éƒ¨åˆ†è¦åœ¨å¤–éƒ¨ç»´æŠ¤è€Œä¸æ˜¯æ”¾åœ¨ä»£ç é‡Œ\n",
    "  - æ¯”å¦‚ Prompt æ¨¡æ¿\n",
    "- å„ç§ç¯å¢ƒä¸‹éƒ½é€‚ç”¨\n",
    "  - æ¯”å¦‚çº¿ç¨‹å®‰å…¨\n",
    "- æ–¹ä¾¿è°ƒè¯•å’Œæµ‹è¯•\n",
    "  - è‡³å°‘è¦èƒ½æ„Ÿè§‰åˆ°ç”¨äº†æ¯”ä¸ç”¨æ–¹ä¾¿å§\n",
    "  - åˆæ³•çš„è¾“å…¥ä¸ä¼šå¼•å‘æ¡†æ¶å†…éƒ¨çš„æŠ¥é”™\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>åˆ’é‡ç‚¹ï¼š</b>é€‰å¯¹äº†æ¡†æ¶ï¼Œäº‹åŠåŠŸå€ï¼›åä¹‹ï¼Œäº‹å€åŠŸåŠã€‚\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579b589b-5a4e-451e-baf0-d2a409a9cb4b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "<b>ä»€ä¹ˆæ˜¯ SDK?</b> https://aws.amazon.com/cn/what-is/sdk/\n",
    "<br/>\n",
    "<b>SDK å’Œ API çš„åŒºåˆ«æ˜¯ä»€ä¹ˆ?</b> https://aws.amazon.com/cn/compare/the-difference-between-sdk-and-api/\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30361cf7-ec31-4c45-871e-28ac4f0db1a9",
   "metadata": {},
   "source": [
    "#### ğŸŒ° ä¸¾ä¸ªä¾‹å­ï¼šä½¿ç”¨ SDKï¼Œ4 è¡Œä»£ç å®ç°ä¸€ä¸ªç®€æ˜“çš„ RAG ç³»ç»Ÿ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2517bfe1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<p>LlamaIndex é»˜è®¤çš„ Embedding æ¨¡å‹æ˜¯ <code>OpenAIEmbedding(model=\"text-embedding-ada-002\")</code></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb03dc82-649d-4fe8-8ba5-023220c8cb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade llama-index\n",
    "# !pip install llama-index-llms-dashscope\n",
    "# !pip install llama-index-llms-openai-like\n",
    "# !pip install llama-index-embeddings-dashscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f36875ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.core import Settings\n",
    "# from llama_index.llms.openai_like import OpenAILike\n",
    "from llama_index.llms.dashscope import DashScope, DashScopeGenerationModels\n",
    "from llama_index.embeddings.dashscope import DashScopeEmbedding, DashScopeTextEmbeddingModels\n",
    "\n",
    "# LlamaIndexé»˜è®¤ä½¿ç”¨çš„å¤§æ¨¡å‹è¢«æ›¿æ¢ä¸ºç™¾ç‚¼\n",
    "# Settings.llm = OpenAILike(\n",
    "#     model=\"qwen-max\",\n",
    "#     api_base=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "#     api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "#     is_chat_model=True\n",
    "# )\n",
    "\n",
    "Settings.llm = DashScope(model_name=DashScopeGenerationModels.QWEN_MAX, api_key=os.getenv(\"DASHSCOPE_API_KEY\"))\n",
    "\n",
    "# LlamaIndexé»˜è®¤ä½¿ç”¨çš„Embeddingæ¨¡å‹è¢«æ›¿æ¢ä¸ºç™¾ç‚¼çš„Embeddingæ¨¡å‹\n",
    "Settings.embed_model = DashScopeEmbedding(\n",
    "    # model_name=\"text-embedding-v1\"\n",
    "    model_name=DashScopeTextEmbeddingModels.TEXT_EMBEDDING_V1,\n",
    "    # api_key=os.getenv(\"DASHSCOPE_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dea32ce7-c7a7-4692-a217-45cf632281ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek-V3 æ‹¥æœ‰æ€»è®¡6710äº¿å‚æ•°ï¼Œå…¶ä¸­æ¯ä¸ªä»¤ç‰Œæ¿€æ´»370äº¿å‚æ•°ã€‚\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./data\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"deepseek v3æœ‰å¤šå°‘å‚æ•°ï¼Ÿ\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e4c617-2fb8-489d-a0d0-34d5ca196f1c",
   "metadata": {},
   "source": [
    "## 2. LlamaIndex ä»‹ç»\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313487e1-a2c7-4d4b-ad41-e3a9f0a0b07e",
   "metadata": {},
   "source": [
    "å®˜ç½‘æ ‡é¢˜ï¼š_ã€Œ Build AI Knowledge Assistants over your enterprise data ã€_\n",
    "\n",
    "LlamaIndex æ˜¯ä¸€ä¸ªä¸ºå¼€å‘ã€ŒçŸ¥è¯†å¢å¼ºã€çš„å¤§è¯­è¨€æ¨¡å‹åº”ç”¨çš„æ¡†æ¶ï¼ˆä¹Ÿå°±æ˜¯ SDKï¼‰ã€‚**çŸ¥è¯†å¢å¼º**ï¼Œæ³›æŒ‡ä»»ä½•åœ¨ç§æœ‰æˆ–ç‰¹å®šé¢†åŸŸæ•°æ®åŸºç¡€ä¸Šåº”ç”¨å¤§è¯­è¨€æ¨¡å‹çš„æƒ…å†µã€‚ä¾‹å¦‚ï¼š\n",
    "\n",
    "<img src=\"./assets/basic_rag.png\" width=800px>\n",
    "\n",
    "- Question-Answering Chatbots (ä¹Ÿå°±æ˜¯ RAG)\n",
    "- Document Understanding and Extraction ï¼ˆæ–‡æ¡£ç†è§£ä¸ä¿¡æ¯æŠ½å–ï¼‰\n",
    "\n",
    "- Autonomous Agents that can perform research and take actions ï¼ˆæ™ºèƒ½ä½“åº”ç”¨ï¼‰\n",
    "- Workflow orchestrating single and multi-agent (ç¼–æ’å•ä¸ªæˆ–å¤šä¸ªæ™ºèƒ½ä½“å½¢æˆå·¥ä½œæµï¼‰\n",
    "\n",
    "LlamaIndex æœ‰ Python å’Œ Typescript ä¸¤ä¸ªç‰ˆæœ¬ï¼ŒPython ç‰ˆçš„æ–‡æ¡£ç›¸å¯¹æ›´å®Œå–„ã€‚\n",
    "\n",
    "- Python æ–‡æ¡£åœ°å€ï¼šhttps://docs.llamaindex.ai/en/stable/\n",
    "- Python API æ¥å£æ–‡æ¡£ï¼šhttps://docs.llamaindex.ai/en/stable/api_reference/\n",
    "\n",
    "- TS æ–‡æ¡£åœ°å€ï¼šhttps://ts.llamaindex.ai/\n",
    "\n",
    "LlamaIndex æ˜¯ä¸€ä¸ªå¼€æºæ¡†æ¶ï¼ŒGithub é“¾æ¥ï¼šhttps://github.com/run-llama\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4822b2-52d5-411c-8244-3432f8733da2",
   "metadata": {},
   "source": [
    "### LlamaIndex çš„æ ¸å¿ƒæ¨¡å—\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c474b43-018a-4687-a51e-54b391a6bbca",
   "metadata": {},
   "source": [
    "<img src=\"./assets/llamaindex.png\" alt=\"LlamaIndex æ ¸å¿ƒæ¨¡å—\" width=\"1400px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca66bf37-48e4-45eb-9e7e-a24e86108ac1",
   "metadata": {},
   "source": [
    "### å®‰è£… LlamaIndex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80bba90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ï¼pip install llama-index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74341a28-fb7f-4e5b-9342-cf5cd850654a",
   "metadata": {},
   "source": [
    "## 3.æ•°æ®åŠ è½½ï¼ˆLoadingï¼‰\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d134a56-ae9a-41a5-a703-ce9dfb3fc600",
   "metadata": {},
   "source": [
    "### 3.1ã€åŠ è½½æœ¬åœ°æ•°æ®\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35bfa25-9a09-4c40-afa1-9f9156e56dc2",
   "metadata": {},
   "source": [
    "`SimpleDirectoryReader` æ˜¯ä¸€ä¸ªç®€å•çš„æœ¬åœ°æ–‡ä»¶åŠ è½½å™¨ã€‚å®ƒä¼šéå†æŒ‡å®šç›®å½•ï¼Œå¹¶æ ¹æ®æ–‡ä»¶æ‰©å±•åè‡ªåŠ¨åŠ è½½æ–‡ä»¶ï¼ˆ**æ–‡æœ¬å†…å®¹**ï¼‰ã€‚\n",
    "\n",
    "æ”¯æŒçš„æ–‡ä»¶ç±»å‹ï¼š\n",
    "\n",
    "- `.csv` - comma-separated values\n",
    "- `.docx` - Microsoft Word\n",
    "- `.epub` - EPUB ebook format\n",
    "- `.hwp` - Hangul Word Processor\n",
    "- `.ipynb` - Jupyter Notebook\n",
    "- `.jpeg`, `.jpg` - JPEG image\n",
    "- `.mbox` - MBOX email archive\n",
    "- `.md` - Markdown\n",
    "- `.mp3`, `.mp4` - audio and video\n",
    "- `.pdf` - Portable Document Format\n",
    "- `.png` - Portable Network Graphics\n",
    "- `.ppt`, `.pptm`, `.pptx` - Microsoft PowerPoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05b6a279-3eca-4685-af91-80fb443fca1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pydantic.v1 import BaseModel\n",
    "\n",
    "def show_json(data):\n",
    "    \"\"\"ç”¨äºå±•ç¤ºjsonæ•°æ®\"\"\"\n",
    "    if isinstance(data, str):\n",
    "        obj = json.loads(data)\n",
    "        print(json.dumps(obj, indent=4, ensure_ascii=False))\n",
    "    elif isinstance(data, dict) or isinstance(data, list):\n",
    "        print(json.dumps(data, indent=4, ensure_ascii=False))\n",
    "    elif issubclass(type(data), BaseModel):\n",
    "        print(json.dumps(data.dict(), indent=4, ensure_ascii=False))\n",
    "\n",
    "def show_list_obj(data):\n",
    "    \"\"\"ç”¨äºå±•ç¤ºä¸€ç»„å¯¹è±¡\"\"\"\n",
    "    if isinstance(data, list):\n",
    "        for item in data:\n",
    "            show_json(item)\n",
    "    else:\n",
    "        raise ValueError(\"Input is not a list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "198ebc20-fab6-46a9-8cca-4b114fe2640f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "reader = SimpleDirectoryReader(\n",
    "        input_dir=\"./data\", # ç›®æ ‡ç›®å½•\n",
    "        recursive=False, # æ˜¯å¦é€’å½’éå†å­ç›®å½•\n",
    "        required_exts=[\".pdf\"] # (å¯é€‰)åªè¯»å–æŒ‡å®šåç¼€çš„æ–‡ä»¶\n",
    "    )\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfb3f1ac-75cb-4c5e-b0a1-53621d8d325e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek-V3 Technical Report\n",
      "DeepSeek-AI\n",
      "research@deepseek.com\n",
      "Abstract\n",
      "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total\n",
      "parameters with 37B activated for each token. To achieve efficient inference and cost-effective\n",
      "training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architec-\n",
      "tures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers\n",
      "an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training\n",
      "objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and\n",
      "high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to\n",
      "fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms\n",
      "other open-source models and achieves performance comparable to leading closed-source\n",
      "models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours\n",
      "for its full training. In addition, its training process is remarkably stable. Throughout the entire\n",
      "training process, we did not experience any irrecoverable loss spikes or perform any rollbacks.\n",
      "The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.\n",
      "MMLU-Pro\n",
      "(EM)\n",
      "GPQA-Diamond\n",
      "(Pass@1)\n",
      "MATH 500\n",
      "(EM)\n",
      "AIME 2024\n",
      "(Pass@1)\n",
      "Codeforces\n",
      "(Percentile)\n",
      "SWE-bench Verified\n",
      "(Resolved)\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100Accuracy / Percentile (%)\n",
      "75.9\n",
      "59.1\n",
      "90.2\n",
      "39.2\n",
      "51.6\n",
      "42.0\n",
      "66.2\n",
      "41.3\n",
      "74.7\n",
      "16.7\n",
      "35.6\n",
      "22.6\n",
      "71.6\n",
      "49.0\n",
      "80.0\n",
      "23.3 24.8 23.8\n",
      "73.3\n",
      "51.1\n",
      "73.8\n",
      "23.3\n",
      "25.3 24.5\n",
      "72.6\n",
      "49.9\n",
      "74.6\n",
      "9.3\n",
      "23.6\n",
      "38.8\n",
      "78.0\n",
      "65.0\n",
      "78.3\n",
      "16.0\n",
      "20.3\n",
      "50.8\n",
      "DeepSeek-V3 DeepSeek-V2.5 Qwen2.5-72B-Inst Llama-3.1-405B-Inst GPT-4o-0513 Claude-3.5-Sonnet-1022\n",
      "Figure 1 | Benchmark performance of DeepSeek-V3 and its counterparts.\n",
      "arXiv:2412.19437v2  [cs.CL]  18 Feb 2025\n",
      "{\n",
      "    \"id_\": \"476d1f3d-15ab-45c6-9174-72ca95a835cf\",\n",
      "    \"embedding\": null,\n",
      "    \"metadata\": {\n",
      "        \"page_label\": \"1\",\n",
      "        \"file_name\": \"deepseek-v3-1-4.pdf\",\n",
      "        \"file_path\": \"D:\\\\github-code\\\\jukeai\\\\ç¬¬4ç« _LlamaIndexçŸ¥è¯†ç®¡ç†ä¸ä¿¡æ¯æ£€ç´¢\\\\data\\\\deepseek-v3-1-4.pdf\",\n",
      "        \"file_type\": \"application/pdf\",\n",
      "        \"file_size\": 192218,\n",
      "        \"creation_date\": \"2026-01-26\",\n",
      "        \"last_modified_date\": \"2025-03-12\"\n",
      "    },\n",
      "    \"excluded_embed_metadata_keys\": [\n",
      "        \"file_name\",\n",
      "        \"file_type\",\n",
      "        \"file_size\",\n",
      "        \"creation_date\",\n",
      "        \"last_modified_date\",\n",
      "        \"last_accessed_date\"\n",
      "    ],\n",
      "    \"excluded_llm_metadata_keys\": [\n",
      "        \"file_name\",\n",
      "        \"file_type\",\n",
      "        \"file_size\",\n",
      "        \"creation_date\",\n",
      "        \"last_modified_date\",\n",
      "        \"last_accessed_date\"\n",
      "    ],\n",
      "    \"relationships\": {},\n",
      "    \"metadata_template\": \"{key}: {value}\",\n",
      "    \"metadata_separator\": \"\\n\",\n",
      "    \"text_resource\": {\n",
      "        \"embeddings\": null,\n",
      "        \"text\": \"DeepSeek-V3 Technical Report\\nDeepSeek-AI\\nresearch@deepseek.com\\nAbstract\\nWe present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total\\nparameters with 37B activated for each token. To achieve efficient inference and cost-effective\\ntraining, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architec-\\ntures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers\\nan auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training\\nobjective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and\\nhigh-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to\\nfully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms\\nother open-source models and achieves performance comparable to leading closed-source\\nmodels. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours\\nfor its full training. In addition, its training process is remarkably stable. Throughout the entire\\ntraining process, we did not experience any irrecoverable loss spikes or perform any rollbacks.\\nThe model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.\\nMMLU-Pro\\n(EM)\\nGPQA-Diamond\\n(Pass@1)\\nMATH 500\\n(EM)\\nAIME 2024\\n(Pass@1)\\nCodeforces\\n(Percentile)\\nSWE-bench Verified\\n(Resolved)\\n0\\n20\\n40\\n60\\n80\\n100Accuracy / Percentile (%)\\n75.9\\n59.1\\n90.2\\n39.2\\n51.6\\n42.0\\n66.2\\n41.3\\n74.7\\n16.7\\n35.6\\n22.6\\n71.6\\n49.0\\n80.0\\n23.3 24.8 23.8\\n73.3\\n51.1\\n73.8\\n23.3\\n25.3 24.5\\n72.6\\n49.9\\n74.6\\n9.3\\n23.6\\n38.8\\n78.0\\n65.0\\n78.3\\n16.0\\n20.3\\n50.8\\nDeepSeek-V3 DeepSeek-V2.5 Qwen2.5-72B-Inst Llama-3.1-405B-Inst GPT-4o-0513 Claude-3.5-Sonnet-1022\\nFigure 1 | Benchmark performance of DeepSeek-V3 and its counterparts.\\narXiv:2412.19437v2  [cs.CL]  18 Feb 2025\",\n",
      "        \"path\": null,\n",
      "        \"url\": null,\n",
      "        \"mimetype\": null\n",
      "    },\n",
      "    \"image_resource\": null,\n",
      "    \"audio_resource\": null,\n",
      "    \"video_resource\": null,\n",
      "    \"text_template\": \"{metadata_str}\\n\\n{content}\",\n",
      "    \"class_name\": \"Document\",\n",
      "    \"text\": \"DeepSeek-V3 Technical Report\\nDeepSeek-AI\\nresearch@deepseek.com\\nAbstract\\nWe present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total\\nparameters with 37B activated for each token. To achieve efficient inference and cost-effective\\ntraining, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architec-\\ntures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers\\nan auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training\\nobjective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and\\nhigh-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to\\nfully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms\\nother open-source models and achieves performance comparable to leading closed-source\\nmodels. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours\\nfor its full training. In addition, its training process is remarkably stable. Throughout the entire\\ntraining process, we did not experience any irrecoverable loss spikes or perform any rollbacks.\\nThe model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.\\nMMLU-Pro\\n(EM)\\nGPQA-Diamond\\n(Pass@1)\\nMATH 500\\n(EM)\\nAIME 2024\\n(Pass@1)\\nCodeforces\\n(Percentile)\\nSWE-bench Verified\\n(Resolved)\\n0\\n20\\n40\\n60\\n80\\n100Accuracy / Percentile (%)\\n75.9\\n59.1\\n90.2\\n39.2\\n51.6\\n42.0\\n66.2\\n41.3\\n74.7\\n16.7\\n35.6\\n22.6\\n71.6\\n49.0\\n80.0\\n23.3 24.8 23.8\\n73.3\\n51.1\\n73.8\\n23.3\\n25.3 24.5\\n72.6\\n49.9\\n74.6\\n9.3\\n23.6\\n38.8\\n78.0\\n65.0\\n78.3\\n16.0\\n20.3\\n50.8\\nDeepSeek-V3 DeepSeek-V2.5 Qwen2.5-72B-Inst Llama-3.1-405B-Inst GPT-4o-0513 Claude-3.5-Sonnet-1022\\nFigure 1 | Benchmark performance of DeepSeek-V3 and its counterparts.\\narXiv:2412.19437v2  [cs.CL]  18 Feb 2025\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(documents[0].text)\n",
    "show_json(documents[0].json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277be7f4-1993-4b74-bd3e-f9d8cb5a827d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>æ³¨æ„ï¼š</b>å¯¹å›¾åƒã€è§†é¢‘ã€è¯­éŸ³ç±»æ–‡ä»¶ï¼Œé»˜è®¤ä¸ä¼šè‡ªåŠ¨æå–å…¶ä¸­æ–‡å­—ã€‚å¦‚éœ€æå–ï¼Œå‚è€ƒä¸‹é¢ä»‹ç»çš„ <code>Data Connectors</code>ã€‚\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c98851-0858-4215-858d-bea70e310d5f",
   "metadata": {},
   "source": [
    "é»˜è®¤çš„ `PDFReader` æ•ˆæœå¹¶ä¸ç†æƒ³ï¼Œæˆ‘ä»¬å¯ä»¥æ›´æ¢æ–‡ä»¶åŠ è½½å™¨\n",
    "\n",
    "<b>LlamaParse</b>\n",
    "\n",
    "é¦–å…ˆï¼Œç™»å½•å¹¶ä» https://cloud.llamaindex.ai â†— æ³¨å†Œå¹¶è·å– api-key ã€‚\n",
    "\n",
    "ç„¶åï¼Œå®‰è£…è¯¥åŒ…ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25c97cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-cloud-services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b093a88b-5d4a-423b-bfe7-fa28b34744e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åœ¨ç³»ç»Ÿç¯å¢ƒå˜é‡é‡Œé…ç½® LLAMA_CLOUD_API_KEY=XXX\n",
    "\n",
    "from llama_cloud_services import LlamaParse\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply() # åªåœ¨Jupyterç¬”è®°ç¯å¢ƒä¸­éœ€è¦æ­¤æ“ä½œï¼Œå¦åˆ™ä¼šæŠ¥é”™\n",
    "\n",
    "# set up parser\n",
    "parser = LlamaParse(\n",
    "    result_type=\"markdown\"  # \"markdown\" and \"text\" are available\n",
    ")\n",
    "file_extractor = {\".pdf\": parser}\n",
    "\n",
    "documents = SimpleDirectoryReader(input_dir=\"./data\", required_exts=[\".pdf\"], file_extractor=file_extractor).load_data()\n",
    "print(documents[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a116d296-e4a4-4710-a41a-318bbab8ec24",
   "metadata": {},
   "source": [
    "### 3.2ã€Data Connectors\n",
    "\n",
    "ç”¨äºå¤„ç†æ›´ä¸°å¯Œçš„æ•°æ®ç±»å‹ï¼Œå¹¶å°†å…¶è¯»å–ä¸º `Document` çš„å½¢å¼ã€‚\n",
    "\n",
    "ä¾‹å¦‚ï¼šç›´æ¥è¯»å–ç½‘é¡µ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19471210-6cb3-4822-8bad-dc701b6ab335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-index-readers-web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d73abf3-91df-488c-a5f3-c7de0c6d4c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "\n",
    "documents = SimpleWebPageReader(html_to_text=True).load_data(\n",
    "    [\"https://edu.guangjuke.com/tx/\"]\n",
    ")\n",
    "\n",
    "print(documents[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a88893-01b8-44a1-9bed-e93e60cec328",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>æ›´å¤š Data Connectors</b>\n",
    "    <ul>\n",
    "        <li>å†…ç½®çš„<a href=\"https://llamahub.ai/l/readers/llama-index-readers-file\">æ–‡ä»¶åŠ è½½å™¨</a></li>\n",
    "        <li>è¿æ¥ä¸‰æ–¹æœåŠ¡çš„<a href=\"https://docs.llamaindex.ai/en/stable/module_guides/loading/connector/modules/\">æ•°æ®åŠ è½½å™¨</a>ï¼Œä¾‹å¦‚æ•°æ®åº“</li>\n",
    "        <li>æ›´å¤šåŠ è½½å™¨å¯ä»¥åœ¨ <a href=\"https://llamahub.ai/\">LlamaHub</a> ä¸Šæ‰¾åˆ°</li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ed8a42-f6fc-430f-9e92-07736e7f359c",
   "metadata": {},
   "source": [
    "## 4. æ–‡æœ¬åˆ‡åˆ†ä¸è§£æï¼ˆChunkingï¼‰\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ad7ebd-9e56-47f9-8136-2e1098139c01",
   "metadata": {},
   "source": [
    "ä¸ºæ–¹ä¾¿æ£€ç´¢ï¼Œæˆ‘ä»¬é€šå¸¸æŠŠ `Document` åˆ‡åˆ†ä¸º `Node`ã€‚\n",
    "\n",
    "åœ¨ LlamaIndex ä¸­ï¼Œ`Node` è¢«å®šä¹‰ä¸ºä¸€ä¸ªæ–‡æœ¬çš„ã€Œchunkã€ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4881c7d9-9107-4704-b322-d3bc34af96f2",
   "metadata": {},
   "source": [
    "### 4.1ã€ä½¿ç”¨ TextSplitters å¯¹æ–‡æœ¬åšåˆ‡åˆ†\n",
    "\n",
    "ä¾‹å¦‚ï¼š`TokenTextSplitter` æŒ‰æŒ‡å®š token æ•°åˆ‡åˆ†æ–‡æœ¬\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a083f7-cda9-45d9-be3e-397ce866e440",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "\n",
    "node_parser = TokenTextSplitter(\n",
    "    chunk_size=512,  # æ¯ä¸ª chunk çš„æœ€å¤§é•¿åº¦\n",
    "    chunk_overlap=200  # chunk ä¹‹é—´é‡å é•¿åº¦\n",
    ")\n",
    "\n",
    "nodes = node_parser.get_nodes_from_documents(\n",
    "    documents, show_progress=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b363de-c5ef-4e20-ad4d-39bccabb3369",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_json(nodes[1].json())\n",
    "show_json(nodes[2].json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f56dbf-4247-4a06-a127-2fa1929455f0",
   "metadata": {},
   "source": [
    "LlamaIndex æä¾›äº†ä¸°å¯Œçš„ `TextSplitter`ï¼Œä¾‹å¦‚ï¼š\n",
    "\n",
    "- [`SentenceSplitter`](https://docs.llamaindex.ai/en/stable/api_reference/node_parsers/sentence_splitter/)ï¼šåœ¨åˆ‡åˆ†æŒ‡å®šé•¿åº¦çš„ chunk åŒæ—¶å°½é‡ä¿è¯å¥å­è¾¹ç•Œä¸è¢«åˆ‡æ–­ï¼›\n",
    "- [`CodeSplitter`](https://docs.llamaindex.ai/en/stable/api_reference/node_parsers/code/)ï¼šæ ¹æ® ASTï¼ˆç¼–è¯‘å™¨çš„æŠ½è±¡å¥æ³•æ ‘ï¼‰åˆ‡åˆ†ä»£ç ï¼Œä¿è¯ä»£ç åŠŸèƒ½ç‰‡æ®µå®Œæ•´ï¼›\n",
    "- [`SemanticSplitterNodeParser`](https://docs.llamaindex.ai/en/stable/api_reference/node_parsers/semantic_splitter/)ï¼šæ ¹æ®è¯­ä¹‰ç›¸å…³æ€§å¯¹å°†æ–‡æœ¬åˆ‡åˆ†ä¸ºç‰‡æ®µã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b692f9fb-af99-4bf5-9d4e-c745438173d6",
   "metadata": {},
   "source": [
    "### 4.2ã€ä½¿ç”¨ NodeParsers å¯¹æœ‰ç»“æ„çš„æ–‡æ¡£åšè§£æ\n",
    "\n",
    "ä¾‹å¦‚ï¼š`HTMLNodeParser`è§£æ HTML æ–‡æ¡£\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c6f533-a203-42a2-b312-3d89e0cbf22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import HTMLNodeParser\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "\n",
    "documents = SimpleWebPageReader(html_to_text=False).load_data(\n",
    "    [\"https://edu.guangjuke.com/tx/\"]\n",
    ")\n",
    "\n",
    "# é»˜è®¤è§£æ [\"p\", \"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"li\", \"b\", \"i\", \"u\", \"section\"]\n",
    "parser = HTMLNodeParser(tags=[\"span\"])  # å¯ä»¥è‡ªå®šä¹‰è§£æå“ªäº›æ ‡ç­¾\n",
    "nodes = parser.get_nodes_from_documents(documents)\n",
    "\n",
    "for node in nodes:\n",
    "    print(node.text+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdc613c-36a8-4afb-871b-097496703eaf",
   "metadata": {},
   "source": [
    "æ›´å¤šçš„ `NodeParser` åŒ…æ‹¬ [`MarkdownNodeParser`](https://docs.llamaindex.ai/en/stable/api_reference/node_parsers/markdown/)ï¼Œ[`JSONNodeParser`](https://docs.llamaindex.ai/en/stable/api_reference/node_parsers/json/)ç­‰ç­‰ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b8557f-20af-477d-918d-14761a9c986d",
   "metadata": {},
   "source": [
    "## 5. ç´¢å¼•ï¼ˆIndexingï¼‰ä¸æ£€ç´¢ï¼ˆRetrievalï¼‰\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df338b16-df37-412d-a385-2d1f4b681112",
   "metadata": {},
   "source": [
    "**åŸºç¡€æ¦‚å¿µ**ï¼šåœ¨ã€Œæ£€ç´¢ã€ç›¸å…³çš„ä¸Šä¸‹æ–‡ä¸­ï¼Œã€Œç´¢å¼•ã€å³`index`ï¼Œ é€šå¸¸æ˜¯æŒ‡ä¸ºäº†å®ç°å¿«é€Ÿæ£€ç´¢è€Œè®¾è®¡çš„ç‰¹å®šã€Œæ•°æ®ç»“æ„ã€ã€‚\n",
    "\n",
    "ç´¢å¼•çš„å…·ä½“åŸç†ä¸å®ç°ä¸æ˜¯æœ¬è¯¾ç¨‹çš„æ•™å­¦é‡ç‚¹ï¼Œæ„Ÿå…´è¶£çš„åŒå­¦å¯ä»¥å‚è€ƒï¼š[ä¼ ç»Ÿç´¢å¼•](https://en.wikipedia.org/wiki/Search_engine_indexing)ã€[å‘é‡ç´¢å¼•](https://medium.com/kx-systems/vector-indexing-a-roadmap-for-vector-databases-65866f07daf5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd397fe-8932-49de-ac37-bed0b585205c",
   "metadata": {},
   "source": [
    "### 5.1ã€å‘é‡æ£€ç´¢\n",
    "\n",
    "1. `VectorStoreIndex` ç›´æ¥åœ¨å†…å­˜ä¸­æ„å»ºä¸€ä¸ª Vector Store å¹¶å»ºç´¢å¼•\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ea17e80-d25c-43ac-b9b5-983c6acb3adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verification and reflection patterns of R1 into DeepSeek-V3 and notably improves its\n",
      "reasoning performance. Meanwhile, we also maintain control over the output style and\n",
      "length of DeepSeek-V3.\n",
      "Summary of Core Evaluation Results\n",
      "â€¢ Knowledge: (1) On educational benchmarks such as MMLU, MMLU-Pro, and GPQA,\n",
      "DeepSeek-V3 outperforms all other open-source models, achieving 88.5 on MMLU, 75.9\n",
      "on MMLU-Pro, and 59.1 on GPQA. Its performance is comparable to leading closed-source\n",
      "models like GPT-4o and Claude-Sonnet-3.5, narrowing the gap between open-source\n",
      "and closed-source models in this domain. (2) For factuality benchmarks, DeepSeek-V3\n",
      "demonstrates superior performance among open-source models on both SimpleQA and\n",
      "Chinese SimpleQA. While it trails behind GPT-4o and Claude-Sonnet-3.5 in English factual\n",
      "knowledge (SimpleQA), it surpasses these models in Chinese factual knowledge (Chinese\n",
      "SimpleQA), highlighting its strength in Chinese factual knowledge.\n",
      "â€¢ Code, Math, and Reasoning: (1) DeepSeek-V3 achieves state-of-the-art performance on\n",
      "math-related benchmarks among all non-long-CoT open-source and closed-source models.\n",
      "Notably, it even outperforms o1-preview on specific benchmarks, such as MATH-500,\n",
      "demonstrating its robust mathematical reasoning capabilities. (2) On coding-related tasks,\n",
      "DeepSeek-V3 emerges as the top-performing model for coding competition benchmarks,\n",
      "such as LiveCodeBench, solidifying its position as the leading model in this domain. For\n",
      "engineering-related tasks, while DeepSeek-V3 performs slightly below Claude-Sonnet-3.5,\n",
      "it still outpaces all other models by a significant margin, demonstrating its competitiveness\n",
      "across diverse technical benchmarks.\n",
      "In the remainder of this paper, we first present a detailed exposition of our DeepSeek-V3\n",
      "model architecture (Section 2). Subsequently, we introduce our infrastructures, encompassing\n",
      "our compute clusters, the training framework, the support for FP8 training, the inference\n",
      "deployment strategy, and our suggestions on future hardware design. Next,\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import TokenTextSplitter, SentenceSplitter\n",
    "\n",
    "# åŠ è½½ pdf æ–‡æ¡£\n",
    "documents = SimpleDirectoryReader(\n",
    "    \"./data\", \n",
    "    required_exts=[\".pdf\"],\n",
    ").load_data()\n",
    "\n",
    "# å®šä¹‰ Node Parser\n",
    "node_parser = TokenTextSplitter(chunk_size=512, chunk_overlap=200)\n",
    "\n",
    "# åˆ‡åˆ†æ–‡æ¡£\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "\n",
    "# æ„å»º indexï¼Œé»˜è®¤æ˜¯åœ¨å†…å­˜ä¸­\n",
    "index = VectorStoreIndex(nodes)\n",
    "\n",
    "# å¦å¤–ä¸€ç§å®ç°æ–¹å¼\n",
    "# index = VectorStoreIndex.from_documents(documents=documents, transformations=[SentenceSplitter(chunk_size=512)])\n",
    "\n",
    "# å†™å…¥æœ¬åœ°æ–‡ä»¶\n",
    "# index.storage_context.persist(persist_dir=\"./doc_emb\")\n",
    "\n",
    "# è·å– retriever\n",
    "vector_retriever = index.as_retriever(\n",
    "    similarity_top_k=2 # è¿”å›2ä¸ªç»“æœ\n",
    ")\n",
    "\n",
    "# æ£€ç´¢\n",
    "results = vector_retriever.retrieve(\"deepseek v3æ•°å­¦èƒ½åŠ›æ€ä¹ˆæ ·ï¼Ÿ\")\n",
    "\n",
    "print(results[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ebec20-7c12-4d2d-a4f4-5cb2abcc5f32",
   "metadata": {},
   "source": [
    "2. ä½¿ç”¨è‡ªå®šä¹‰çš„ Vector Storeï¼Œä»¥ `Qdrant` ä¸ºä¾‹ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5e7648-0598-4df7-923f-42fe2f172da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-index-vector-stores-qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a65a38da-3e71-4fa8-872d-eff6b4f3856b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node ID: 72cffc27-1f56-41c8-8c14-f8745f48755a\n",
      "Text: verification and reflection patterns of R1 into DeepSeek-V3 and\n",
      "notably improves its reasoning performance. Meanwhile, we also\n",
      "maintain control over the output style and length of DeepSeek-V3.\n",
      "Summary of Core Evaluation Results â€¢ Knowledge: (1) On educational\n",
      "benchmarks such as MMLU, MMLU-Pro, and GPQA, DeepSeek-V3 outperforms\n",
      "all other open-sou...\n",
      "Score:  0.687\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.indices.vector_store.base import VectorStoreIndex\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance\n",
    "\n",
    "client = QdrantClient(location=\":memory:\")\n",
    "collection_name = \"demo\"\n",
    "collection = client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE)\n",
    ")\n",
    "\n",
    "vector_store = QdrantVectorStore(client=client, collection_name=collection_name)\n",
    "# storage: æŒ‡å®šå­˜å‚¨ç©ºé—´\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# åˆ›å»º indexï¼šé€šè¿‡ Storage Context å…³è”åˆ°è‡ªå®šä¹‰çš„ Vector Store\n",
    "index = VectorStoreIndex(nodes, storage_context=storage_context)\n",
    "\n",
    "# è·å– retriever\n",
    "vector_retriever = index.as_retriever(similarity_top_k=1)\n",
    "\n",
    "# æ£€ç´¢\n",
    "results = vector_retriever.retrieve(\"deepseek v3æ•°å­¦èƒ½åŠ›æ€ä¹ˆæ ·\")\n",
    "\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9913e5d5-7c84-4833-b21d-2fe440749a63",
   "metadata": {},
   "source": [
    "### 5.2ã€æ›´å¤šç´¢å¼•ä¸æ£€ç´¢æ–¹å¼\n",
    "\n",
    "LlamaIndex å†…ç½®äº†ä¸°å¯Œçš„æ£€ç´¢æœºåˆ¶ï¼Œä¾‹å¦‚ï¼š\n",
    "\n",
    "- å…³é”®å­—æ£€ç´¢\n",
    "\n",
    "  - [`BM25Retriever`](https://docs.llamaindex.ai/en/stable/api_reference/retrievers/bm25/)ï¼šåŸºäº tokenizer å®ç°çš„ BM25 ç»å…¸æ£€ç´¢ç®—æ³•\n",
    "  - [`KeywordTableGPTRetriever`](https://docs.llamaindex.ai/en/stable/api_reference/retrievers/keyword/#llama_index.core.indices.keyword_table.retrievers.KeywordTableGPTRetriever)ï¼šä½¿ç”¨ GPT æå–æ£€ç´¢å…³é”®å­—\n",
    "  - [`KeywordTableSimpleRetriever`](https://docs.llamaindex.ai/en/stable/api_reference/retrievers/keyword/#llama_index.core.indices.keyword_table.retrievers.KeywordTableSimpleRetriever)ï¼šä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æ£€ç´¢å…³é”®å­—\n",
    "  - [`KeywordTableRAKERetriever`](https://docs.llamaindex.ai/en/stable/api_reference/retrievers/keyword/#llama_index.core.indices.keyword_table.retrievers.KeywordTableRAKERetriever)ï¼šä½¿ç”¨[`RAKE`](https://pypi.org/project/rake-nltk/)ç®—æ³•æå–æ£€ç´¢å…³é”®å­—ï¼ˆæœ‰è¯­è¨€é™åˆ¶ï¼‰\n",
    "\n",
    "- RAG-Fusion [`QueryFusionRetriever`](https://docs.llamaindex.ai/en/stable/api_reference/retrievers/query_fusion/)\n",
    "\n",
    "- è¿˜æ”¯æŒ [KnowledgeGraph](https://docs.llamaindex.ai/en/stable/api_reference/retrievers/knowledge_graph/)ã€[SQL](https://docs.llamaindex.ai/en/stable/api_reference/retrievers/sql/#llama_index.core.retrievers.SQLRetriever)ã€[Text-to-SQL](https://docs.llamaindex.ai/en/stable/api_reference/retrievers/sql/#llama_index.core.retrievers.NLSQLRetriever) ç­‰ç­‰\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af53e99b-2ab5-4520-ba96-4a9979a94480",
   "metadata": {},
   "source": [
    "### 5.3ã€æ£€ç´¢åå¤„ç†\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c53c7c-9077-42bc-a5c0-832688b352b8",
   "metadata": {},
   "source": [
    "LlamaIndex çš„ `Node Postprocessors` æä¾›äº†ä¸€ç³»åˆ—æ£€ç´¢åå¤„ç†æ¨¡å—ã€‚\n",
    "\n",
    "ä¾‹å¦‚ï¼šæˆ‘ä»¬å¯ä»¥ç”¨ä¸åŒæ¨¡å‹å¯¹æ£€ç´¢åçš„ `Nodes` åšé‡æ’åº\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7176f29c-be6b-491c-b2e8-6e604ad201b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] evaluations, and discussions (Section 5). Lastly,\n",
      "we conclude this work, discuss existing limitations of DeepSeek-V3, and propose potential\n",
      "directions for future research (Section 6).\n",
      "2. Architecture\n",
      "We first introduce the basic architecture of DeepSeek-V3, featured by Multi-head Latent Atten-\n",
      "tion (MLA) (DeepSeek-AI, 2024c) for efficient inference and DeepSeekMoE (Dai et al., 2024)\n",
      "for economical training. Then, we present a Multi-Token Prediction (MTP) training objective,\n",
      "which we have observed to enhance the overall performance on evaluation benchmarks. For\n",
      "other minor details not explicitly mentioned, DeepSeek-V3 adheres to the settings of DeepSeek-\n",
      "V2 (DeepSeek-AI, 2024c).\n",
      "2.1. Basic Architecture\n",
      "The basic architecture of DeepSeek-V3 is still within the Transformer (Vaswani et al., 2017)\n",
      "framework. For efficient inference and economical training, DeepSeek-V3 also adopts MLA\n",
      "and DeepSeekMoE, which have been thoroughly validated by DeepSeek-V2. Compared with\n",
      "DeepSeek-V2, an exception is that we additionally introduce an auxiliary-loss-free load balancing\n",
      "6\n",
      "\n",
      "[1] DeepSeek-V3 Technical Report\n",
      "DeepSeek-AI\n",
      "research@deepseek.com\n",
      "Abstract\n",
      "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total\n",
      "parameters with 37B activated for each token. To achieve efficient inference and cost-effective\n",
      "training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architec-\n",
      "tures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers\n",
      "an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training\n",
      "objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and\n",
      "high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to\n",
      "fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms\n",
      "other open-source models and achieves performance comparable to leading closed-source\n",
      "models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours\n",
      "for its full training. In addition, its training process is remarkably stable. Throughout the entire\n",
      "training process, we did not experience any irrecoverable loss spikes or perform any rollbacks.\n",
      "The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.\n",
      "MMLU-Pro\n",
      "(EM)\n",
      "GPQA-Diamond\n",
      "(Pass@1)\n",
      "MATH 500\n",
      "(EM)\n",
      "AIME 2024\n",
      "(Pass@1)\n",
      "Codeforces\n",
      "(Percentile)\n",
      "SWE-bench Verified\n",
      "(Resolved)\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100Accuracy / Percentile (%)\n",
      "75.9\n",
      "59.1\n",
      "90.2\n",
      "39.2\n",
      "51.6\n",
      "42.0\n",
      "66.2\n",
      "41.3\n",
      "74.7\n",
      "16.7\n",
      "35.6\n",
      "22.6\n",
      "71.6\n",
      "49.0\n",
      "80.0\n",
      "23.3 24.8 23.8\n",
      "73.3\n",
      "51.1\n",
      "73.8\n",
      "23.3\n",
      "25.3\n",
      "\n",
      "[2] verification and reflection patterns of R1 into DeepSeek-V3 and notably improves its\n",
      "reasoning performance. Meanwhile, we also maintain control over the output style and\n",
      "length of DeepSeek-V3.\n",
      "Summary of Core Evaluation Results\n",
      "â€¢ Knowledge: (1) On educational benchmarks such as MMLU, MMLU-Pro, and GPQA,\n",
      "DeepSeek-V3 outperforms all other open-source models, achieving 88.5 on MMLU, 75.9\n",
      "on MMLU-Pro, and 59.1 on GPQA. Its performance is comparable to leading closed-source\n",
      "models like GPT-4o and Claude-Sonnet-3.5, narrowing the gap between open-source\n",
      "and closed-source models in this domain. (2) For factuality benchmarks, DeepSeek-V3\n",
      "demonstrates superior performance among open-source models on both SimpleQA and\n",
      "Chinese SimpleQA. While it trails behind GPT-4o and Claude-Sonnet-3.5 in English factual\n",
      "knowledge (SimpleQA), it surpasses these models in Chinese factual knowledge (Chinese\n",
      "SimpleQA), highlighting its strength in Chinese factual knowledge.\n",
      "â€¢ Code, Math, and Reasoning: (1) DeepSeek-V3 achieves state-of-the-art performance on\n",
      "math-related benchmarks among all non-long-CoT open-source and closed-source models.\n",
      "Notably, it even outperforms o1-preview on specific benchmarks, such as MATH-500,\n",
      "demonstrating its robust mathematical reasoning capabilities. (2) On coding-related tasks,\n",
      "DeepSeek-V3 emerges as the top-performing model for coding competition benchmarks,\n",
      "such as LiveCodeBench, solidifying its position as the leading model in this domain. For\n",
      "engineering-related tasks, while DeepSeek-V3 performs slightly below Claude-Sonnet-3.5,\n",
      "it still outpaces all other models by a significant margin, demonstrating its competitiveness\n",
      "across diverse technical benchmarks.\n",
      "In the remainder of this paper, we first present a detailed exposition of our DeepSeek-V3\n",
      "model architecture (Section 2). Subsequently, we introduce our infrastructures, encompassing\n",
      "our compute clusters, the training framework, the support for FP8 training, the inference\n",
      "deployment strategy, and our suggestions on future hardware design. Next,\n",
      "\n",
      "[3] open-source and closed-source models.\n",
      "Notably, it even outperforms o1-preview on specific benchmarks, such as MATH-500,\n",
      "demonstrating its robust mathematical reasoning capabilities. (2) On coding-related tasks,\n",
      "DeepSeek-V3 emerges as the top-performing model for coding competition benchmarks,\n",
      "such as LiveCodeBench, solidifying its position as the leading model in this domain. For\n",
      "engineering-related tasks, while DeepSeek-V3 performs slightly below Claude-Sonnet-3.5,\n",
      "it still outpaces all other models by a significant margin, demonstrating its competitiveness\n",
      "across diverse technical benchmarks.\n",
      "In the remainder of this paper, we first present a detailed exposition of our DeepSeek-V3\n",
      "model architecture (Section 2). Subsequently, we introduce our infrastructures, encompassing\n",
      "our compute clusters, the training framework, the support for FP8 training, the inference\n",
      "deployment strategy, and our suggestions on future hardware design. Next, we describe our\n",
      "pre-training process, including the construction of training data, hyper-parameter settings, long-\n",
      "context extension techniques, the associated evaluations, as well as some discussions (Section 4).\n",
      "Thereafter, we discuss our efforts on post-training, which include Supervised Fine-Tuning (SFT),\n",
      "Reinforcement Learning (RL), the corresponding evaluations, and discussions (Section 5). Lastly,\n",
      "we conclude this work, discuss existing limitations of DeepSeek-V3, and propose potential\n",
      "directions for future research (Section 6).\n",
      "2. Architecture\n",
      "We first introduce the basic architecture of DeepSeek-V3, featured by Multi-head Latent Atten-\n",
      "tion (MLA) (DeepSeek-AI, 2024c) for efficient inference and DeepSeekMoE (Dai et al., 2024)\n",
      "for economical training. Then, we present a Multi-Token Prediction (MTP) training objective,\n",
      "which we have observed to enhance the overall performance on evaluation benchmarks. For\n",
      "other minor details not explicitly mentioned, DeepSeek-V3 adheres to the settings of DeepSeek-\n",
      "V2 (DeepSeek-AI, 2024c).\n",
      "2.1. Basic Architecture\n",
      "The basic architecture of DeepSeek-V3 is still within the Transformer (Vaswani et al., 2017)\n",
      "framework. For\n",
      "\n",
      "[4] our pre-\n",
      "training stage is completed in less than two months and costs 2664K GPU hours. Combined\n",
      "with 119K GPU hours for the context length extension and 5K GPU hours for post-training,\n",
      "DeepSeek-V3 costs only 2.788M GPU hours for its full training. Assuming the rental price of\n",
      "the H800 GPU is $2 per GPU hour, our total training costs amount to only $5.576M. Note that\n",
      "the aforementioned costs include only the official training of DeepSeek-V3, excluding the costs\n",
      "associated with prior research and ablation experiments on architectures, algorithms, or data.\n",
      "Our main contribution includes:\n",
      "Architecture: Innovative Load Balancing Strategy and Training Objective\n",
      "â€¢ On top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free\n",
      "strategy for load balancing, which minimizes the performance degradation that arises\n",
      "from encouraging load balancing.\n",
      "â€¢ We investigate a Multi-Token Prediction (MTP) objective and prove it beneficial to model\n",
      "performance. It can also be used for speculative decoding for inference acceleration.\n",
      "Pre-Training: Towards Ultimate Training Efficiency\n",
      "â€¢ We design an FP8 mixed precision training framework and, for the first time, validate the\n",
      "feasibility and effectiveness of FP8 training on an extremely large-scale model.\n",
      "â€¢ Through the co-design of algorithms, frameworks, and hardware, we overcome the\n",
      "communication bottleneck in cross-node MoE training, achieving near-full computation-\n",
      "communication overlap. This significantly enhances our training efficiency and reduces the\n",
      "training costs, enabling us to further scale up the model size without additional overhead.\n",
      "â€¢ At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of\n",
      "DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model.\n",
      "The subsequent training stages after pre-training require only 0.1M GPU hours.\n",
      "Post-Training: Knowledge Distillation from DeepSeek-R1\n",
      "â€¢ We introduce an innovative methodology to distill reasoning capabilities from the long-\n",
      "Chain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models,\n",
      "into standard LLMs, particularly DeepSeek-V3. Our pipeline elegantly incorporates the\n",
      "5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# è·å– retriever\n",
    "vector_retriever = index.as_retriever(similarity_top_k=5)\n",
    "\n",
    "# æ£€ç´¢\n",
    "nodes = vector_retriever.retrieve(\"deepseek v3æœ‰å¤šå°‘å‚æ•°?\")\n",
    "\n",
    "for i, node in enumerate(nodes):\n",
    "    print(f\"[{i}] {node.text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65bae2b4-7c36-44fa-9566-0d1b3f34972a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] DeepSeek-V3 Technical Report\n",
      "DeepSeek-AI\n",
      "research@deepseek.com\n",
      "Abstract\n",
      "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total\n",
      "parameters with 37B activated for each token. To achieve efficient inference and cost-effective\n",
      "training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architec-\n",
      "tures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers\n",
      "an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training\n",
      "objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and\n",
      "high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to\n",
      "fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms\n",
      "other open-source models and achieves performance comparable to leading closed-source\n",
      "models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours\n",
      "for its full training. In addition, its training process is remarkably stable. Throughout the entire\n",
      "training process, we did not experience any irrecoverable loss spikes or perform any rollbacks.\n",
      "The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.\n",
      "MMLU-Pro\n",
      "(EM)\n",
      "GPQA-Diamond\n",
      "(Pass@1)\n",
      "MATH 500\n",
      "(EM)\n",
      "AIME 2024\n",
      "(Pass@1)\n",
      "Codeforces\n",
      "(Percentile)\n",
      "SWE-bench Verified\n",
      "(Resolved)\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100Accuracy / Percentile (%)\n",
      "75.9\n",
      "59.1\n",
      "90.2\n",
      "39.2\n",
      "51.6\n",
      "42.0\n",
      "66.2\n",
      "41.3\n",
      "74.7\n",
      "16.7\n",
      "35.6\n",
      "22.6\n",
      "71.6\n",
      "49.0\n",
      "80.0\n",
      "23.3 24.8 23.8\n",
      "73.3\n",
      "51.1\n",
      "73.8\n",
      "23.3\n",
      "25.3\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.postprocessor import LLMRerank\n",
    "\n",
    "postprocessor = LLMRerank(top_n=2)\n",
    "\n",
    "nodes = postprocessor.postprocess_nodes(nodes, query_str=\"deepseek v3æœ‰å¤šå°‘å‚æ•°?\")\n",
    "\n",
    "for i, node in enumerate(nodes):\n",
    "    print(f\"[{i}] {node.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ca41ea-3112-491a-a1a1-5ec8db295b61",
   "metadata": {},
   "source": [
    "æ›´å¤šçš„ Rerank åŠå…¶å®ƒåå¤„ç†æ–¹æ³•ï¼Œå‚è€ƒå®˜æ–¹æ–‡æ¡£ï¼š[Node Postprocessor Modules](https://docs.llamaindex.ai/en/stable/module_guides/querying/node_postprocessors/node_postprocessors/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392d8d69-cd42-445c-a0cd-0dd7c66d07bc",
   "metadata": {},
   "source": [
    "## 6. ç”Ÿæˆå›å¤ï¼ˆQA & Chatï¼‰\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4248c1-602c-4df6-bd6a-72e994faf1ed",
   "metadata": {},
   "source": [
    "### 6.1 å•è½®é—®ç­”ï¼ˆQuery Engineï¼‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "261d35c3-8b54-4840-ab88-c04b348b0fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek-V3åœ¨æ•°å­¦ç›¸å…³åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨éé•¿é“¾æ€ç»´é“¾ï¼ˆnon-long-CoTï¼‰çš„å¼€æºå’Œé—­æºæ¨¡å‹ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›æ°´å¹³ã€‚å®ƒç”šè‡³åœ¨æŸäº›ç‰¹å®šåŸºå‡†æµ‹è¯•å¦‚MATH-500ä¸Šè¶…è¶Šäº†o1-previewï¼Œæ˜¾ç¤ºå‡ºå…¶å¼ºå¤§çš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚\n"
     ]
    }
   ],
   "source": [
    "qa_engine = index.as_query_engine()\n",
    "response = qa_engine.query(\"deepseek v3æ•°å­¦èƒ½åŠ›æ€ä¹ˆæ ·?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d55521-5b67-4c26-a8de-440d2e1046a7",
   "metadata": {},
   "source": [
    "#### æµå¼è¾“å‡º\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1817400e-9311-4161-89e4-507267862524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek-V3åœ¨æ•°å­¦ç›¸å…³åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨éé•¿é“¾æ€ç»´é“¾ï¼ˆnon-long-CoTï¼‰çš„å¼€æºå’Œé—­æºæ¨¡å‹ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å®ƒåœ¨ç‰¹å®šçš„åŸºå‡†æµ‹è¯•å¦‚MATH-500ä¸Šç”šè‡³è¶…è¿‡äº†o1-previewï¼Œè¿™è¡¨æ˜å®ƒå…·æœ‰å¼ºå¤§çš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚"
     ]
    }
   ],
   "source": [
    "qa_engine = index.as_query_engine(streaming=True)\n",
    "response = qa_engine.query(\"deepseek v3æ•°å­¦èƒ½åŠ›æ€ä¹ˆæ ·?\")\n",
    "response.print_response_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf53205-beb2-4571-81aa-c5f8e12142f5",
   "metadata": {},
   "source": [
    "### 6.2 å¤šè½®å¯¹è¯ï¼ˆChat Engineï¼‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81a73a48-469e-4d40-adf3-e0e5ec44305a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-26 17:26:44,297 - INFO - Condensed question: deepseek v3æ•°å­¦èƒ½åŠ›æ€ä¹ˆæ ·?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek-V3åœ¨æ•°å­¦ç›¸å…³åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒåœ¨æ‰€æœ‰éé•¿é“¾æ€ç»´é“¾ï¼ˆnon-long-CoTï¼‰çš„å¼€æºå’Œé—­æºæ¨¡å‹ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ç‰¹åˆ«æ˜¯åœ¨MATH-500è¿™æ ·çš„ç‰¹å®šåŸºå‡†æµ‹è¯•ä¸­ï¼ŒDeepSeek-V3ç”šè‡³è¶…è¿‡äº†o1-previewæ¨¡å‹ï¼Œè¿™å±•ç¤ºäº†å…¶å¼ºå¤§çš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚\n",
      "\n",
      "æ€»ç»“ä¸€ä¸‹ï¼ŒDeepSeek-V3åœ¨æ•°å­¦æ–¹é¢çš„èƒ½åŠ›éå¸¸å¼ºï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤æ‚çš„æ•°å­¦é—®é¢˜æ—¶ï¼Œå®ƒçš„è¡¨ç°ä¼˜äºè®¸å¤šå…¶ä»–æ¨¡å‹ã€‚\n"
     ]
    }
   ],
   "source": [
    "chat_engine = index.as_chat_engine()\n",
    "response = chat_engine.chat(\"deepseek v3æ•°å­¦èƒ½åŠ›æ€ä¹ˆæ ·?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "712c8c71-6420-4c6f-821c-1bd1c31dc8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-26 17:26:58,615 - INFO - Condensed question: DeepSeek-V3çš„ä»£ç èƒ½åŠ›å¦‚ä½•ï¼Ÿ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Response\n"
     ]
    }
   ],
   "source": [
    "response = chat_engine.chat(\"ä»£ç èƒ½åŠ›å‘¢?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3094d188-2241-4995-9704-1d2aeb878e1e",
   "metadata": {},
   "source": [
    "#### æµå¼è¾“å‡º\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf33effa-ad36-4eea-b1d1-926ca5cc8dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-26 17:27:07,863 - INFO - Condensed question: deepseek v3æ•°å­¦èƒ½åŠ›æ€ä¹ˆæ ·?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek-V3åœ¨æ•°å­¦ç›¸å…³åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ã€‚æ ¹æ®æ–‡æ¡£ï¼Œå®ƒåœ¨æ‰€æœ‰éé•¿é“¾æ€ç»´é“¾ï¼ˆnon-long-CoTï¼‰çš„å¼€æºå’Œé—­æºæ¨¡å‹ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚ç‰¹åˆ«å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®ƒåœ¨æŸäº›ç‰¹å®šçš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œå¦‚MATH-500ï¼Œç”šè‡³è¶…è¿‡äº†o1-previewæ¨¡å‹ï¼Œè¿™å±•ç¤ºäº†å…¶å¼ºå¤§çš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚å› æ­¤ï¼Œå¯ä»¥è¯´DeepSeek-V3å…·æœ‰éå¸¸ä¼˜ç§€çš„æ•°å­¦å¤„ç†èƒ½åŠ›ã€‚"
     ]
    }
   ],
   "source": [
    "chat_engine = index.as_chat_engine()\n",
    "streaming_response = chat_engine.stream_chat(\"deepseek v3æ•°å­¦èƒ½åŠ›æ€ä¹ˆæ ·?\")\n",
    "# streaming_response.print_response_stream()\n",
    "for token in streaming_response.response_gen:\n",
    "    print(token, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7eb76fa-e4ba-4c47-9785-53eadddf478e",
   "metadata": {},
   "source": [
    "## 7. åº•å±‚æ¥å£ï¼šPromptã€LLM ä¸ Embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0cce84-9f79-41ef-a6c4-caaeb70d029e",
   "metadata": {},
   "source": [
    "### 7.1 Prompt æ¨¡æ¿\n",
    "\n",
    "#### `PromptTemplate` å®šä¹‰æç¤ºè¯æ¨¡æ¿\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41358d83-4fcb-430c-9c2e-c1ac0839cbfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'å†™ä¸€ä¸ªå…³äºå°æ˜çš„ç¬‘è¯'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\"å†™ä¸€ä¸ªå…³äº{topic}çš„ç¬‘è¯\")\n",
    "\n",
    "prompt.format(topic=\"å°æ˜\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a94232-e8d5-4bf2-8751-8851d924fcee",
   "metadata": {},
   "source": [
    "#### `ChatPromptTemplate` å®šä¹‰å¤šè½®æ¶ˆæ¯æ¨¡æ¿\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edb05592-835f-4cb2-bfa0-862796c36fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system: ä½ å«å°æ˜ï¼Œä½ å¿…é¡»æ ¹æ®ç”¨æˆ·æä¾›çš„ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚\n",
      "user: å·²çŸ¥ä¸Šä¸‹æ–‡ï¼š\n",
      "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•\n",
      "\n",
      "é—®é¢˜ï¼šè¿™æ˜¯ä»€ä¹ˆ\n",
      "assistant: \n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "from llama_index.core import ChatPromptTemplate\n",
    "\n",
    "chat_text_qa_msgs = [\n",
    "    ChatMessage(\n",
    "        role=MessageRole.SYSTEM,\n",
    "        content=\"ä½ å«{name}ï¼Œä½ å¿…é¡»æ ¹æ®ç”¨æˆ·æä¾›çš„ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚\",\n",
    "    ),\n",
    "    ChatMessage(\n",
    "        role=MessageRole.USER, \n",
    "        content=(\n",
    "            \"å·²çŸ¥ä¸Šä¸‹æ–‡ï¼š\\n\" \\\n",
    "            \"{context}\\n\\n\" \\\n",
    "            \"é—®é¢˜ï¼š{question}\"\n",
    "        )\n",
    "    ),\n",
    "]\n",
    "text_qa_template = ChatPromptTemplate(chat_text_qa_msgs)\n",
    "\n",
    "print(\n",
    "    text_qa_template.format(\n",
    "        name=\"å°æ˜\",\n",
    "        context=\"è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•\",\n",
    "        question=\"è¿™æ˜¯ä»€ä¹ˆ\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98931d9-f411-4433-9900-649f74a40b6e",
   "metadata": {},
   "source": [
    "### 7.2 è¯­è¨€æ¨¡å‹\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1febbfc2-06c1-4692-9ac1-8843e5554098",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0, model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c0d3842-32ff-4510-9853-47e7282b56e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-26 17:28:18,348 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 401 Unauthorized\"\n"
     ]
    },
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': \"You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAuthenticationError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m response = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcomplete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43må°æ˜\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(response.text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\jukeai-chapter4\\Lib\\site-packages\\llama_index_instrumentation\\dispatcher.py:335\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    332\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    336\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    337\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    338\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\jukeai-chapter4\\Lib\\site-packages\\llama_index\\core\\llms\\callbacks.py:435\u001b[39m, in \u001b[36mllm_completion_callback.<locals>.wrap.<locals>.wrapped_llm_predict\u001b[39m\u001b[34m(_self, *args, **kwargs)\u001b[39m\n\u001b[32m    426\u001b[39m event_id = callback_manager.on_event_start(\n\u001b[32m    427\u001b[39m     CBEventType.LLM,\n\u001b[32m    428\u001b[39m     payload={\n\u001b[32m   (...)\u001b[39m\u001b[32m    432\u001b[39m     },\n\u001b[32m    433\u001b[39m )\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m435\u001b[39m     f_return_val = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    437\u001b[39m     callback_manager.on_event_end(\n\u001b[32m    438\u001b[39m         CBEventType.LLM,\n\u001b[32m    439\u001b[39m         payload={EventPayload.EXCEPTION: e},\n\u001b[32m    440\u001b[39m         event_id=event_id,\n\u001b[32m    441\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\jukeai-chapter4\\Lib\\site-packages\\llama_index\\llms\\openai\\base.py:424\u001b[39m, in \u001b[36mOpenAI.complete\u001b[39m\u001b[34m(self, prompt, formatted, **kwargs)\u001b[39m\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    423\u001b[39m     complete_fn = \u001b[38;5;28mself\u001b[39m._complete\n\u001b[32m--> \u001b[39m\u001b[32m424\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcomplete_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\jukeai-chapter4\\Lib\\site-packages\\llama_index\\core\\base\\llms\\generic_utils.py:184\u001b[39m, in \u001b[36mchat_to_completion_decorator.<locals>.wrapper\u001b[39m\u001b[34m(prompt, **kwargs)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(prompt: \u001b[38;5;28mstr\u001b[39m, **kwargs: Any) -> CompletionResponse:\n\u001b[32m    182\u001b[39m     \u001b[38;5;66;03m# normalize input\u001b[39;00m\n\u001b[32m    183\u001b[39m     messages = prompt_to_messages(prompt)\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m     chat_response = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    185\u001b[39m     \u001b[38;5;66;03m# normalize output\u001b[39;00m\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m chat_response_to_completion_response(chat_response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\jukeai-chapter4\\Lib\\site-packages\\llama_index\\llms\\openai\\base.py:114\u001b[39m, in \u001b[36mllm_retry_decorator.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m    107\u001b[39m retry = create_retry_decorator(\n\u001b[32m    108\u001b[39m     max_retries=max_retries,\n\u001b[32m    109\u001b[39m     random_exponential=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    112\u001b[39m     max_seconds=\u001b[32m20\u001b[39m,\n\u001b[32m    113\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\jukeai-chapter4\\Lib\\site-packages\\tenacity\\__init__.py:338\u001b[39m, in \u001b[36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    336\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    337\u001b[39m wrapped_f.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\jukeai-chapter4\\Lib\\site-packages\\tenacity\\__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    475\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\jukeai-chapter4\\Lib\\site-packages\\tenacity\\__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    376\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\jukeai-chapter4\\Lib\\site-packages\\tenacity\\__init__.py:400\u001b[39m, in \u001b[36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[33m\"\u001b[39m\u001b[33mRetryCallState\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.iter_state.is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.retry_run_result):\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m         \u001b[38;5;28mself\u001b[39m._add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutcome\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    401\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.after \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\jukeai-chapter4\\Lib\\concurrent\\futures\\_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\jukeai-chapter4\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\jukeai-chapter4\\Lib\\site-packages\\tenacity\\__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    482\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\jukeai-chapter4\\Lib\\site-packages\\llama_index\\llms\\openai\\base.py:495\u001b[39m, in \u001b[36mOpenAI._chat\u001b[39m\u001b[34m(self, messages, **kwargs)\u001b[39m\n\u001b[32m    489\u001b[39m message_dicts = to_openai_message_dicts(\n\u001b[32m    490\u001b[39m     messages,\n\u001b[32m    491\u001b[39m     model=\u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m    492\u001b[39m )\n\u001b[32m    494\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reuse_client:\n\u001b[32m--> \u001b[39m\u001b[32m495\u001b[39m     response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_model_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    501\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m client:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\jukeai-chapter4\\Lib\\site-packages\\openai\\_utils\\_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\jukeai-chapter4\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:1192\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1145\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1146\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1147\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1189\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   1190\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1191\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1201\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1203\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1204\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1205\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1206\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1207\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1208\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1209\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1210\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1211\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1212\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1213\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_retention\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_retention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1214\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1215\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1216\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1217\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1218\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1219\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1220\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1221\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1225\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1227\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1228\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1229\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1230\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1231\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1232\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1237\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1238\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1242\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\jukeai-chapter4\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\jukeai-chapter4\\Lib\\site-packages\\openai\\_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAuthenticationError\u001b[39m: Error code: 401 - {'error': {'message': \"You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}"
     ]
    }
   ],
   "source": [
    "response = llm.complete(prompt.format(topic=\"å°æ˜\"))\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cbe1cc-9dfc-4274-8cdd-b4dd6d3eb5bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response = llm.complete(\n",
    "    text_qa_template.format(\n",
    "        name=\"å°æ˜\",\n",
    "        context=\"è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•\",\n",
    "        question=\"ä½ æ˜¯è°ï¼Œæˆ‘ä»¬åœ¨å¹²å˜›\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4234c43f-d46c-4755-afb3-ade10c308bf8",
   "metadata": {},
   "source": [
    "#### è¿æ¥DeepSeek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad92e176-6153-4059-a01b-e2957a052a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-index-llms-deepseek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "511a86bd-c5eb-45e7-8e0a-7e56b2fe7f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\.conda\\envs\\jukeai-chapter4\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "2026-01-26 17:29:42,357 - INFO - HTTP Request: POST https://api.deepseek.com/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä¸€ä¸ªç¨‹åºå‘˜å»é¢è¯•ã€‚\n",
      "\n",
      "é¢è¯•å®˜é—®ï¼šâ€œä½ å†™è¿‡å¤šå°‘è¡Œä»£ç ï¼Ÿâ€\n",
      "\n",
      "ç¨‹åºå‘˜ï¼šâ€œå¤§æ¦‚å‡ ç™¾ä¸‡è¡Œå§ã€‚â€\n",
      "\n",
      "é¢è¯•å®˜ï¼šâ€œé‚£ä½ çŠ¯è¿‡å¤šå°‘é”™è¯¯ï¼Ÿâ€\n",
      "\n",
      "ç¨‹åºå‘˜ï¼šâ€œä¸€ä¸ªéƒ½æ²¡æœ‰ã€‚â€\n",
      "\n",
      "é¢è¯•å®˜å¾ˆæƒŠè®¶ï¼šâ€œçœŸçš„ï¼Ÿä¸€æ¬¡é”™éƒ½æ²¡å‡ºè¿‡ï¼Ÿâ€\n",
      "\n",
      "ç¨‹åºå‘˜ç‚¹ç‚¹å¤´ï¼šâ€œå—¯ã€‚å› ä¸ºæˆ‘çš„ä»£ç é‡Œå…¨æ˜¯ `// TODO: è¿™é‡Œå¥½åƒä¸å¯¹ï¼Œä½†å…ˆè¿™æ ·å§`ã€‚â€\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from llama_index.llms.deepseek import DeepSeek\n",
    "\n",
    "llm = DeepSeek(model=\"deepseek-chat\", api_key=os.getenv(\"DEEPSEEK_API_KEY\"), temperature=1.5)\n",
    "\n",
    "response = llm.complete(\"å†™ä¸ªç¬‘è¯\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f49f6da-d220-4c36-b697-e3b525902af7",
   "metadata": {},
   "source": [
    "#### è®¾ç½®å…¨å±€ä½¿ç”¨çš„è¯­è¨€æ¨¡å‹\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94943bec-563c-44b5-80bb-9c0a7c05382c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = DeepSeek(model=\"deepseek-chat\", api_key=os.getenv(\"DEEPSEEK_API_KEY\"), temperature=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dd7b76-2594-4c39-8e81-33e3c985df38",
   "metadata": {},
   "source": [
    "é™¤ OpenAI å¤–ï¼ŒLlamaIndex å·²é›†æˆå¤šä¸ªå¤§è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬äº‘æœåŠ¡ API å’Œæœ¬åœ°éƒ¨ç½² APIï¼Œè¯¦è§å®˜æ–¹æ–‡æ¡£ï¼š[Available LLM integrations](https://docs.llamaindex.ai/en/stable/module_guides/models/llms/modules/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b110fc0b-ec6c-445b-a3c2-3ddef41d9589",
   "metadata": {},
   "source": [
    "### 7.3 Embedding æ¨¡å‹\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5be1257-eb1a-4e2d-bad9-2cd002841093",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# å…¨å±€è®¾å®š\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\", dimensions=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb025ce6-c065-4cf2-9e9d-5a01f04c26ee",
   "metadata": {},
   "source": [
    "LlamaIndex åŒæ ·é›†æˆäº†å¤šç§ Embedding æ¨¡å‹ï¼ŒåŒ…æ‹¬äº‘æœåŠ¡ API å’Œå¼€æºæ¨¡å‹ï¼ˆHuggingFaceï¼‰ç­‰ï¼Œè¯¦è§[å®˜æ–¹æ–‡æ¡£](https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings/)ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be91aa7d-6f3a-4387-859c-c7357d7c5d15",
   "metadata": {},
   "source": [
    "## 8. åŸºäº LlamaIndex å®ç°ä¸€ä¸ªåŠŸèƒ½è¾ƒå®Œæ•´çš„ RAG ç³»ç»Ÿ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a1b844-605e-4be2-9cae-2e9e56e46b40",
   "metadata": {},
   "source": [
    "åŠŸèƒ½è¦æ±‚ï¼š\n",
    "\n",
    "- åŠ è½½æŒ‡å®šç›®å½•çš„æ–‡ä»¶\n",
    "- æ”¯æŒ RAG-Fusion\n",
    "- ä½¿ç”¨ Qdrant å‘é‡æ•°æ®åº“ï¼Œå¹¶æŒä¹…åŒ–åˆ°æœ¬åœ°\n",
    "- æ”¯æŒæ£€ç´¢åæ’åº\n",
    "- æ”¯æŒå¤šè½®å¯¹è¯\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90348baf-3a5e-4cc4-968d-e90f9d315495",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance\n",
    "\n",
    "EMBEDDING_DIM = 1536\n",
    "COLLECTION_NAME = \"full_demo\"\n",
    "PATH = \"./qdrant_db\"\n",
    "\n",
    "client = QdrantClient(path=PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf485f1b-7da4-412e-a121-a7e6b5f3ece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, get_response_synthesizer\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.response_synthesizers import ResponseMode\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core.postprocessor import LLMRerank, SimilarityPostprocessor\n",
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.chat_engine import CondenseQuestionChatEngine\n",
    "from llama_index.llms.dashscope import DashScope, DashScopeGenerationModels\n",
    "from llama_index.embeddings.dashscope import DashScopeEmbedding, DashScopeTextEmbeddingModels\n",
    "\n",
    "# 1. æŒ‡å®šå…¨å±€llmä¸embeddingæ¨¡å‹\n",
    "Settings.llm = DashScope(model_name=DashScopeGenerationModels.QWEN_MAX,api_key=os.getenv(\"DASHSCOPE_API_KEY\"))\n",
    "Settings.embed_model = DashScopeEmbedding(model_name=DashScopeTextEmbeddingModels.TEXT_EMBEDDING_V1)\n",
    "\n",
    "# 2. æŒ‡å®šå…¨å±€æ–‡æ¡£å¤„ç†çš„ Ingestion Pipeline\n",
    "Settings.transformations = [SentenceSplitter(chunk_size=512, chunk_overlap=200)]\n",
    "\n",
    "# 3. åŠ è½½æœ¬åœ°æ–‡æ¡£\n",
    "documents = SimpleDirectoryReader(\"./data\").load_data()\n",
    "\n",
    "if client.collection_exists(collection_name=COLLECTION_NAME):\n",
    "    client.delete_collection(collection_name=COLLECTION_NAME)\n",
    "\n",
    "# 4. åˆ›å»º collection\n",
    "client.create_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    vectors_config=VectorParams(size=EMBEDDING_DIM, distance=Distance.COSINE)\n",
    ")\n",
    "\n",
    "# 5. åˆ›å»º Vector Store\n",
    "vector_store = QdrantVectorStore(client=client, collection_name=COLLECTION_NAME)\n",
    "\n",
    "# 6. æŒ‡å®š Vector Store çš„ Storage ç”¨äº index\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context\n",
    ")\n",
    "\n",
    "# 7. å®šä¹‰æ£€ç´¢åæ’åºæ¨¡å‹\n",
    "reranker = LLMRerank(top_n=2)\n",
    "# æœ€ç»ˆæ‰“åˆ†ä½äº0.6çš„æ–‡æ¡£è¢«è¿‡æ»¤æ‰\n",
    "sp = SimilarityPostprocessor(similarity_cutoff=0.6)\n",
    "\n",
    "# 8. å®šä¹‰ RAG Fusion æ£€ç´¢å™¨\n",
    "fusion_retriever = QueryFusionRetriever(\n",
    "    [index.as_retriever()],\n",
    "    similarity_top_k=5, # æ£€ç´¢å¬å› top k ç»“æœ\n",
    "    num_queries=3,  # ç”Ÿæˆ query æ•°\n",
    "    use_async=False,\n",
    "    # query_gen_prompt=\"\",  # å¯ä»¥è‡ªå®šä¹‰ query ç”Ÿæˆçš„ prompt æ¨¡æ¿\n",
    ")\n",
    "\n",
    "# 9. æ„å»ºå•è½® query engine\n",
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "    fusion_retriever,\n",
    "    node_postprocessors=[reranker],\n",
    "    response_synthesizer=get_response_synthesizer(\n",
    "        response_mode = ResponseMode.REFINE\n",
    "    )\n",
    ")\n",
    "\n",
    "# 10. å¯¹è¯å¼•æ“\n",
    "chat_engine = CondenseQuestionChatEngine.from_defaults(\n",
    "    query_engine=query_engine, \n",
    "    # condense_question_prompt=\"\" # å¯ä»¥è‡ªå®šä¹‰ chat message prompt æ¨¡æ¿\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0124ef05-eb4b-417b-aa56-7128460162dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User: deepseek v3æœ‰å¤šå°‘å‚æ•°ï¼Ÿ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-26 17:31:06,721 - INFO - Querying with: deepseek v3æœ‰å¤šå°‘å‚æ•°ï¼Ÿ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: DeepSeek-V3æ€»å…±æœ‰671äº¿ä¸ªå‚æ•°ï¼Œæ¯ä¸ªä»¤ç‰Œæ¿€æ´»37äº¿ä¸ªå‚æ•°ã€‚\n"
     ]
    }
   ],
   "source": [
    "# æµ‹è¯•å¤šè½®å¯¹è¯\n",
    "# User: deepseek v3æœ‰å¤šå°‘å‚æ•°\n",
    "# User: æ¯æ¬¡æ¿€æ´»å¤šå°‘\n",
    "\n",
    "while True:\n",
    "    question=input(\"User:\")\n",
    "    if question.strip() == \"\":\n",
    "        break\n",
    "    response = chat_engine.chat(question)\n",
    "    print(f\"AI: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de36489",
   "metadata": {},
   "source": [
    "## 9. Text2SQL / NL2SQL / NL2Chart / ChatBI\n",
    "\n",
    "#### 9.1 åŸºæœ¬ä»‹ç»\n",
    "\n",
    "Text2SQL æ˜¯ä¸€ç§å°†è‡ªç„¶è¯­è¨€è½¬æ¢ä¸ºSQLæŸ¥è¯¢è¯­å¥çš„æŠ€æœ¯ã€‚\n",
    "\n",
    "è¿™é¡¹æŠ€æœ¯çš„æ„ä¹‰ï¼šè®©æ¯ä¸ªäººéƒ½èƒ½åƒå¯¹è¯ä¸€æ ·æŸ¥è¯¢æ•°æ®åº“ï¼Œè·å–æ‰€éœ€ä¿¡æ¯ï¼Œè€Œä¸å¿…å­¦ä¹ SQLè¯­æ³•ã€‚\n",
    "\n",
    "#### 9.2 å…¸å‹åº”ç”¨åœºæ™¯\n",
    "\n",
    "- ä¸šåŠ¡åˆ†æå¸ˆçš„æ•°æ®è‡ªåŠ©æœåŠ¡\n",
    "\n",
    "- æ™ºèƒ½BIä¸æ•°æ®å¯è§†åŒ–\n",
    "\n",
    "- å®¢æœä¸å†…éƒ¨æ•°æ®åº“æŸ¥è¯¢\n",
    "\n",
    "- è·¨éƒ¨é—¨æ•°æ®åä½œä¸åˆ†äº«\n",
    "\n",
    "- è¿è¥æ•°æ®åˆ†æä¸å†³ç­–æ”¯æŒ\n",
    "\n",
    "\n",
    "#### 9.3 Text2SQLæ ¸å¿ƒèƒ½åŠ›ä¸æŒ‘æˆ˜\n",
    "\n",
    "ä¸€ä¸ªæˆç†Ÿçš„Text2SQLç³»ç»Ÿéœ€è¦å…·å¤‡ä»¥ä¸‹å…³é”®èƒ½åŠ›ï¼š\n",
    "\n",
    "| æ ¸å¿ƒèƒ½åŠ›       | è¯´æ˜                   | æŠ€æœ¯æŒ‘æˆ˜             |\n",
    "| -------------- | ---------------------- | -------------------- |\n",
    "| è¯­ä¹‰ç†è§£       | ç†è§£ç”¨æˆ·çœŸæ­£çš„æŸ¥è¯¢æ„å›¾ | å¤„ç†æ­§ä¹‰ã€ä¸Šä¸‹æ–‡æ¨æ–­ |\n",
    "| æ•°æ®åº“ç»“æ„æ„ŸçŸ¥ | äº†è§£è¡¨ç»“æ„ã€å­—æ®µå…³ç³»   | è‡ªåŠ¨æ˜ å°„å­—æ®µä¸å®ä½“   |\n",
    "| å¤æ‚æŸ¥è¯¢æ„å»º   | æ”¯æŒå¤šè¡¨è¿æ¥ã€èšåˆç­‰   | å­æŸ¥è¯¢ã€åµŒå¥—é€»è¾‘è½¬æ¢ |\n",
    "| ä¸Šä¸‹æ–‡è®°å¿†     | ç†è§£å¤šè½®å¯¹è¯ä¸­çš„æŒ‡ä»£   | ç»´æŠ¤æŸ¥è¯¢çŠ¶æ€         |\n",
    "| é”™è¯¯å¤„ç†       | è¯†åˆ«å¹¶ä¿®æ­£é”™è¯¯è¾“å…¥     | æ¨¡ç³ŠåŒ¹é…ã€å®¹é”™æœºåˆ¶   |\n",
    "\n",
    "#### 9.4 å®ç°Text2SQLçš„æŠ€æœ¯æ¶æ„\n",
    "\n",
    "- æ¶æ„ä¸€ï¼šåŸºäºWorkflowå·¥ä½œæµæ–¹æ¡ˆ\n",
    "\n",
    "- æ¶æ„äºŒï¼šåŸºäºLangChainçš„æ•°æ®åº“é“¾æ–¹æ¡ˆ\n",
    "\n",
    "- æ¶æ„ä¸‰ï¼šä¼ä¸šçº§è§£å†³æ–¹æ¡ˆ\n",
    "\n",
    "    1. Vannaï¼ˆå¼€æºï¼‰\n",
    "\n",
    "        - å®˜ç½‘ï¼šhttps://vanna.ai/\n",
    "\n",
    "    2. é˜¿é‡Œäº‘ï¼ˆå•†ä¸šï¼‰\n",
    "\n",
    "        - [è‡ªç„¶è¯­è¨€åˆ°SQLè¯­è¨€è½¬ä¹‰ï¼ˆåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„NL2SQLï¼‰](http://help.aliyun.com/zh/polardb/polardb-for-mysql/user-guide/llm-based-nl2sql?spm=a2c4g.11186623.help-menu-2249963.d_5_25_1_0.5d942b63IaNo7t&scm=20140722.H_2669074._.OR_help-T_cn~zh-V_1)\n",
    "\n",
    "        - [è‡ªç„¶è¯­è¨€ç”Ÿæˆæ™ºèƒ½å›¾è¡¨NL2Chart](https://help.aliyun.com/zh/polardb/polardb-for-mysql/user-guide/nl2chart?spm=a2c4g.11186623.help-menu-2249963.d_5_25_1_1.16325ef0KtuFXl&scm=20140722.H_2922405._.OR_help-T_cn~zh-V_1)\n",
    "\n",
    "    3. è…¾è®¯äº‘ï¼ˆå•†ä¸šï¼‰\n",
    "\n",
    "        - ChatBIäº§å“ https://cloud.tencent.com/document/product/590/107689"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb4313c",
   "metadata": {},
   "source": [
    "## 10. å·¥ä½œæµï¼ˆWorkflowï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9016c66a",
   "metadata": {},
   "source": [
    "### 10.1 å·¥ä½œæµï¼ˆWorkflowï¼‰ç®€ä»‹\n",
    "\n",
    "å·¥ä½œæµé¡¾åæ€ä¹‰æ˜¯å¯¹ä¸€äº›åˆ—å·¥ä½œæ­¥éª¤çš„æŠ½è±¡ã€‚\n",
    "\n",
    "LlamaIndex çš„å·¥ä½œæµæ˜¯äº‹ä»¶ï¼ˆ`event`ï¼‰é©±åŠ¨çš„ï¼š\n",
    "\n",
    "- å·¥ä½œæµç”± `step` ç»„æˆ\n",
    "- æ¯ä¸ª `step` å¤„ç†ç‰¹å®šçš„äº‹ä»¶\n",
    "- `step` ä¹Ÿä¼šäº§ç”Ÿæ–°çš„äº‹ä»¶ï¼ˆäº¤ç”±åç»§çš„ `step` è¿›è¡Œå¤„ç†ï¼‰\n",
    "- ç›´åˆ°äº§ç”Ÿ `StopEvent` æ•´ä¸ªå·¥ä½œæµç»“æŸ\n",
    "\n",
    "LlamaIndex Workflowsï¼šhttps://docs.llamaindex.ai/en/stable/module_guides/workflow/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99b8edc-2764-4f07-ad2e-1e8c549b8157",
   "metadata": {},
   "source": [
    "### 10.2 å·¥ä½œæµè®¾è®¡\n",
    "\n",
    "ä½¿ç”¨è‡ªç„¶è¯­è¨€æŸ¥è¯¢æ•°æ®åº“ï¼Œæ•°æ®åº“ä¸­åŒ…å«å¤šå¼ è¡¨\n",
    "\n",
    "å·¥ä½œæµè®¾è®¡ï¼š\n",
    "\n",
    "<img src=\"./assets/workflow.png\" alt=\"å·¥ä½œæµ\" width=\"1000\"/>\n",
    "\n",
    "åˆ†æ­¥è¯´æ˜ï¼š\n",
    "\n",
    "1. ç”¨æˆ·è¾“å…¥è‡ªç„¶è¯­è¨€æŸ¥è¯¢\n",
    "2. ç³»ç»Ÿå…ˆå»æ£€ç´¢è·ŸæŸ¥è¯¢ç›¸å…³çš„è¡¨\n",
    "3. æ ¹æ®è¡¨çš„ Schema è®©å¤§æ¨¡å‹ç”Ÿæˆ SQL\n",
    "4. ç”¨ç”Ÿæˆçš„ SQL æŸ¥è¯¢æ•°æ®åº“\n",
    "5. æ ¹æ®æŸ¥è¯¢ç»“æœï¼Œè°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆè‡ªç„¶è¯­è¨€å›å¤"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be28368-98b7-4dcf-8cca-a05f443cb66b",
   "metadata": {},
   "source": [
    "### 10.3 æ•°æ®å‡†å¤‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7487c6-f529-4987-b069-1e96d20303ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸‹è½½ WikiTableQuestions\n",
    "# WikiTableQuestions æ˜¯ä¸€ä¸ªä¸ºè¡¨æ ¼é—®ç­”è®¾è®¡çš„æ•°æ®é›†ã€‚å…¶ä¸­åŒ…å« 2,108 ä¸ªä»ç»´åŸºç™¾ç§‘æå–çš„ HTML è¡¨æ ¼\n",
    "\n",
    "# !wget \"https://github.com/ppasupat/WikiTableQuestions/releases/download/v1.0.2/WikiTableQuestions-1.0.2-compact.zip\" -O wiki_data.zip\n",
    "# !unzip wiki_data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7dfdbf-c064-4575-b59e-6562e86aff21",
   "metadata": {},
   "source": [
    "1. éå†ç›®å½•åŠ è½½è¡¨æ ¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd4661dd-856a-4476-b771-1164fecadf47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing file: WikiTableQuestions\\csv\\200-csv\\0.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\1.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\10.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\11.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\12.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\14.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\15.csv\n",
      "Error parsing WikiTableQuestions\\csv\\200-csv\\15.csv: Error tokenizing data. C error: Expected 4 fields in line 16, saw 5\n",
      "\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\17.csv\n",
      "Error parsing WikiTableQuestions\\csv\\200-csv\\17.csv: Error tokenizing data. C error: Expected 6 fields in line 5, saw 7\n",
      "\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\18.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\20.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\22.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\24.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\25.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\26.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\28.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\29.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\3.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\30.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\31.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\32.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\33.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\34.csv\n",
      "Error parsing WikiTableQuestions\\csv\\200-csv\\34.csv: Error tokenizing data. C error: Expected 4 fields in line 6, saw 13\n",
      "\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\35.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\36.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\37.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\38.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\4.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\41.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\42.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\44.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\45.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\46.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\47.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\48.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\7.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\8.csv\n",
      "processing file: WikiTableQuestions\\csv\\200-csv\\9.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(\"./WikiTableQuestions/csv/200-csv\")\n",
    "csv_files = sorted([f for f in data_dir.glob(\"*.csv\")])\n",
    "dfs = []\n",
    "for csv_file in csv_files:\n",
    "    print(f\"processing file: {csv_file}\")\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        dfs.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing {csv_file}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7e7eef-d81a-4dd2-bcfd-1fb5dd89603a",
   "metadata": {},
   "source": [
    "2. ä¸ºæ¯ä¸ªè¡¨ç”Ÿæˆä¸€æ®µæ–‡å­—è¡¨è¿°ï¼ˆç”¨äºæ£€ç´¢ï¼‰ï¼Œä¿å­˜åœ¨ `WikiTableQuestions_TableInfo` ç›®å½•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25c1a118-6f5e-470a-bb02-54fd5f1497ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.prompts import ChatPromptTemplate\n",
    "from llama_index.core.bridge.pydantic import BaseModel, Field\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "\n",
    "class TableInfo(BaseModel):\n",
    "    \"\"\"Information regarding a structured table.\"\"\"\n",
    "\n",
    "    table_name: str = Field(\n",
    "        ..., description=\"table name (must be underscores and NO spaces)\"\n",
    "    )\n",
    "    table_summary: str = Field(\n",
    "        ..., description=\"short, concise summary/caption of the table\"\n",
    "    )\n",
    "\n",
    "\n",
    "prompt_str = \"\"\"\n",
    "Give me a summary of the table with the following JSON format.\n",
    "\n",
    "- The table name must be unique to the table and describe it while being concise. \n",
    "- Do NOT output a generic table name (e.g. table, my_table).\n",
    "\n",
    "Do NOT make the table name one of the following: {exclude_table_name_list}\n",
    "\n",
    "Table:\n",
    "{table_str}\n",
    "\n",
    "Summary: \"\"\"\n",
    "\n",
    "prompt_tmpl = ChatPromptTemplate(\n",
    "    message_templates=[ChatMessage.from_str(prompt_str, role=\"user\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aaded9f-4af5-4ab9-b994-374ac420bf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "tableinfo_dir = \"WikiTableQuestions_TableInfo\"\n",
    "# !mkdir {tableinfo_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e4bf4ce-f969-45a2-b705-adc23a0c1cf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def _get_tableinfo_with_index(idx: int) -> str:\n",
    "    results_gen = Path(tableinfo_dir).glob(f\"{idx}_*\")\n",
    "    results_list = list(results_gen)\n",
    "    if len(results_list) == 0:\n",
    "        return None\n",
    "    elif len(results_list) == 1:\n",
    "        path = results_list[0]\n",
    "        with open(path, 'r') as file:\n",
    "            data = json.load(file) \n",
    "            return TableInfo.model_validate(data)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"More than one file matching index: {list(results_gen)}\"\n",
    "        )\n",
    "    \n",
    "\n",
    "table_names = set()\n",
    "table_infos = []\n",
    "for idx, df in enumerate(dfs):\n",
    "    table_info = _get_tableinfo_with_index(idx)\n",
    "    if table_info:\n",
    "        table_infos.append(table_info)\n",
    "    else:\n",
    "        while True:\n",
    "            df_str = df.head(10).to_csv()\n",
    "            table_info = llm.structured_predict(\n",
    "                TableInfo,\n",
    "                prompt_tmpl,\n",
    "                table_str=df_str,\n",
    "                exclude_table_name_list=str(list(table_names)),\n",
    "            )\n",
    "            table_name = table_info.table_name\n",
    "            print(f\"Processed table: {table_name}\")\n",
    "            if table_name not in table_names:\n",
    "                table_names.add(table_name)\n",
    "                break\n",
    "            else:\n",
    "                # try again\n",
    "                print(f\"Table name {table_name} already exists, trying again.\")\n",
    "                pass\n",
    "\n",
    "        out_file = f\"{tableinfo_dir}/{idx}_{table_name}.json\"\n",
    "        json.dump(table_info.dict(), open(out_file, \"w\"))\n",
    "    table_infos.append(table_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73345b81-07ff-49b0-9ccd-ed553c195f93",
   "metadata": {},
   "source": [
    "3. å°†ä¸Šè¿°è¡¨æ ¼å­˜å…¥ SQLite æ•°æ®åº“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09b6fd32-3255-4f2d-9962-eb9449f56565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating table: progressive_rock_album_chart_positions\n",
      "Creating table: filmography_of_diane\n",
      "Creating table: annual_fatalities_and_accidents_statistics\n",
      "Creating table: academy_awards_1972_results\n",
      "Creating table: theatrical_award_nominations_and_wins\n",
      "Creating table: bad_boy_artists_album_release_summary\n",
      "Creating table: south_dakota_radio_stations\n",
      "Creating table: missing_persons_case_summary_1982\n",
      "Creating table: chart_performance_of_singles\n",
      "Creating table: kodachrome_film_types_and_dates\n",
      "Creating table: bbc_radio_service_costs_2012_2013\n",
      "Creating table: french_airports_usage_summary\n",
      "Creating table: voter_registration_summary_by_party\n",
      "Creating table: norwegian_club_performance_statistics\n",
      "Creating table: triple_crown_winners_history\n",
      "Creating table: grammy_awards_summary_for_artist\n",
      "Creating table: boxing_fight_results_history\n",
      "Creating table: historical_sports_team_performance\n",
      "Creating table: yamato_population_density_summary\n",
      "Creating table: voter_registration_summary_by_party_distribution\n",
      "Creating table: best_actress_award_nominations_and_wins\n",
      "Creating table: uk_ministerial_positions_and_titles_history\n",
      "Creating table: municipality_merger_summary\n",
      "Creating table: euro_2020_group_stage_results\n",
      "Creating table: binary_encoding_probabilities\n",
      "Creating table: monthly_climate_statistics\n",
      "Creating table: italian_government_term_history\n",
      "Creating table: new_mexico_government_officials\n",
      "Creating table: monthly_climate_statistics_summary\n",
      "Creating table: historical_rainfall_experiment_drops\n",
      "Creating table: monthly_weather_statistics\n",
      "Creating table: multilingual_greetings_and_phrases\n",
      "Creating table: ohio_private_schools_summary\n",
      "Creating table: cancer_related_genetic_factors\n"
     ]
    }
   ],
   "source": [
    "# put data into sqlite db\n",
    "from sqlalchemy import (\n",
    "    create_engine,\n",
    "    MetaData,\n",
    "    Table,\n",
    "    Column,\n",
    "    String,\n",
    "    Integer,\n",
    ")\n",
    "import re\n",
    "\n",
    "\n",
    "# Function to create a sanitized column name\n",
    "def sanitize_column_name(col_name):\n",
    "    # Remove special characters and replace spaces with underscores\n",
    "    return re.sub(r\"\\W+\", \"_\", col_name)\n",
    "\n",
    "\n",
    "# Function to create a table from a DataFrame using SQLAlchemy\n",
    "def create_table_from_dataframe(df: pd.DataFrame, table_name: str, engine, metadata_obj):\n",
    "    # Sanitize column names\n",
    "    sanitized_columns = {col: sanitize_column_name(col) for col in df.columns}\n",
    "    df = df.rename(columns=sanitized_columns)\n",
    "\n",
    "    # Dynamically create columns based on DataFrame columns and data types\n",
    "    columns = [\n",
    "        Column(col, String if dtype == \"object\" else Integer)\n",
    "        for col, dtype in zip(df.columns, df.dtypes)\n",
    "    ]\n",
    "\n",
    "    # Create a table with the defined columns\n",
    "    table = Table(table_name, metadata_obj, *columns)\n",
    "\n",
    "    # Create the table in the database\n",
    "    metadata_obj.create_all(engine)\n",
    "\n",
    "    # Insert data from DataFrame into the table\n",
    "    with engine.connect() as conn:\n",
    "        for _, row in df.iterrows():\n",
    "            insert_stmt = table.insert().values(**row.to_dict())\n",
    "            conn.execute(insert_stmt)\n",
    "        conn.commit()\n",
    "\n",
    "\n",
    "# engine = create_engine(\"sqlite:///:memory:\")\n",
    "engine = create_engine(\"sqlite:///wiki_table_questions.db\")\n",
    "metadata_obj = MetaData()\n",
    "for idx, df in enumerate(dfs):\n",
    "    tableinfo = _get_tableinfo_with_index(idx)\n",
    "    print(f\"Creating table: {tableinfo.table_name}\")\n",
    "    create_table_from_dataframe(df, tableinfo.table_name, engine, metadata_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b6ab4f-7db7-47ef-ad8a-898b19540d56",
   "metadata": {},
   "source": [
    "### 10.4 æ„å»ºåŸºç¡€å·¥å…·"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2ebdd1-88e4-4e3e-b025-000890ac300e",
   "metadata": {},
   "source": [
    "1. åˆ›å»ºåŸºäºè¡¨çš„æè¿°çš„å‘é‡ç´¢å¼•\n",
    "\n",
    "- `ObjectIndex` æ˜¯ä¸€ä¸ª LlamaIndex å†…ç½®çš„æ¨¡å—ï¼Œé€šè¿‡ç´¢å¼• (Indexï¼‰æ£€ç´¢ä»»æ„ Python å¯¹è±¡\n",
    "- è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ `VectorStoreIndex` ä¹Ÿå°±æ˜¯å‘é‡æ£€ç´¢ï¼Œå¹¶é€šè¿‡ `SQLTableNodeMapping` å°†æ–‡æœ¬æè¿°çš„ `node` å’Œæ•°æ®åº“çš„è¡¨å½¢æˆæ˜ å°„\n",
    "- ç›¸å…³æ–‡æ¡£ï¼šhttps://docs.llamaindex.ai/en/stable/examples/objects/object_index/#the-objectindex-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49e12988-cc89-4618-b3f6-a6a038c3c8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.dashscope import DashScope, DashScopeGenerationModels\n",
    "from llama_index.embeddings.dashscope import DashScopeEmbedding, DashScopeTextEmbeddingModels\n",
    "from llama_index.core.objects import (\n",
    "    SQLTableNodeMapping,\n",
    "    ObjectIndex,\n",
    "    SQLTableSchema,\n",
    ")\n",
    "from llama_index.core import SQLDatabase, VectorStoreIndex\n",
    "\n",
    "# è®¾ç½®å…¨å±€æ¨¡å‹\n",
    "Settings.llm = DashScope(model_name=DashScopeGenerationModels.QWEN_MAX, api_key=os.getenv(\"DASHSCOPE_API_KEY\"))\n",
    "Settings.embed_model = DashScopeEmbedding(model_name=DashScopeTextEmbeddingModels.TEXT_EMBEDDING_V1)\n",
    "\n",
    "sql_database = SQLDatabase(engine)\n",
    "\n",
    "table_node_mapping = SQLTableNodeMapping(sql_database)\n",
    "table_schema_objs = [\n",
    "    SQLTableSchema(table_name=t.table_name, context_str=t.table_summary)\n",
    "    for t in table_infos\n",
    "]  # add a SQLTableSchema for each table\n",
    "\n",
    "obj_index = ObjectIndex.from_objects(\n",
    "    table_schema_objs,\n",
    "    table_node_mapping,\n",
    "    VectorStoreIndex,\n",
    ")\n",
    "obj_retriever = obj_index.as_retriever(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef92b78b-0530-468d-90ec-309d1614d390",
   "metadata": {},
   "source": [
    "2. åˆ›å»º SQL æŸ¥è¯¢å™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "948b8bee-2af6-4487-8b27-a2b2a5d7420b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import SQLRetriever\n",
    "from typing import List\n",
    "\n",
    "sql_retriever = SQLRetriever(sql_database)\n",
    "\n",
    "\n",
    "def get_table_context_str(table_schema_objs: List[SQLTableSchema]):\n",
    "    \"\"\"Get table context string.\"\"\"\n",
    "    context_strs = []\n",
    "    for table_schema_obj in table_schema_objs:\n",
    "        table_info = sql_database.get_single_table_info(\n",
    "            table_schema_obj.table_name\n",
    "        )\n",
    "        if table_schema_obj.context_str:\n",
    "            table_opt_context = \" The table description is: \"\n",
    "            table_opt_context += table_schema_obj.context_str\n",
    "            table_info += table_opt_context\n",
    "\n",
    "        context_strs.append(table_info)\n",
    "    return \"\\n\\n\".join(context_strs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d5bac2-4c10-4411-bcd1-d825727df41f",
   "metadata": {},
   "source": [
    "3. åˆ›å»º Text2SQL çš„æç¤ºè¯ï¼ˆç³»ç»Ÿé»˜è®¤æ¨¡æ¿ï¼‰ï¼Œå’Œè¾“å‡ºç»“æœè§£æå™¨ï¼ˆä»ç”Ÿæˆçš„æ–‡æœ¬ä¸­æŠ½å–SQLï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da0c9ea7-5aa5-4069-8baf-9ca62c9e278a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given an input question, first create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer. You can order the results by a relevant column to return the most interesting examples in the database.\n",
      "\n",
      "Never query for all the columns from a specific table, only ask for a few relevant columns given the question.\n",
      "\n",
      "Pay attention to use only the column names that you can see in the schema description. Be careful to not query for columns that do not exist. Pay attention to which column is in which table. Also, qualify column names with the table name when needed. You are required to use the following format, each taking one line:\n",
      "\n",
      "Question: Question here\n",
      "SQLQuery: SQL Query to run\n",
      "SQLResult: Result of the SQLQuery\n",
      "Answer: Final answer here\n",
      "\n",
      "Only use tables listed below.\n",
      "{schema}\n",
      "\n",
      "Question: {query_str}\n",
      "SQLQuery: \n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.prompts.default_prompts import DEFAULT_TEXT_TO_SQL_PROMPT\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.core.llms import ChatResponse\n",
    "\n",
    "def parse_response_to_sql(chat_response: ChatResponse) -> str:\n",
    "    \"\"\"Parse response to SQL.\"\"\"\n",
    "    response = chat_response.message.content\n",
    "    sql_query_start = response.find(\"SQLQuery:\")\n",
    "    if sql_query_start != -1:\n",
    "        response = response[sql_query_start:]\n",
    "        # TODO: move to removeprefix after Python 3.9+\n",
    "        if response.startswith(\"SQLQuery:\"):\n",
    "            response = response[len(\"SQLQuery:\") :]\n",
    "    sql_result_start = response.find(\"SQLResult:\")\n",
    "    if sql_result_start != -1:\n",
    "        response = response[:sql_result_start]\n",
    "    return response.strip().strip(\"```\").strip()\n",
    "\n",
    "\n",
    "text2sql_prompt = DEFAULT_TEXT_TO_SQL_PROMPT.partial_format(\n",
    "    dialect=engine.dialect.name\n",
    ")\n",
    "print(text2sql_prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691c7cf0-1217-4d16-8196-2937411bbbed",
   "metadata": {},
   "source": [
    "4. åˆ›å»ºè‡ªç„¶è¯­è¨€å›å¤ç”Ÿæˆæ¨¡æ¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1fbf506-9c46-44bd-9148-0712e4e835a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_synthesis_prompt_str = (\n",
    "    \"Given an input question, synthesize a response from the query results.\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"SQL: {sql_query}\\n\"\n",
    "    \"SQL Response: {context_str}\\n\"\n",
    "    \"Response: \"\n",
    ")\n",
    "response_synthesis_prompt = PromptTemplate(\n",
    "    response_synthesis_prompt_str,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f4b9121-7da6-496c-8506-48fee7d4ac5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = DashScope(model_name=DashScopeGenerationModels.QWEN_MAX, api_key=os.getenv(\"DASHSCOPE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258cc672-bed9-45b6-a9cd-80d162f96fd1",
   "metadata": {},
   "source": [
    "### 10.5 å®šä¹‰å·¥ä½œæµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ac5f9db-6dc7-4828-bd2b-1ff7d1e16abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    Workflow,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    step,\n",
    "    Context,\n",
    "    Event,\n",
    ")\n",
    "\n",
    "# äº‹ä»¶ï¼šæ‰¾åˆ°æ•°æ®åº“ä¸­ç›¸å…³çš„è¡¨\n",
    "class TableRetrieveEvent(Event):\n",
    "    \"\"\"Result of running table retrieval.\"\"\"\n",
    "\n",
    "    table_context_str: str\n",
    "    query: str\n",
    "\n",
    "# äº‹ä»¶ï¼šæ–‡æœ¬è½¬ SQL\n",
    "class TextToSQLEvent(Event):\n",
    "    \"\"\"Text-to-SQL event.\"\"\"\n",
    "\n",
    "    sql: str\n",
    "    query: str\n",
    "\n",
    "\n",
    "class TextToSQLWorkflow1(Workflow):\n",
    "    \"\"\"Text-to-SQL Workflow that does query-time table retrieval.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        obj_retriever,\n",
    "        text2sql_prompt,\n",
    "        sql_retriever,\n",
    "        response_synthesis_prompt,\n",
    "        llm,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.obj_retriever = obj_retriever\n",
    "        self.text2sql_prompt = text2sql_prompt\n",
    "        self.sql_retriever = sql_retriever\n",
    "        self.response_synthesis_prompt = response_synthesis_prompt\n",
    "        self.llm = llm\n",
    "\n",
    "    @step\n",
    "    def retrieve_tables(\n",
    "        self, ctx: Context, ev: StartEvent\n",
    "    ) -> TableRetrieveEvent:\n",
    "        \"\"\"Retrieve tables.\"\"\"\n",
    "        table_schema_objs = self.obj_retriever.retrieve(ev.query)\n",
    "        table_context_str = get_table_context_str(table_schema_objs)\n",
    "        print(\"====\\n\"+table_context_str+\"\\n====\")\n",
    "        return TableRetrieveEvent(\n",
    "            table_context_str=table_context_str, query=ev.query\n",
    "        )\n",
    "\n",
    "    @step\n",
    "    def generate_sql(\n",
    "        self, ctx: Context, ev: TableRetrieveEvent\n",
    "    ) -> TextToSQLEvent:\n",
    "        \"\"\"Generate SQL statement.\"\"\"\n",
    "        fmt_messages = self.text2sql_prompt.format_messages(\n",
    "            query_str=ev.query, schema=ev.table_context_str\n",
    "        )\n",
    "        chat_response = self.llm.chat(fmt_messages)\n",
    "        sql = parse_response_to_sql(chat_response)\n",
    "        print(\"====\\n\"+sql+\"\\n====\")\n",
    "        return TextToSQLEvent(sql=sql, query=ev.query)\n",
    "\n",
    "    @step\n",
    "    def generate_response(self, ctx: Context, ev: TextToSQLEvent) -> StopEvent:\n",
    "        \"\"\"Run SQL retrieval and generate response.\"\"\"\n",
    "        retrieved_rows = self.sql_retriever.retrieve(ev.sql)\n",
    "        print(\"====\\n\"+str(retrieved_rows)+\"\\n====\")\n",
    "        fmt_messages = self.response_synthesis_prompt.format_messages(\n",
    "            sql_query=ev.sql,\n",
    "            context_str=str(retrieved_rows),\n",
    "            query_str=ev.query,\n",
    "        )\n",
    "        chat_response = llm.chat(fmt_messages)\n",
    "        return StopEvent(result=chat_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b3060a2-20e4-4153-b3f9-6b91ed4d7020",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = TextToSQLWorkflow1(\n",
    "    obj_retriever,\n",
    "    text2sql_prompt,\n",
    "    sql_retriever,\n",
    "    response_synthesis_prompt,\n",
    "    llm,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ead701cd-c79d-4b48-8a20-9c2322140712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====\n",
      "Table 'bad_boy_artists_album_release_summary' has columns: Act (VARCHAR), Year_signed (INTEGER), _Albums_released_under_Bad_Boy (VARCHAR), . The table description is: Summary of artists signed to Bad Boy Records and their album releases.\n",
      "\n",
      "Table 'best_actress_award_nominations_and_wins' has columns: Year (INTEGER), Award (VARCHAR), Film (VARCHAR), Result (VARCHAR), . The table description is: Summary of Best Actress award nominations and wins from 1976 to 2006.\n",
      "\n",
      "Table 'grammy_awards_summary_for_artist' has columns: Year (INTEGER), Award (VARCHAR), Work_Artist (VARCHAR), Result (VARCHAR), . The table description is: Summary of Grammy Award nominations and wins for a specific artist over the years.\n",
      "====\n",
      "====\n",
      "SELECT Year_signed FROM bad_boy_artists_album_release_summary WHERE Act = 'The Notorious B.I.G'\n",
      "====\n",
      "====\n",
      "[NodeWithScore(node=TextNode(id_='7a94b96b-7d07-4611-94f4-e62a06416fa9', embedding=None, metadata={'sql_query': \"SELECT Year_signed FROM bad_boy_artists_album_release_summary WHERE Act = 'The Notorious B.I.G'\", 'result': [(1993,), (1993,), (1993,), (1993,), (1993,), (1993,), (1993,), (1993,), (1993,), (1993,), (1993,), (1993,)], 'col_keys': ['Year_signed']}, excluded_embed_metadata_keys=['sql_query', 'result', 'col_keys'], excluded_llm_metadata_keys=['sql_query', 'result', 'col_keys'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text='[(1993,), (1993,), (1993,), (1993,), (1993,), (1993,), (1993,), (1993,), (1993,), (1993,), (1993,), (1993,)]', mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=None)]\n",
      "====\n",
      "assistant: The Notorious B.I.G. was signed to Bad Boy in the year 1993.\n"
     ]
    }
   ],
   "source": [
    "response = await workflow.run(\n",
    "    query=\"What was the year that The Notorious B.I.G was signed to Bad Boy?\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b64fac-b8b8-4fad-8ecc-5a9431469706",
   "metadata": {},
   "source": [
    "### 10.6 å¯è§†åŒ–å·¥ä½œæµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b370ab8-9c87-4baa-92ba-3e729a481857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-index-utils-workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "933d00bd-a602-4958-bed3-d1b530192895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_to_sql_table_retrieval.html\n"
     ]
    }
   ],
   "source": [
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "\n",
    "draw_all_possible_flows(\n",
    "    TextToSQLWorkflow1, filename=\"text_to_sql_table_retrieval.html\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fc8317-0cac-458b-9ee5-969e4b46442c",
   "metadata": {},
   "source": [
    "### 10.7 å·¥ä½œæµç®¡ç†æ¡†æ¶æ„ä¹‰æ˜¯ä»€ä¹ˆ\n",
    "\n",
    "æ€è€ƒä»¥ä¸‹æƒ…å†µï¼š\n",
    "\n",
    "- `step` çš„æ‰§è¡Œé¡ºåºæœ‰é€»è¾‘åˆ†æ”¯\n",
    "- `step` çš„æ‰§è¡Œæœ‰å¾ªç¯\n",
    "- `step` çš„æ‰§è¡Œå¯ä»¥å¹¶è¡Œ\n",
    "- ä¸€ä¸ª `step` çš„è§¦å‘æ¡ä»¶ä¾èµ–å‰é¢è‹¥å¹² `step` çš„ç»“æœï¼Œä¸”å®ƒä»¬ä¹‹é—´å¯èƒ½æœ‰å¾ªç¯æˆ–è€…å¹¶è¡Œ\n",
    "\n",
    "<img src=\"./assets/workflow2.png\" alt=\"å·¥ä½œæµä¸¾ä¾‹\" width=\"800\"/>\n",
    "\n",
    "æ‰€ä»¥ï¼Œå·¥ä½œæµç®¡ç†æ¡†æ¶çš„æ„æ€æ˜¯ä¾¿äºå°†å•ä¸ªäº‹ä»¶çš„å¤„ç†é€»è¾‘å’Œäº‹ä»¶ä¹‹é—´çš„æ‰§è¡Œé¡ºåºç‹¬ç«‹å¼€\n",
    "\n",
    "å…³äº LlamaIndex å·¥ä½œæµçš„æ›´è¯¦ç»†æ–‡æ¡£ï¼šhttps://docs.llamaindex.ai/en/stable/examples/workflow/workflows_cookbook/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98c98c0-6636-4d71-9156-731e7ca28b6f",
   "metadata": {},
   "source": [
    "## 11. LlamaIndex çš„æ›´å¤šåŠŸèƒ½\n",
    "\n",
    "- æ™ºèƒ½ä½“ï¼ˆAgentï¼‰å¼€å‘æ¡†æ¶ï¼šhttps://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/\n",
    "- RAG çš„è¯„æµ‹ï¼šhttps://docs.llamaindex.ai/en/stable/module_guides/evaluating/\n",
    "- è¿‡ç¨‹ç›‘æ§ï¼šhttps://docs.llamaindex.ai/en/stable/module_guides/observability/\n",
    "\n",
    "ä»¥ä¸Šå†…å®¹æ¶‰åŠè¾ƒå¤šèƒŒæ™¯çŸ¥è¯†ï¼Œæš‚æ—¶ä¸åœ¨æœ¬è¯¾å±•å¼€ï¼Œç›¸å…³çŸ¥è¯†ä¼šåœ¨åé¢è¯¾ç¨‹ä¸­é€ä¸€è¯¦ç»†è®²è§£ã€‚\n",
    "\n",
    "æ­¤å¤–ï¼ŒLlamaIndex é’ˆå¯¹ç”Ÿäº§çº§çš„ RAG ç³»ç»Ÿä¸­é‡åˆ°çš„å„ä¸ªæ–¹é¢çš„ç»†èŠ‚é—®é¢˜ï¼Œæ€»ç»“äº†å¾ˆå¤šé«˜ç«¯æŠ€å·§ï¼ˆ[Advanced Topics](https://docs.llamaindex.ai/en/stable/optimizing/production_rag/)ï¼‰ï¼Œå¯¹å®æˆ˜å¾ˆæœ‰å‚è€ƒä»·å€¼ï¼Œéå¸¸æ¨èæœ‰èƒ½åŠ›çš„åŒå­¦é˜…è¯»ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e9b8c6",
   "metadata": {},
   "source": [
    "## 12. å­¦ä¹ æ‰“å¡\n",
    "\n",
    "1. æŒæ¡ LlamaIndex æ¡†æ¶æ ¸å¿ƒæ¨¡å—\n",
    "2. ç†Ÿç»ƒä½¿ç”¨ LlamaIndex é«˜æ•ˆå¼€å‘ä¸€ä¸ªè´´åˆè‡ªå·±éœ€æ±‚çš„RAGç³»ç»Ÿ\n",
    "3. ç†è§£ LlamaIndex ä¸­çš„å·¥ä½œæµï¼ˆWorkflowï¼‰å®ç°\n",
    "4. ç»“åˆè‡ªå·±çš„ä¸šåŠ¡åœºæ™¯ï¼Œé€šè¿‡NL2SQLæŠ€æœ¯æ¥å®ç°ä¸€ä¸ªåŠŸèƒ½"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
