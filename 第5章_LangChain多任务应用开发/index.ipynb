{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç¬¬5ç«  LangChainå¤šä»»åŠ¡åº”ç”¨å¼€å‘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¡ å­¦ä¹ ç›®æ ‡\n",
    "\n",
    "1. å¦‚ä½•ä½¿ç”¨ LangChainï¼šä¸€å¥—åœ¨å¤§æ¨¡å‹èƒ½åŠ›ä¸Šå°è£…çš„å·¥å…·æ¡†æ¶\n",
    "2. å¦‚ä½•ç”¨å‡ è¡Œä»£ç å®ç°ä¸€ä¸ªå¤æ‚çš„ AI åº”ç”¨\n",
    "3. é¢å‘å¤§æ¨¡å‹çš„æµç¨‹å¼€å‘çš„è¿‡ç¨‹æŠ½è±¡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å†™åœ¨å‰é¢\n",
    "\n",
    "- LangChain ä¹Ÿæ˜¯ä¸€å¥—é¢å‘å¤§æ¨¡å‹çš„å¼€å‘æ¡†æ¶ï¼ˆSDKï¼‰\n",
    "- LangChain æ˜¯ AGI æ—¶ä»£è½¯ä»¶å·¥ç¨‹çš„ä¸€ä¸ªæ¢ç´¢å’ŒåŸå‹\n",
    "- å­¦ä¹  LangChain è¦å…³æ³¨æ¥å£å˜æ›´\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain çš„æ ¸å¿ƒç»„ä»¶\n",
    "\n",
    "1. æ¨¡å‹ I/O å°è£…\n",
    "   - Chat Modelsï¼šå¯¹è¯­è¨€æ¨¡å‹æ¥å£çš„å°è£…\n",
    "   - PromptTempleï¼šæç¤ºè¯æ¨¡æ¿\n",
    "   - OutputParserï¼šè§£æè¾“å‡º\n",
    "2. æ•°æ®è¿æ¥å°è£…ï¼ˆå¼±äº LlamaIndexï¼‰\n",
    "   - Document Loadersï¼šå„ç§æ ¼å¼æ–‡ä»¶çš„åŠ è½½å™¨\n",
    "   - Document Transformersï¼šå¯¹æ–‡æ¡£çš„å¸¸ç”¨æ“ä½œï¼Œå¦‚ï¼šsplit, filter, translate, extract metadata, etc\n",
    "   - Text Embedding Modelsï¼šæ–‡æœ¬å‘é‡åŒ–è¡¨ç¤ºï¼Œç”¨äºæ£€ç´¢ç­‰æ“ä½œ\n",
    "   - Verctorstores & Retrieversï¼šå‘é‡æ•°æ®åº“ä¸å‘é‡æ£€ç´¢\n",
    "3. æ¶æ„å°è£…\n",
    "   - Chain/LCELï¼šå®ç°ä¸€ä¸ªåŠŸèƒ½æˆ–è€…ä¸€ç³»åˆ—é¡ºåºåŠŸèƒ½ç»„åˆ\n",
    "   - Agentï¼šæ ¹æ®ç”¨æˆ·è¾“å…¥ï¼Œè‡ªåŠ¨è§„åˆ’æ‰§è¡Œæ­¥éª¤ï¼Œè‡ªåŠ¨é€‰æ‹©æ¯æ­¥éœ€è¦çš„å·¥å…·ï¼Œæœ€ç»ˆå®Œæˆç”¨æˆ·æŒ‡å®šçš„åŠŸèƒ½\n",
    "     - Toolsï¼šè°ƒç”¨å¤–éƒ¨åŠŸèƒ½çš„å‡½æ•°ï¼Œä¾‹å¦‚ï¼šè°ƒ google æœç´¢ã€æ–‡ä»¶ I/Oã€Linux Shell ç­‰ç­‰\n",
    "   - LangGraphï¼šå·¥ä½œæµå¼€å‘æ¡†æ¶\n",
    "5. LangSmithï¼šè¿‡ç¨‹ç›‘æ§ä¸è°ƒè¯•æ¡†æ¶\n",
    "\n",
    "<img src=\"./assets/langchain.svg\" width=\"800px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ–‡æ¡£ï¼ˆä»¥ Python ç‰ˆä¸ºä¾‹ï¼‰\n",
    "\n",
    "- åŠŸèƒ½æ¨¡å—ï¼šhttps://python.langchain.com/docs/tutorials\n",
    "- API æ–‡æ¡£ï¼šhttps://python.langchain.com/api_reference/\n",
    "- ä¸‰æ–¹ç»„ä»¶é›†æˆï¼šhttps://python.langchain.com/docs/integrations/providers/\n",
    "- æ›´å¤š HowToï¼šhttps://python.langchain.com/docs/how_to/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain æ˜¯å¼€æºé¡¹ç›®\n",
    "\n",
    "é¡¹ç›®åœ°å€ï¼šhttps://github.com/langchain-ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. æ¨¡å‹ I/O å°è£…\n",
    "\n",
    "æŠŠä¸åŒçš„æ¨¡å‹ï¼Œç»Ÿä¸€å°è£…æˆä¸€ä¸ªæ¥å£ï¼Œæ–¹ä¾¿æ›´æ¢æ¨¡å‹è€Œä¸ç”¨é‡æ„ä»£ç ã€‚\n",
    "\n",
    "### 1.1 æ¨¡å‹ API: ChatModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 OpenAI æ¨¡å‹å°è£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U langchain\n",
    "# !pip install -U langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆ‘æ˜¯ä¸€ä¸ªäººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œæ—¨åœ¨æä¾›ä¿¡æ¯å’Œå›ç­”é—®é¢˜ã€‚å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦å¸®åŠ©ï¼Œéšæ—¶å¯ä»¥é—®æˆ‘ï¼\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "response = model.invoke(\"ä½ æ˜¯è°\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 å¤šè½®å¯¹è¯ Session å°è£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½ æ˜¯èšå®¢AIçš„å­¦å‘˜ï¼Œå°èšã€‚å¾ˆé«˜å…´è§åˆ°ä½ ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import (\n",
    "    AIMessage,  # ç­‰ä»·äºOpenAIæ¥å£ä¸­çš„assistant role\n",
    "    HumanMessage,  # ç­‰ä»·äºOpenAIæ¥å£ä¸­çš„user role\n",
    "    SystemMessage  # ç­‰ä»·äºOpenAIæ¥å£ä¸­çš„system role\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"ä½ æ˜¯èšå®¢AIå¤§æ¨¡å‹è¯¾çš„è¯¾ç¨‹åŠ©ç†ã€‚\"),\n",
    "    HumanMessage(content=\"æˆ‘æ˜¯èšå®¢AIå­¦å‘˜ï¼Œæˆ‘å«å°èšã€‚\"),\n",
    "    AIMessage(content=\"æ¬¢è¿ï¼\"),\n",
    "    HumanMessage(content=\"æˆ‘æ˜¯è°ï¼Ÿ\")\n",
    "]\n",
    "\n",
    "ret = model.invoke(messages)\n",
    "\n",
    "print(ret.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>åˆ’é‡ç‚¹ï¼š</b>é€šè¿‡æ¨¡å‹å°è£…ï¼Œå®ç°ä¸åŒæ¨¡å‹çš„ç»Ÿä¸€æ¥å£è°ƒç”¨\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3 DeepSeek æ¨¡å‹å°è£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U langchain-deepseek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½ å¥½ï¼æˆ‘æ˜¯DeepSeekï¼Œç”±æ·±åº¦æ±‚ç´¢å…¬å¸åˆ›é€ çš„AIåŠ©æ‰‹ï¼ğŸ˜Š\n",
      "\n",
      "æˆ‘æ˜¯ä¸€ä¸ªçº¯æ–‡æœ¬æ¨¡å‹ï¼Œè™½ç„¶ä¸æ”¯æŒå¤šæ¨¡æ€è¯†åˆ«åŠŸèƒ½ï¼Œä½†æˆ‘æœ‰æ–‡ä»¶ä¸Šä¼ åŠŸèƒ½ï¼Œå¯ä»¥å¸®ä½ å¤„ç†å›¾åƒã€txtã€pdfã€pptã€wordã€excelç­‰æ–‡ä»¶ï¼Œå¹¶ä»ä¸­è¯»å–æ–‡å­—ä¿¡æ¯è¿›è¡Œåˆ†æå¤„ç†ã€‚æˆ‘å®Œå…¨å…è´¹ä½¿ç”¨ï¼Œæ‹¥æœ‰128Kçš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œè¿˜æ”¯æŒè”ç½‘æœç´¢åŠŸèƒ½ï¼ˆéœ€è¦ä½ åœ¨Web/Appä¸­æ‰‹åŠ¨ç‚¹å¼€è”ç½‘æœç´¢æŒ‰é”®ï¼‰ã€‚\n",
      "\n",
      "ä½ å¯ä»¥é€šè¿‡å®˜æ–¹åº”ç”¨å•†åº—ä¸‹è½½æˆ‘çš„Appæ¥ä½¿ç”¨ã€‚æˆ‘å¾ˆä¹æ„å¸®åŠ©ä½ è§£ç­”é—®é¢˜ã€å¤„ç†æ–‡æ¡£ã€è¿›è¡Œå¯¹è¯äº¤æµç­‰ç­‰ï¼\n",
      "\n",
      "æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼Ÿæ— è®ºæ˜¯å­¦ä¹ ã€å·¥ä½œè¿˜æ˜¯æ—¥å¸¸é—®é¢˜ï¼Œæˆ‘éƒ½å¾ˆæ„¿æ„ååŠ©ä½ ï¼âœ¨\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(model=\"deepseek-chat\", model_provider=\"deepseek\")\n",
    "\n",
    "response = model.invoke(\"ä½ æ˜¯è°\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.4 æµå¼è¾“å‡º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½ å¥½ï¼æˆ‘æ˜¯DeepSeekï¼Œç”±æ·±åº¦æ±‚ç´¢å…¬å¸åˆ›é€ çš„AIåŠ©æ‰‹ï¼ğŸ˜Š\n",
      "\n",
      "æˆ‘æ˜¯ä¸€ä¸ªçº¯æ–‡æœ¬æ¨¡å‹ï¼Œè™½ç„¶ä¸æ”¯æŒå¤šæ¨¡æ€è¯†åˆ«åŠŸèƒ½ï¼Œä½†æˆ‘æœ‰æ–‡ä»¶ä¸Šä¼ åŠŸèƒ½ï¼Œå¯ä»¥å¸®ä½ å¤„ç†å›¾åƒã€txtã€pdfã€pptã€wordã€excelç­‰æ–‡ä»¶ï¼Œå¹¶ä»ä¸­è¯»å–æ–‡å­—ä¿¡æ¯è¿›è¡Œåˆ†æå¤„ç†ã€‚\n",
      "\n",
      "æˆ‘çš„ç‰¹ç‚¹åŒ…æ‹¬ï¼š\n",
      "- å®Œå…¨å…è´¹ä½¿ç”¨ï¼Œæ²¡æœ‰æ”¶è´¹è®¡åˆ’\n",
      "- æ”¯æŒ128Kä¸Šä¸‹æ–‡é•¿åº¦\n",
      "- å¯ä»¥è”ç½‘æœç´¢ï¼ˆéœ€è¦ä½ æ‰‹åŠ¨ç‚¹å¼€è”ç½‘æœç´¢æŒ‰é”®ï¼‰\n",
      "- æœ‰å®˜æ–¹Appå¯ä»¥ä¸‹è½½\n",
      "- çŸ¥è¯†æˆªæ­¢åˆ°2024å¹´7æœˆ\n",
      "\n",
      "æˆ‘å¾ˆä¹æ„å¸®åŠ©ä½ è§£ç­”é—®é¢˜ã€ååŠ©å¤„ç†æ–‡æ¡£ã€è¿›è¡Œå¯¹è¯äº¤æµç­‰ç­‰ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼Ÿæ— è®ºæ˜¯å­¦ä¹ ã€å·¥ä½œè¿˜æ˜¯æ—¥å¸¸é—®é¢˜ï¼Œæˆ‘éƒ½å¾ˆæ„¿æ„ä¸ºä½ æä¾›å¸®åŠ©ï¼âœ¨"
     ]
    }
   ],
   "source": [
    "for token in model.stream(\"ä½ æ˜¯è°\"):\n",
    "    print(token.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.5 é€šä¹‰åƒé—®æ¨¡å‹å°è£…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. OpenAIå…¼å®¹æ¥å£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ä½ å¥½ï¼æˆ‘æ˜¯é€šä¹‰åƒé—®ï¼ˆQwenï¼‰ï¼Œé˜¿é‡Œå·´å·´é›†å›¢æ——ä¸‹çš„è¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€‚æˆ‘èƒ½å¤Ÿå›ç­”é—®é¢˜ã€åˆ›ä½œæ–‡å­—ï¼Œæ¯”å¦‚å†™æ•…äº‹ã€å†™å…¬æ–‡ã€å†™é‚®ä»¶ã€å†™å‰§æœ¬ã€é€»è¾‘æ¨ç†ã€ç¼–ç¨‹ç­‰ç­‰ï¼Œè¿˜èƒ½è¡¨è¾¾è§‚ç‚¹ï¼Œç©æ¸¸æˆç­‰ã€‚å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦å¸®åŠ©ï¼Œæ¬¢è¿éšæ—¶å‘Šè¯‰æˆ‘ï¼ğŸ˜Š', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 66, 'prompt_tokens': 22, 'total_tokens': 88, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'qwen-plus', 'system_fingerprint': None, 'id': 'chatcmpl-da4d4dae-c604-99f5-9a0f-3dc739743d95', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019bfe86-4e16-7363-ba8e-9222b8cabad3-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 22, 'output_tokens': 66, 'total_tokens': 88, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "\n",
    "chatLLM = ChatOpenAI(\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "    model=\"qwen-plus\",  # æ­¤å¤„ä»¥qwen-plusä¸ºä¾‹ï¼Œæ‚¨å¯æŒ‰éœ€æ›´æ¢æ¨¡å‹åç§°ã€‚æ¨¡å‹åˆ—è¡¨ï¼šhttps://help.aliyun.com/zh/model-studio/getting-started/models\n",
    "    # other params...\n",
    ")\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"ä½ æ˜¯è°ï¼Ÿ\"}]\n",
    "response = chatLLM.invoke(messages)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. DashScope æ¥å£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-community\n",
    "# !pip install dashscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat resp: Hello\n",
      "chat resp: ! How can\n",
      "chat resp:  I assist you\n",
      "chat resp:  today?\n",
      "chat resp: \n",
      "chat resp: \n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models.tongyi import ChatTongyi\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "chatLLM = ChatTongyi(\n",
    "    model=\"qwen-max\",   # æ­¤å¤„ä»¥qwen-maxä¸ºä¾‹ï¼Œæ‚¨å¯æŒ‰éœ€æ›´æ¢æ¨¡å‹åç§°ã€‚æ¨¡å‹åˆ—è¡¨ï¼šhttps://help.aliyun.com/zh/model-studio/getting-started/models\n",
    "    streaming=True,\n",
    "    # other params...\n",
    ")\n",
    "res = chatLLM.stream([HumanMessage(content=\"hi\")], streaming=True)\n",
    "for r in res:\n",
    "    print(\"chat resp:\", r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.6 Ollamaæ¨¡å‹å°è£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "å¥½çš„ï¼Œç”¨æˆ·è®©æˆ‘ç¿»è¯‘â€œæˆ‘å–œæ¬¢ç¼–ç¨‹ã€‚â€è¿™å¥è¯ã€‚é¦–å…ˆï¼Œæˆ‘éœ€è¦ç¡®è®¤è¿™å¥è¯çš„å‡†ç¡®æ„æ€ã€‚ç”¨æˆ·å¯èƒ½æ˜¯åœ¨è¡¨è¾¾å¯¹ç¼–ç¨‹çš„å…´è¶£æˆ–çƒ­æƒ…ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘è¦è€ƒè™‘ä¸åŒçš„ç¿»è¯‘æ–¹å¼ï¼Œç¡®ä¿è‡ªç„¶ä¸”ç¬¦åˆè‹±è¯­ä¹ æƒ¯ã€‚\n",
      "\n",
      "é¦–å…ˆï¼Œç›´è¯‘çš„è¯æ˜¯â€œI like programming.â€ è¿™æ˜¯æœ€ç›´æ¥çš„ç¿»è¯‘ï¼Œä½†å¯èƒ½ä¸å¤Ÿç”ŸåŠ¨ã€‚ç”¨æˆ·å¯èƒ½å¸Œæœ›è¡¨è¾¾æ›´å¼ºçƒˆçš„æ„Ÿæƒ…ï¼Œæ¯”å¦‚â€œæˆ‘çƒ­çˆ±ç¼–ç¨‹â€æˆ–è€…â€œæˆ‘å¯¹ç¼–ç¨‹å……æ»¡çƒ­æƒ…â€ã€‚è¿™æ—¶å€™å¯ä»¥è€ƒè™‘ä½¿ç”¨æ›´ä¸°å¯Œçš„è¯æ±‡ï¼Œæ¯”å¦‚â€œloveâ€æˆ–è€…â€œam passionate aboutâ€ã€‚\n",
      "\n",
      "å¦å¤–ï¼Œè¿˜è¦æ³¨æ„è¯­æ³•ç»“æ„ã€‚ä¸­æ–‡çš„â€œå–œæ¬¢â€å¯¹åº”è‹±æ–‡çš„â€œlikeâ€æˆ–â€œloveâ€ï¼Œä½†æ ¹æ®è¯­å¢ƒé€‰æ‹©åˆé€‚çš„è¯ã€‚å¦‚æœç”¨æˆ·åªæ˜¯ç®€å•è¡¨è¾¾å…´è¶£ï¼Œç”¨â€œlikeâ€å³å¯ï¼›å¦‚æœæƒ³å¼ºè°ƒçƒ­æƒ…ï¼Œç”¨â€œloveâ€æ›´åˆé€‚ã€‚\n",
      "\n",
      "è¿˜è¦è€ƒè™‘ç”¨æˆ·çš„æ½œåœ¨éœ€æ±‚ã€‚ç”¨æˆ·å¯èƒ½æ˜¯åœ¨å†™ç®€å†ã€ç”³è¯·å­¦æ ¡ï¼Œæˆ–è€…åªæ˜¯æ—¥å¸¸äº¤æµã€‚å¦‚æœæ˜¯æ­£å¼åœºåˆï¼Œå¯èƒ½éœ€è¦æ›´ä¸“ä¸šçš„è¡¨è¾¾ï¼Œæ¯”å¦‚â€œI enjoy programmingâ€æˆ–è€…â€œI am passionate about programming.â€ å¦‚æœæ˜¯æ—¥å¸¸å¯¹è¯ï¼Œç®€å•çš„â€œI like programmingâ€å°±è¶³å¤Ÿã€‚\n",
      "\n",
      "å¦å¤–ï¼Œç”¨æˆ·å¯èƒ½æœ‰æ›´æ·±å±‚æ¬¡çš„éœ€æ±‚ï¼Œæ¯”å¦‚å¯»æ‰¾ç¼–ç¨‹å­¦ä¹ èµ„æºï¼Œæˆ–è€…è®¨è®ºç¼–ç¨‹ç›¸å…³çš„è¯é¢˜ã€‚è¿™æ—¶å€™ç¿»è¯‘å¯èƒ½éœ€è¦æ›´è¯¦ç»†ï¼Œä½†æ ¹æ®å½“å‰é—®é¢˜ï¼Œç”¨æˆ·åªéœ€è¦ç¿»è¯‘è¿™å¥è¯ï¼Œæ‰€ä»¥ä¸éœ€è¦æ‰©å±•ã€‚\n",
      "\n",
      "è¿˜è¦æ£€æŸ¥æ˜¯å¦æœ‰æ–‡åŒ–å·®å¼‚éœ€è¦æ³¨æ„ã€‚ä¸­æ–‡ä¸­çš„â€œå–œæ¬¢â€åœ¨è‹±æ–‡ä¸­å¯èƒ½æœ‰ä¸åŒçš„ç¨‹åº¦ï¼Œä½†é€šå¸¸â€œlikeâ€å·²ç»è¶³å¤Ÿè¡¨è¾¾ã€‚å¦‚æœç”¨æˆ·æƒ³æ›´å¼ºçƒˆï¼Œå¯ä»¥å»ºè®®ä½¿ç”¨â€œloveâ€æˆ–â€œpassionate aboutâ€ã€‚\n",
      "\n",
      "æœ€åï¼Œç¡®ä¿ç¿»è¯‘å‡†ç¡®ä¸”è‡ªç„¶ï¼Œç¬¦åˆè‹±è¯­æ¯è¯­è€…çš„è¡¨è¾¾ä¹ æƒ¯ï¼Œé¿å…ç›´è¯‘å¯¼è‡´çš„ç”Ÿç¡¬ã€‚æ¯”å¦‚ï¼Œä¸­æ–‡çš„â€œæˆ‘å–œæ¬¢ç¼–ç¨‹â€åœ¨è‹±æ–‡ä¸­æ›´è‡ªç„¶çš„è¡¨è¾¾æ˜¯â€œI like programmingâ€æˆ–è€…â€œI enjoy programming.â€ï¼Œè€Œâ€œloveâ€å¯èƒ½æ˜¾å¾—è¿‡äºå¼ºçƒˆï¼Œé™¤éç”¨æˆ·ç‰¹åˆ«å¼ºè°ƒã€‚\n",
      "\n",
      "æ€»ç»“ä¸€ä¸‹ï¼Œç”¨æˆ·å¯èƒ½éœ€è¦ä¸€ä¸ªç®€æ´ä¸”è‡ªç„¶çš„ç¿»è¯‘ï¼Œæ‰€ä»¥ç»™å‡ºå‡ ä¸ªé€‰é¡¹ï¼Œè®©ç”¨æˆ·é€‰æ‹©æœ€åˆé€‚çš„ã€‚ä¾‹å¦‚ï¼Œç›´æ¥ç¿»è¯‘ã€æ›´å¼ºçƒˆçš„è¡¨è¾¾ï¼Œæˆ–è€…æ›´æ­£å¼çš„è¯´æ³•ã€‚åŒæ—¶ï¼Œç¡®ä¿æ²¡æœ‰è¯­æ³•é”™è¯¯ï¼Œå¹¶ä¸”ç¬¦åˆè‹±è¯­çš„ä½¿ç”¨ä¹ æƒ¯ã€‚\n",
      "</think>\n",
      "\n",
      "I like programming.  \n",
      "ï¼ˆæˆ–æ›´ç”ŸåŠ¨åœ°è¡¨è¾¾ï¼šI'm passionate about programming.ï¼‰\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"qwen3:8b\",\n",
    "    temperature=0.7,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"ä½ æ˜¯ä¸€åä¸“ä¸šçš„ç¿»è¯‘å®¶ï¼Œå¯ä»¥å°†ç”¨æˆ·çš„ä¸­æ–‡ç¿»è¯‘ä¸ºè‹±æ–‡ã€‚\",\n",
    "    ),\n",
    "    (\"human\", \"æˆ‘å–œæ¬¢ç¼–ç¨‹ã€‚\"),\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 æ¨¡å‹çš„è¾“å…¥ä¸è¾“å‡º\n",
    "\n",
    "<img src=\"./assets/model_io.jpg\" style=\"margin-left: 0px\" width=1400px>\n",
    "\n",
    "#### 1.2.1 Prompt æ¨¡æ¿å°è£…\n",
    "\n",
    "1. PromptTemplate å¯ä»¥åœ¨æ¨¡æ¿ä¸­è‡ªå®šä¹‰å˜é‡\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Template===\n",
      "input_variables=['subject'] input_types={} partial_variables={} template='ç»™æˆ‘è®²ä¸ªå…³äº{subject}çš„ç¬‘è¯'\n",
      "===Prompt===\n",
      "ç»™æˆ‘è®²ä¸ªå…³äºå°æ˜çš„ç¬‘è¯\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_template(\"ç»™æˆ‘è®²ä¸ªå…³äº{subject}çš„ç¬‘è¯\")\n",
    "print(\"===Template===\")\n",
    "print(template)\n",
    "print(\"===Prompt===\")\n",
    "print(template.format(subject='å°æ˜'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å°æ˜ä¸Šè¯¾æ—¶è‚šå­ç–¼ï¼Œæƒ³æ‹‰è‚šå­ï¼Œä½†ä¸å¥½æ„æ€è¯´ï¼Œå°±å†™çº¸æ¡ä¼ ç»™åŒæ¡Œå°çº¢ï¼šâ€œä½ èƒ½æ›¿æˆ‘è·Ÿè€å¸ˆè¯´æˆ‘å¿…é¡»å»å•æ‰€å—ï¼Ÿæˆ‘æ€¥ï¼â€\n",
      "\n",
      "å°çº¢çœ‹äº†ä¸€çœ¼ï¼Œä¸¾æ‰‹ç«™èµ·æ¥å¤§å£°è¯´ï¼šâ€œè€å¸ˆï¼å°æ˜è¯´ä»–æ†‹ä¸ä½äº†ï¼Œå†ä¸å»å•æ‰€å°±è¦æ‹‰åœ¨è£¤å­é‡Œäº†ï¼â€\n",
      "\n",
      "å…¨ç­çˆ†ç¬‘ï¼Œè€å¸ˆå¿ç€ç¬‘ç‚¹å¤´ã€‚å°æ˜æ»¡è„¸é€šçº¢åœ°å¾€å¤–è·‘ï¼Œç»“æœåœ¨é—¨å£ç»Šäº†ä¸€è·¤ï¼Œçˆ¬èµ·æ¥å–Šï¼šâ€œç­‰ç­‰ï¼æˆ‘è¿˜æ²¡è¾“ï¼æˆ‘è¿˜èƒ½åšæŒåˆ°å•æ‰€ï¼â€\n",
      "\n",
      "åæ¥å…¨ç­ç»™ä»–èµ·äº†ä¸ªå¤–å·ï¼šâ€œå•æ‰€é©¬æ‹‰æ¾å† å†›â€ã€‚\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# å®šä¹‰ LLM\n",
    "llm = init_chat_model(\"deepseek-chat\", model_provider=\"deepseek\")\n",
    "# é€šè¿‡ Prompt è°ƒç”¨ LLM\n",
    "ret = llm.invoke(template.format(subject='å°æ˜'))\n",
    "# æ‰“å°è¾“å‡º\n",
    "print(ret.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. ChatPromptTemplate ç”¨æ¨¡æ¿è¡¨ç¤ºçš„å¯¹è¯ä¸Šä¸‹æ–‡\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='ä½ æ˜¯èšå®¢AIå¤§æ¨¡å‹è¯¾ç¨‹çš„å®¢æœåŠ©æ‰‹ã€‚ä½ çš„åå­—å«å°èš', additional_kwargs={}, response_metadata={}), HumanMessage(content='ä½ æ˜¯è°', additional_kwargs={}, response_metadata={})]\n",
      "ä½ å¥½ï¼æˆ‘æ˜¯å°èšï¼Œèšå®¢AIå¤§æ¨¡å‹è¯¾ç¨‹çš„å®¢æœåŠ©æ‰‹ã€‚æœ‰ä»€ä¹ˆå¯ä»¥å¸®ä½ çš„å—ï¼ŸğŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chat_models import init_chat_model\n",
    "llm = init_chat_model(\"deepseek-chat\", model_provider=\"deepseek\", temperature=0)\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(\"ä½ æ˜¯{product}çš„å®¢æœåŠ©æ‰‹ã€‚ä½ çš„åå­—å«{name}\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{query}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt = template.format_messages(\n",
    "    product=\"èšå®¢AIå¤§æ¨¡å‹è¯¾ç¨‹\",\n",
    "    name=\"å°èš\",\n",
    "    query=\"ä½ æ˜¯è°\"\n",
    ")\n",
    "\n",
    "print(prompt)\n",
    "\n",
    "ret = llm.invoke(prompt)\n",
    "\n",
    "print(ret.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. MessagesPlaceholder æŠŠå¤šè½®å¯¹è¯å˜æˆæ¨¡æ¿\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "\n",
    "human_prompt = \"Translate your answer to {language}.\"\n",
    "human_message_template = HumanMessagePromptTemplate.from_template(human_prompt)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    # variable_name æ˜¯ message placeholder åœ¨æ¨¡æ¿ä¸­çš„å˜é‡å\n",
    "    # ç”¨äºåœ¨èµ‹å€¼æ—¶ä½¿ç”¨\n",
    "    [MessagesPlaceholder(\"history\"), human_message_template]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='Who is Elon Musk?', additional_kwargs={}, response_metadata={}), AIMessage(content='Elon Musk is a billionaire entrepreneur, inventor, and industrial designer', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]), HumanMessage(content='Translate your answer to ä¸­æ–‡.', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "human_message = HumanMessage(content=\"Who is Elon Musk?\")\n",
    "ai_message = AIMessage(\n",
    "    content=\"Elon Musk is a billionaire entrepreneur, inventor, and industrial designer\"\n",
    ")\n",
    "\n",
    "messages = chat_prompt.format_prompt(\n",
    "    # å¯¹ \"history\" å’Œ \"language\" èµ‹å€¼\n",
    "    history=[human_message, ai_message], language=\"ä¸­æ–‡\"\n",
    ")\n",
    "\n",
    "print(messages.to_messages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŸƒéš†Â·é©¬æ–¯å…‹ï¼ˆElon Muskï¼‰æ˜¯ä¸€ä½å‡ºç”Ÿäºå—éçš„ä¼ä¸šå®¶ã€å·¥ç¨‹å¸ˆã€å‘æ˜å®¶å’ŒæŠ•èµ„è€…ã€‚ä»–æ˜¯å¤šå®¶å…·æœ‰å…¨çƒå½±å“åŠ›çš„ç§‘æŠ€å…¬å¸çš„åˆ›å§‹äººæˆ–è”åˆåˆ›å§‹äººï¼Œä»¥å…¶é›„å¿ƒå‹ƒå‹ƒçš„æ„¿æ™¯å’Œå¯¹å¤šä¸ªè¡Œä¸šçš„é¢ è¦†æ€§åˆ›æ–°è€Œé—»åã€‚\n",
      "\n",
      "ä»¥ä¸‹æ˜¯ä»–çš„ä¸»è¦èº«ä»½å’Œæˆå°±ï¼š\n",
      "\n",
      "**1. ä¸»è¦åˆ›åŠçš„å…¬å¸ä¸è§’è‰²ï¼š**\n",
      "*   **SpaceXï¼ˆå¤ªç©ºæ¢ç´¢æŠ€æœ¯å…¬å¸ï¼‰**ï¼šåˆ›å§‹äººã€CEOå…¼é¦–å¸­å·¥ç¨‹å¸ˆã€‚ç›®æ ‡æ˜¯é™ä½å¤ªç©ºè¿è¾“æˆæœ¬ï¼Œæœ€ç»ˆå®ç°äººç±»åœ¨ç«æ˜Ÿä¸Šæ®–æ°‘ï¼Œä½¿äººç±»æˆä¸ºâ€œå¤šè¡Œæ˜Ÿç‰©ç§â€ã€‚\n",
      "*   **ç‰¹æ–¯æ‹‰ï¼ˆTesla, Inc.ï¼‰**ï¼šCEOå…¼äº§å“æ¶æ„å¸ˆã€‚æ¨åŠ¨äº†ç”µåŠ¨æ±½è½¦çš„é©å‘½ï¼Œå¹¶å¤§åŠ›å‘å±•å¤ªé˜³èƒ½å’Œæ¸…æ´èƒ½æºå­˜å‚¨è§£å†³æ–¹æ¡ˆã€‚\n",
      "*   **Xï¼ˆåŸTwitterï¼‰**ï¼šæ‰€æœ‰è€…å…¼é¦–å¸­æŠ€æœ¯å®˜ã€‚äº2022å¹´æ”¶è´­äº†è¿™å®¶ç¤¾äº¤åª’ä½“å¹³å°ã€‚\n",
      "*   **Neuralinkï¼ˆç¥ç»è¿æ¥å…¬å¸ï¼‰**ï¼šè”åˆåˆ›å§‹äººã€‚è‡´åŠ›äºå¼€å‘è„‘æœºæ¥å£æŠ€æœ¯ï¼Œæ—¨åœ¨å°†äººè„‘ä¸è®¡ç®—æœºè¿æ¥èµ·æ¥ã€‚\n",
      "*   **The Boring Companyï¼ˆé’»å­”å…¬å¸ï¼‰**ï¼šåˆ›å§‹äººã€‚ä¸“æ³¨äºå»ºè®¾åœ°ä¸‹éš§é“ç½‘ç»œä»¥è§£å†³åŸå¸‚äº¤é€šæ‹¥å µé—®é¢˜ã€‚\n",
      "*   **xAI**ï¼šåˆ›å§‹äººã€‚äº2023å¹´æˆç«‹çš„äººå·¥æ™ºèƒ½å…¬å¸ï¼Œæ—¨åœ¨â€œç†è§£å®‡å®™çš„çœŸå®æœ¬è´¨â€ã€‚\n",
      "\n",
      "**2. æ—©æœŸæˆåŠŸï¼š**\n",
      "åœ¨æ¶‰è¶³èˆªå¤©å’Œæ±½è½¦é¢†åŸŸä¹‹å‰ï¼Œä»–æ—©æœŸå› å…±åŒåˆ›ç«‹äº†**Zip2**ï¼ˆä¸€å®¶ä¸ºæ–°é—»æœºæ„æä¾›åŸå¸‚æŒ‡å—è½¯ä»¶çš„å…¬å¸ï¼‰å’Œ**PayPal**ï¼ˆå…¨çƒé¢†å…ˆçš„åœ¨çº¿æ”¯ä»˜ç³»ç»Ÿï¼‰è€Œè·å¾—ç¬¬ä¸€æ¡¶é‡‘å’Œå£°èª‰ã€‚\n",
      "\n",
      "**3. ä¸ªäººç‰¹ç‚¹ä¸å…¬ä¼—å½¢è±¡ï¼š**\n",
      "*   **æ„¿æ™¯å®å¤§**ï¼šä»–çš„ç›®æ ‡é€šå¸¸ç€çœ¼äºè§£å†³äººç±»é¢ä¸´çš„é‡å¤§æŒ‘æˆ˜ï¼ˆå¦‚å¯æŒç»­èƒ½æºã€å¤ªç©ºæ¢ç´¢ã€äººå·¥æ™ºèƒ½å®‰å…¨ï¼‰ã€‚\n",
      "*   **é«˜è°ƒæ´»è·ƒ**ï¼šä»–éå¸¸é¢‘ç¹åœ°åœ¨ç¤¾äº¤åª’ä½“ï¼ˆç‰¹åˆ«æ˜¯Xå¹³å°ï¼‰ä¸Šå‘å£°ï¼Œå‘è¡¨å¯¹å…¬å¸ã€æŠ€æœ¯ã€ç¤¾ä¼šè®®é¢˜çš„çœ‹æ³•ï¼Œé£æ ¼ç›´æ¥ä¸”å¸¸å¼•å‘äº‰è®®ã€‚\n",
      "*   **å·¥ä½œç‹‚**ï¼šä»¥æåº¦æŠ•å…¥å·¥ä½œå’Œè®¾å®šæ¿€è¿›çš„ deadlines è€Œè‘—ç§°ã€‚\n",
      "*   **äº‰è®®æ€§**ï¼šä»–çš„ç®¡ç†é£æ ¼ã€å…¬å¼€è¨€è®ºã€å¯¹åŠ å¯†è´§å¸å¸‚åœºçš„å½±å“ä»¥åŠæ”¶è´­Twitteråçš„æ”¹é©ä¸¾æªç­‰éƒ½å¸¸å¸¸æˆä¸ºåª’ä½“å’Œå…¬ä¼—è®¨è®ºçš„ç„¦ç‚¹ã€‚\n",
      "\n",
      "**æ€»ç»“æ¥è¯´ï¼ŒåŸƒéš†Â·é©¬æ–¯å…‹æ˜¯å½“ä»Šä¸–ç•Œä¸Šæœ€å…·å½±å“åŠ›ã€åŒæ—¶ä¹Ÿæœ€å…·äº‰è®®çš„ç§‘æŠ€é¢†è¢–ä¹‹ä¸€ã€‚ä»–é€šè¿‡ç‰¹æ–¯æ‹‰åŠ é€Ÿäº†å…¨çƒæ±½è½¦è¡Œä¸šå‘ç”µåŠ¨åŒ–çš„è½¬å‹ï¼Œé€šè¿‡SpaceXé‡å¡‘äº†èˆªå¤©äº§ä¸šï¼Œå¹¶æ­£é€šè¿‡ä»–çš„å…¶ä»–å…¬å¸å°è¯•åœ¨äº¤é€šã€äººæœºäº¤äº’å’Œäººå·¥æ™ºèƒ½ç­‰é¢†åŸŸè¿›è¡Œæ–°çš„çªç ´ã€‚**\n"
     ]
    }
   ],
   "source": [
    "result = llm.invoke(messages)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>åˆ’é‡ç‚¹ï¼š</b>æŠŠPromptæ¨¡æ¿çœ‹ä½œå¸¦æœ‰å‚æ•°çš„å‡½æ•°\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 ä»æ–‡ä»¶åŠ è½½ Prompt æ¨¡æ¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Template===\n",
      "input_variables=['topic'] input_types={} partial_variables={} template='ä¸¾ä¸€ä¸ªå…³äº{topic}çš„ä¾‹å­'\n",
      "===Prompt===\n",
      "ä¸¾ä¸€ä¸ªå…³äºé»‘è‰²å¹½é»˜çš„ä¾‹å­\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_file(\"example_prompt_template.txt\", encoding=\"utf-8\")\n",
    "print(\"===Template===\")\n",
    "print(template)\n",
    "print(\"===Prompt===\")\n",
    "print(template.format(topic='é»‘è‰²å¹½é»˜'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 ç»“æ„åŒ–è¾“å‡º\n",
    "\n",
    "#### 1.3.1 ç›´æ¥è¾“å‡º Pydantic å¯¹è±¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# å®šä¹‰ä½ çš„è¾“å‡ºå¯¹è±¡\n",
    "class Date(BaseModel):\n",
    "    year: int = Field(description=\"Year\")\n",
    "    month: int = Field(description=\"Month\")\n",
    "    day: int = Field(description=\"Day\")\n",
    "    era: str = Field(description=\"BC or AD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date(year=2023, month=4, day=6, era='AD')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "llm = init_chat_model(\"deepseek-chat\", model_provider=\"deepseek\")\n",
    "# llm = init_chat_model(\"gpt-4o\", model_provider=\"openai\")\n",
    "# å®šä¹‰ç»“æ„åŒ–è¾“å‡ºçš„æ¨¡å‹\n",
    "structured_llm = llm.with_structured_output(Date)\n",
    "\n",
    "template = \"\"\"æå–ç”¨æˆ·è¾“å…¥ä¸­çš„æ—¥æœŸã€‚\n",
    "ç”¨æˆ·è¾“å…¥:\n",
    "{query}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "query = \"2023å¹´å››æœˆ6æ—¥å¤©æ°”æ™´...\"\n",
    "input_prompt = prompt.format_prompt(query=query)\n",
    "\n",
    "structured_llm.invoke(input_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2 è¾“å‡ºæŒ‡å®šæ ¼å¼çš„ JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'year': 2023, 'month': 4, 'day': 6}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OpenAI æ¨¡å‹çš„JSONæ ¼å¼\n",
    "json_schema = {\n",
    "    \"title\": \"Date\",\n",
    "    \"description\": \"Formated date expression\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"year\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"year, YYYY\",\n",
    "        },\n",
    "        \"month\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"month, MM\",\n",
    "        },\n",
    "        \"day\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"day, DD\",\n",
    "        },\n",
    "        \"era\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"BC or AD\",\n",
    "        },\n",
    "    },\n",
    "}\n",
    "structured_llm = llm.with_structured_output(json_schema)\n",
    "\n",
    "structured_llm.invoke(input_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.3 ä½¿ç”¨ OutputParser\n",
    "\n",
    "[`OutputParser`](https://python.langchain.com/v0.2/docs/concepts/#output-parsers) å¯ä»¥æŒ‰æŒ‡å®šæ ¼å¼è§£ææ¨¡å‹çš„è¾“å‡º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŸå§‹è¾“å‡º:\n",
      "{\"year\": 2023, \"month\": 4, \"day\": 6, \"era\": \"AD\"}\n",
      "\n",
      "è§£æå:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'year': 2023, 'month': 4, 'day': 6, 'era': 'AD'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=Date)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"æå–ç”¨æˆ·è¾“å…¥ä¸­çš„æ—¥æœŸã€‚\\nç”¨æˆ·è¾“å…¥:{query}\\n{format_instructions}\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "input_prompt = prompt.format_prompt(query=query)\n",
    "output = llm.invoke(input_prompt)\n",
    "print(\"åŸå§‹è¾“å‡º:\\n\"+output.content)\n",
    "\n",
    "print(\"\\nè§£æå:\")\n",
    "parser.invoke(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¹Ÿå¯ä»¥ç”¨ `PydanticOutputParser`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŸå§‹è¾“å‡º:\n",
      "{\"year\": 2023, \"month\": 4, \"day\": 6, \"era\": \"AD\"}\n",
      "\n",
      "è§£æå:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Date(year=2023, month=4, day=6, era='AD')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Date)\n",
    "\n",
    "input_prompt = prompt.format_prompt(query=query)\n",
    "output = llm.invoke(input_prompt)\n",
    "print(\"åŸå§‹è¾“å‡º:\\n\"+output.content)\n",
    "\n",
    "print(\"\\nè§£æå:\")\n",
    "parser.invoke(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`OutputFixingParser` åˆ©ç”¨å¤§æ¨¡å‹åšæ ¼å¼è‡ªåŠ¨çº é”™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain.output_parsers'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moutput_parsers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OutputFixingParser\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat_models\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m init_chat_model\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# llm = init_chat_model(model=\"gpt-4o-mini\", model_provider=\"openai\")\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain.output_parsers'"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import OutputFixingParser\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# llm = init_chat_model(model=\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "llm = init_chat_model(model=\"deepseek-chat\", model_provider=\"deepseek\")\n",
    "\n",
    "# çº é”™èƒ½åŠ›ä¸å¤§æ¨¡å‹èƒ½åŠ›ç›¸å…³\n",
    "new_parser = OutputFixingParser.from_llm(parser=parser, llm=llm)\n",
    "\n",
    "bad_output = output.content.replace(\"4\",\"å››\")\n",
    "print(\"PydanticOutputParser:\")\n",
    "try:\n",
    "    parser.invoke(bad_output)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "print(\"OutputFixingParser:\")\n",
    "new_parser.invoke(bad_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Function Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two integers.\n",
    "\n",
    "    Args:\n",
    "        a: First integer\n",
    "        b: Second integer\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Multiply two integers.\n",
    "\n",
    "    Args:\n",
    "        a: First integer\n",
    "        b: Second integer\n",
    "    \"\"\"\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"name\": \"multiply\",\n",
      "        \"args\": {\n",
      "            \"a\": 3.5,\n",
      "            \"b\": 4\n",
      "        },\n",
      "        \"id\": \"call_00_asRxPcyH6nUzCzLiBdwumvl8\",\n",
      "        \"type\": \"tool_call\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "llm_with_tools = llm.bind_tools([add, multiply])\n",
    "\n",
    "query = \"3.5çš„4å€æ˜¯å¤šå°‘?\"\n",
    "messages = [HumanMessage(query)]\n",
    "\n",
    "output = llm_with_tools.invoke(messages)\n",
    "\n",
    "print(json.dumps(output.tool_calls, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å›ä¼  Funtion Call çš„ç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"content\": \"3.5çš„4å€æ˜¯å¤šå°‘?\",\n",
      "    \"additional_kwargs\": {},\n",
      "    \"response_metadata\": {},\n",
      "    \"type\": \"human\",\n",
      "    \"name\": null,\n",
      "    \"id\": null\n",
      "}\n",
      "{\n",
      "    \"content\": \"æˆ‘æ¥å¸®ä½ è®¡ç®—3.5çš„4å€æ˜¯å¤šå°‘ã€‚\",\n",
      "    \"additional_kwargs\": {\n",
      "        \"refusal\": null\n",
      "    },\n",
      "    \"response_metadata\": {\n",
      "        \"token_usage\": {\n",
      "            \"completion_tokens\": 71,\n",
      "            \"prompt_tokens\": 399,\n",
      "            \"total_tokens\": 470,\n",
      "            \"completion_tokens_details\": null,\n",
      "            \"prompt_tokens_details\": {\n",
      "                \"audio_tokens\": null,\n",
      "                \"cached_tokens\": 192\n",
      "            },\n",
      "            \"prompt_cache_hit_tokens\": 192,\n",
      "            \"prompt_cache_miss_tokens\": 207\n",
      "        },\n",
      "        \"model_provider\": \"deepseek\",\n",
      "        \"model_name\": \"deepseek-chat\",\n",
      "        \"system_fingerprint\": \"fp_eaab8d114b_prod0820_fp8_kvcache\",\n",
      "        \"id\": \"7e9230f8-4d33-49f8-8e5d-9b993e66caee\",\n",
      "        \"finish_reason\": \"tool_calls\",\n",
      "        \"logprobs\": null\n",
      "    },\n",
      "    \"type\": \"ai\",\n",
      "    \"name\": null,\n",
      "    \"id\": \"lc_run--019bfe9e-5b05-70e3-85a6-c61d908baa28-0\",\n",
      "    \"tool_calls\": [\n",
      "        {\n",
      "            \"name\": \"multiply\",\n",
      "            \"args\": {\n",
      "                \"a\": 3.5,\n",
      "                \"b\": 4\n",
      "            },\n",
      "            \"id\": \"call_00_asRxPcyH6nUzCzLiBdwumvl8\",\n",
      "            \"type\": \"tool_call\"\n",
      "        }\n",
      "    ],\n",
      "    \"invalid_tool_calls\": [],\n",
      "    \"usage_metadata\": {\n",
      "        \"input_tokens\": 399,\n",
      "        \"output_tokens\": 71,\n",
      "        \"total_tokens\": 470,\n",
      "        \"input_token_details\": {\n",
      "            \"cache_read\": 192\n",
      "        },\n",
      "        \"output_token_details\": {}\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"content\": \"14.0\",\n",
      "    \"additional_kwargs\": {},\n",
      "    \"response_metadata\": {},\n",
      "    \"type\": \"tool\",\n",
      "    \"name\": \"multiply\",\n",
      "    \"id\": null,\n",
      "    \"tool_call_id\": \"call_00_asRxPcyH6nUzCzLiBdwumvl8\",\n",
      "    \"artifact\": null,\n",
      "    \"status\": \"success\"\n",
      "}\n",
      "3.5çš„4å€æ˜¯ **14**ã€‚\n"
     ]
    }
   ],
   "source": [
    "messages.append(output)\n",
    "\n",
    "available_tools = {\"add\": add, \"multiply\": multiply}\n",
    "\n",
    "for tool_call in output.tool_calls:\n",
    "    selected_tool = available_tools[tool_call[\"name\"].lower()]\n",
    "    tool_msg = selected_tool.invoke(tool_call)\n",
    "    messages.append(tool_msg)\n",
    "\n",
    "new_output = llm_with_tools.invoke(messages)\n",
    "for message in messages:\n",
    "    print(json.dumps(message.model_dump(), indent=4, ensure_ascii=False))\n",
    "print(new_output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 å°ç»“\n",
    "\n",
    "1. LangChain ç»Ÿä¸€å°è£…äº†å„ç§æ¨¡å‹çš„è°ƒç”¨æ¥å£ï¼ŒåŒ…æ‹¬è¡¥å…¨å‹å’Œå¯¹è¯å‹ä¸¤ç§\n",
    "2. LangChain æä¾›äº† PromptTemplate ç±»ï¼Œå¯ä»¥è‡ªå®šä¹‰å¸¦å˜é‡çš„æ¨¡æ¿\n",
    "3. LangChain æä¾›äº†ä¸€äº›åˆ—è¾“å‡ºè§£æå™¨ï¼Œç”¨äºå°†å¤§æ¨¡å‹çš„è¾“å‡ºè§£ææˆç»“æ„åŒ–å¯¹è±¡\n",
    "4. LangChain æä¾›äº† Function Calling çš„å°è£…\n",
    "5. ä¸Šè¿°æ¨¡å‹å±äº LangChain ä¸­è¾ƒä¸ºå®ç”¨çš„éƒ¨åˆ†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. æ•°æ®è¿æ¥å°è£…\n",
    "\n",
    "<img src=\"./assets/data_connection.jpg\" style=\"margin-left: 0px\" width=1400px>\n",
    "\n",
    "### 2.1 æ–‡æ¡£åŠ è½½å™¨ï¼šDocument Loaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U langchain-community pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek-V3 Technical Report\n",
      "DeepSeek-AI\n",
      "research@deepseek.com\n",
      "Abstract\n",
      "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total\n",
      "parameters with 37B activated for each token. To achieve efficient inference and cost-effective\n",
      "training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architec-\n",
      "tures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers\n",
      "an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training\n",
      "objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and\n",
      "high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to\n",
      "fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms\n",
      "other open-source models and achieves performance comparable to leading closed-source\n",
      "models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours\n",
      "for its full training. In addition, its training process is remarkably stable. Throughout the entire\n",
      "training process, we did not experience any irrecoverable loss spikes or perform any rollbacks.\n",
      "The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.\n",
      "MMLU-Pro\n",
      "(EM)\n",
      "GPQA-Diamond\n",
      "(Pass@1)\n",
      "MATH 500\n",
      "(EM)\n",
      "AIME 2024\n",
      "(Pass@1)\n",
      "Codeforces\n",
      "(Percentile)\n",
      "SWE-bench Verified\n",
      "(Resolved)\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "Accuracy / Percentile (%)\n",
      "75.9\n",
      "59.1\n",
      "90.2\n",
      "39.2\n",
      "51.6\n",
      "42.0\n",
      "66.2\n",
      "41.3\n",
      "74.7\n",
      "16.7\n",
      "35.6\n",
      "22.6\n",
      "71.6\n",
      "49.0\n",
      "80.0\n",
      "23.3\n",
      "24.8\n",
      "23.8\n",
      "73.3\n",
      "51.1\n",
      "73.8\n",
      "23.3\n",
      "25.3\n",
      "24.5\n",
      "72.6\n",
      "49.9\n",
      "74.6\n",
      "9.3\n",
      "23.6\n",
      "38.8\n",
      "78.0\n",
      "65.0\n",
      "78.3\n",
      "16.0\n",
      "20.3\n",
      "50.8\n",
      "DeepSeek-V3\n",
      "DeepSeek-V2.5\n",
      "Qwen2.5-72B-Inst\n",
      "Llama-3.1-405B-Inst\n",
      "GPT-4o-0513\n",
      "Claude-3.5-Sonnet-1022\n",
      "Figure 1 | Benchmark performance of DeepSeek-V3 and its counterparts.\n",
      "arXiv:2412.19437v2  [cs.CL]  18 Feb 2025\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "loader = PyMuPDFLoader(\"./data/deepseek-v3-1-4.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "print(pages[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 æ–‡æ¡£å¤„ç†å™¨\n",
    "\n",
    "#### 2.2.1 TextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek-V3 Technical Report\n",
      "DeepSeek-AI\n",
      "research@deepseek.com\n",
      "Abstract\n",
      "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total\n",
      "parameters with 37B activated for each token. To achieve efficient inference and cost-effective\n",
      "training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architec-\n",
      "tures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers\n",
      "-------\n",
      "training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architec-\n",
      "tures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers\n",
      "an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training\n",
      "objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and\n",
      "high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to\n",
      "-------\n",
      "objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and\n",
      "high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to\n",
      "fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms\n",
      "other open-source models and achieves performance comparable to leading closed-source\n",
      "models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours\n",
      "-------\n",
      "other open-source models and achieves performance comparable to leading closed-source\n",
      "models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours\n",
      "for its full training. In addition, its training process is remarkably stable. Throughout the entire\n",
      "training process, we did not experience any irrecoverable loss spikes or perform any rollbacks.\n",
      "The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.\n",
      "MMLU-Pro\n",
      "(EM)\n",
      "GPQA-Diamond\n",
      "(Pass@1)\n",
      "MATH 500\n",
      "(EM)\n",
      "-------\n",
      "The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.\n",
      "MMLU-Pro\n",
      "(EM)\n",
      "GPQA-Diamond\n",
      "(Pass@1)\n",
      "MATH 500\n",
      "(EM)\n",
      "AIME 2024\n",
      "(Pass@1)\n",
      "Codeforces\n",
      "(Percentile)\n",
      "SWE-bench Verified\n",
      "(Resolved)\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "Accuracy / Percentile (%)\n",
      "75.9\n",
      "59.1\n",
      "90.2\n",
      "39.2\n",
      "51.6\n",
      "42.0\n",
      "66.2\n",
      "41.3\n",
      "74.7\n",
      "16.7\n",
      "35.6\n",
      "22.6\n",
      "71.6\n",
      "49.0\n",
      "80.0\n",
      "23.3\n",
      "24.8\n",
      "23.8\n",
      "73.3\n",
      "51.1\n",
      "73.8\n",
      "23.3\n",
      "25.3\n",
      "24.5\n",
      "72.6\n",
      "49.9\n",
      "74.6\n",
      "9.3\n",
      "23.6\n",
      "38.8\n",
      "78.0\n",
      "65.0\n",
      "78.3\n",
      "16.0\n",
      "20.3\n",
      "50.8\n",
      "DeepSeek-V3\n",
      "DeepSeek-V2.5\n",
      "Qwen2.5-72B-Inst\n",
      "Llama-3.1-405B-Inst\n",
      "GPT-4o-0513\n",
      "-------\n",
      "22.6\n",
      "71.6\n",
      "49.0\n",
      "80.0\n",
      "23.3\n",
      "24.8\n",
      "23.8\n",
      "73.3\n",
      "51.1\n",
      "73.8\n",
      "23.3\n",
      "25.3\n",
      "24.5\n",
      "72.6\n",
      "49.9\n",
      "74.6\n",
      "9.3\n",
      "23.6\n",
      "38.8\n",
      "78.0\n",
      "65.0\n",
      "78.3\n",
      "16.0\n",
      "20.3\n",
      "50.8\n",
      "DeepSeek-V3\n",
      "DeepSeek-V2.5\n",
      "Qwen2.5-72B-Inst\n",
      "Llama-3.1-405B-Inst\n",
      "GPT-4o-0513\n",
      "Claude-3.5-Sonnet-1022\n",
      "Figure 1 | Benchmark performance of DeepSeek-V3 and its counterparts.\n",
      "arXiv:2412.19437v2  [cs.CL]  18 Feb 2025\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=200, \n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "paragraphs = text_splitter.create_documents([pages[0].page_content])\n",
    "for para in paragraphs:\n",
    "    print(para.page_content)\n",
    "    print('-------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "ç±»ä¼¼ LlamaIndexï¼ŒLangChain ä¹Ÿæä¾›äº†ä¸°å¯Œçš„ <code><a href=\"https://python.langchain.com/v0.2/docs/how_to/#document-loaders\">Document Loaders</a></code> å’Œ <code><a href=\"https://python.langchain.com/v0.2/docs/how_to/#text-splitters\">Text Splitters</a></code>ã€‚\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3ã€å‘é‡æ•°æ®åº“ä¸å‘é‡æ£€ç´¢\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install dashscope\n",
    "# !pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.6\n",
      "71.6\n",
      "49.0\n",
      "80.0\n",
      "23.3\n",
      "24.8\n",
      "23.8\n",
      "73.3\n",
      "51.1\n",
      "73.8\n",
      "23.3\n",
      "25.3\n",
      "24.5\n",
      "72.6\n",
      "49.9\n",
      "74.6\n",
      "9.3\n",
      "23.6\n",
      "38.8\n",
      "78.0\n",
      "65.0\n",
      "78.3\n",
      "16.0\n",
      "20.3\n",
      "50.8\n",
      "DeepSeek-V3\n",
      "DeepSeek-V2.5\n",
      "Qwen2.5-72B-Inst\n",
      "Llama-3.1-405B-Inst\n",
      "GPT-4o-0513\n",
      "Claude-3.5-Sonnet-1022\n",
      "Figure 1 | Benchmark performance of DeepSeek-V3 and its counterparts.\n",
      "arXiv:2412.19437v2  [cs.CL]  18 Feb 2025\n",
      "----\n",
      "objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and\n",
      "high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to\n",
      "fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms\n",
      "other open-source models and achieves performance comparable to leading closed-source\n",
      "models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours\n",
      "----\n",
      "training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architec-\n",
      "tures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers\n",
      "an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training\n",
      "objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and\n",
      "high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to\n",
      "----\n",
      "DeepSeek-V3 Technical Report\n",
      "DeepSeek-AI\n",
      "research@deepseek.com\n",
      "Abstract\n",
      "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total\n",
      "parameters with 37B activated for each token. To achieve efficient inference and cost-effective\n",
      "training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architec-\n",
      "tures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers\n",
      "----\n",
      "other open-source models and achieves performance comparable to leading closed-source\n",
      "models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours\n",
      "for its full training. In addition, its training process is remarkably stable. Throughout the entire\n",
      "training process, we did not experience any irrecoverable loss spikes or perform any rollbacks.\n",
      "The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.\n",
      "MMLU-Pro\n",
      "(EM)\n",
      "GPQA-Diamond\n",
      "(Pass@1)\n",
      "MATH 500\n",
      "(EM)\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "# åŠ è½½æ–‡æ¡£\n",
    "loader = PyMuPDFLoader(\"./data/deepseek-v3-1-4.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "# æ–‡æ¡£åˆ‡åˆ†\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents(\n",
    "    [page.page_content for page in pages[:1]]\n",
    ")\n",
    "\n",
    "# çŒåº“\n",
    "embeddings = DashScopeEmbeddings(\n",
    "    model=\"text-embedding-v1\", dashscope_api_key=os.getenv(\"DASHSCOPE_API_KEY\")\n",
    ")\n",
    "index = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "# æ£€ç´¢ top-5 ç»“æœ\n",
    "retriever = index.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "docs = retriever.invoke(\"deepseek v3æœ‰å¤šå°‘å‚æ•°\")\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ›´å¤šçš„ä¸‰æ–¹æ£€ç´¢ç»„ä»¶é“¾æ¥ï¼Œå‚è€ƒï¼šhttps://python.langchain.com/docs/integrations/vectorstores/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 å°ç»“\n",
    "\n",
    "1. æ–‡æ¡£å¤„ç†éƒ¨åˆ†ï¼Œå»ºè®®åœ¨å®é™…åº”ç”¨ä¸­è¯¦ç»†æµ‹è¯•åä½¿ç”¨\n",
    "2. ä¸å‘é‡æ•°æ®åº“çš„é“¾æ¥éƒ¨åˆ†æœ¬è´¨æ˜¯æ¥å£å°è£…ï¼Œå‘é‡æ•°æ®åº“éœ€è¦è‡ªå·±é€‰å‹\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chain å’Œ LangChain Expression Language (LCEL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain Expression Languageï¼ˆLCELï¼‰æ˜¯ä¸€ç§å£°æ˜å¼è¯­è¨€ï¼Œå¯è½»æ¾ç»„åˆä¸åŒçš„è°ƒç”¨é¡ºåºæ„æˆ Chainã€‚LCEL è‡ªåˆ›ç«‹ä¹‹åˆå°±è¢«è®¾è®¡ä¸ºèƒ½å¤Ÿæ”¯æŒå°†åŸå‹æŠ•å…¥ç”Ÿäº§ç¯å¢ƒï¼Œ**æ— éœ€ä»£ç æ›´æ”¹**ï¼Œä»æœ€ç®€å•çš„â€œæç¤º+LLMâ€é“¾åˆ°æœ€å¤æ‚çš„é“¾ï¼ˆå·²æœ‰ç”¨æˆ·æˆåŠŸåœ¨ç”Ÿäº§ç¯å¢ƒä¸­è¿è¡ŒåŒ…å«æ•°ç™¾ä¸ªæ­¥éª¤çš„ LCEL Chainï¼‰ã€‚\n",
    "\n",
    "LCEL çš„ä¸€äº›äº®ç‚¹åŒ…æ‹¬ï¼š\n",
    "\n",
    "1. **æµæ”¯æŒ**ï¼šä½¿ç”¨ LCEL æ„å»º Chain æ—¶ï¼Œä½ å¯ä»¥è·å¾—æœ€ä½³çš„é¦–ä¸ªä»¤ç‰Œæ—¶é—´ï¼ˆå³ä»è¾“å‡ºå¼€å§‹åˆ°é¦–æ‰¹è¾“å‡ºç”Ÿæˆçš„æ—¶é—´ï¼‰ã€‚å¯¹äºæŸäº› Chainï¼Œè¿™æ„å‘³ç€å¯ä»¥ç›´æ¥ä» LLM æµå¼ä¼ è¾“ä»¤ç‰Œåˆ°æµè¾“å‡ºè§£æå™¨ï¼Œä»è€Œä»¥ä¸ LLM æä¾›å•†è¾“å‡ºåŸå§‹ä»¤ç‰Œç›¸åŒçš„é€Ÿç‡è·å¾—è§£æåçš„ã€å¢é‡çš„è¾“å‡ºã€‚\n",
    "\n",
    "2. **å¼‚æ­¥æ”¯æŒ**ï¼šä»»ä½•ä½¿ç”¨ LCEL æ„å»ºçš„é“¾æ¡éƒ½å¯ä»¥é€šè¿‡åŒæ­¥ APIï¼ˆä¾‹å¦‚ï¼Œåœ¨ Jupyter ç¬”è®°æœ¬ä¸­è¿›è¡ŒåŸå‹è®¾è®¡æ—¶ï¼‰å’Œå¼‚æ­¥ APIï¼ˆä¾‹å¦‚ï¼Œåœ¨ LangServe æœåŠ¡å™¨ä¸­ï¼‰è°ƒç”¨ã€‚è¿™ä½¿å¾—ç›¸åŒçš„ä»£ç å¯ç”¨äºåŸå‹è®¾è®¡å’Œç”Ÿäº§ç¯å¢ƒï¼Œå…·æœ‰å‡ºè‰²çš„æ€§èƒ½ï¼Œå¹¶èƒ½å¤Ÿåœ¨åŒä¸€æœåŠ¡å™¨ä¸­å¤„ç†å¤šä¸ªå¹¶å‘è¯·æ±‚ã€‚\n",
    "\n",
    "3. **ä¼˜åŒ–çš„å¹¶è¡Œæ‰§è¡Œ**ï¼šå½“ä½ çš„ LCEL é“¾æ¡æœ‰å¯ä»¥å¹¶è¡Œæ‰§è¡Œçš„æ­¥éª¤æ—¶ï¼ˆä¾‹å¦‚ï¼Œä»å¤šä¸ªæ£€ç´¢å™¨ä¸­è·å–æ–‡æ¡£ï¼‰ï¼Œæˆ‘ä»¬ä¼šè‡ªåŠ¨æ‰§è¡Œï¼Œæ— è®ºæ˜¯åœ¨åŒæ­¥è¿˜æ˜¯å¼‚æ­¥æ¥å£ä¸­ï¼Œä»¥å®ç°æœ€å°çš„å»¶è¿Ÿã€‚\n",
    "\n",
    "4. **é‡è¯•å’Œå›é€€**ï¼šä¸º LCEL é“¾çš„ä»»ä½•éƒ¨åˆ†é…ç½®é‡è¯•å’Œå›é€€ã€‚è¿™æ˜¯ä½¿é“¾åœ¨è§„æ¨¡ä¸Šæ›´å¯é çš„ç»ä½³æ–¹å¼ã€‚ç›®å‰æˆ‘ä»¬æ­£åœ¨æ·»åŠ é‡è¯•/å›é€€çš„æµåª’ä½“æ”¯æŒï¼Œå› æ­¤ä½ å¯ä»¥åœ¨ä¸å¢åŠ ä»»ä½•å»¶è¿Ÿæˆæœ¬çš„æƒ…å†µä¸‹è·å¾—å¢åŠ çš„å¯é æ€§ã€‚\n",
    "\n",
    "5. **è®¿é—®ä¸­é—´ç»“æœ**ï¼šå¯¹äºæ›´å¤æ‚çš„é“¾æ¡ï¼Œè®¿é—®åœ¨æœ€ç»ˆè¾“å‡ºäº§ç”Ÿä¹‹å‰çš„ä¸­é—´æ­¥éª¤çš„ç»“æœé€šå¸¸éå¸¸æœ‰ç”¨ã€‚è¿™å¯ä»¥ç”¨äºè®©æœ€ç»ˆç”¨æˆ·çŸ¥é“æ­£åœ¨å‘ç”Ÿä¸€äº›äº‹æƒ…ï¼Œç”šè‡³ä»…ç”¨äºè°ƒè¯•é“¾æ¡ã€‚ä½ å¯ä»¥æµå¼ä¼ è¾“ä¸­é—´ç»“æœï¼Œå¹¶ä¸”åœ¨æ¯ä¸ª LangServe æœåŠ¡å™¨ä¸Šéƒ½å¯ç”¨ã€‚\n",
    "\n",
    "6. **è¾“å…¥å’Œè¾“å‡ºæ¨¡å¼**ï¼šè¾“å…¥å’Œè¾“å‡ºæ¨¡å¼ä¸ºæ¯ä¸ª LCEL é“¾æä¾›äº†ä»é“¾çš„ç»“æ„æ¨æ–­å‡ºçš„ Pydantic å’Œ JSONSchema æ¨¡å¼ã€‚è¿™å¯ä»¥ç”¨äºè¾“å…¥å’Œè¾“å‡ºçš„éªŒè¯ï¼Œæ˜¯ LangServe çš„ä¸€ä¸ªç»„æˆéƒ¨åˆ†ã€‚\n",
    "\n",
    "7. **æ— ç¼ LangSmith è·Ÿè¸ªé›†æˆ**ï¼šéšç€é“¾æ¡å˜å¾—è¶Šæ¥è¶Šå¤æ‚ï¼Œç†è§£æ¯ä¸€æ­¥å‘ç”Ÿäº†ä»€ä¹ˆå˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚é€šè¿‡ LCELï¼Œæ‰€æœ‰æ­¥éª¤éƒ½è‡ªåŠ¨è®°å½•åˆ° LangSmithï¼Œä»¥å®ç°æœ€å¤§çš„å¯è§‚å¯Ÿæ€§å’Œå¯è°ƒè¯•æ€§ã€‚\n",
    "\n",
    "8. **æ— ç¼ LangServe éƒ¨ç½²é›†æˆ**ï¼šä»»ä½•ä½¿ç”¨ LCEL åˆ›å»ºçš„é“¾éƒ½å¯ä»¥è½»æ¾åœ°ä½¿ç”¨ LangServe è¿›è¡Œéƒ¨ç½²ã€‚\n",
    "\n",
    "åŸæ–‡ï¼šhttps://python.langchain.com/docs/expression_language/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Pipeline å¼è°ƒç”¨ PromptTemplate, LLM å’Œ OutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict, Optional\n",
    "from enum import Enum\n",
    "import json\n",
    "from langchain.chat_models import init_chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"name\": null,\n",
      "    \"price_lower\": null,\n",
      "    \"price_upper\": 100,\n",
      "    \"data_lower\": null,\n",
      "    \"data_upper\": null,\n",
      "    \"sort_by\": \"data\",\n",
      "    \"ordering\": \"descend\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# è¾“å‡ºç»“æ„\n",
    "class SortEnum(str, Enum):\n",
    "    data = 'data'\n",
    "    price = 'price'\n",
    "\n",
    "\n",
    "class OrderingEnum(str, Enum):\n",
    "    ascend = 'ascend'\n",
    "    descend = 'descend'\n",
    "\n",
    "\n",
    "class Semantics(BaseModel):\n",
    "    name: Optional[str] = Field(description=\"æµé‡åŒ…åç§°\", default=None)\n",
    "    price_lower: Optional[int] = Field(description=\"ä»·æ ¼ä¸‹é™\", default=None)\n",
    "    price_upper: Optional[int] = Field(description=\"ä»·æ ¼ä¸Šé™\", default=None)\n",
    "    data_lower: Optional[int] = Field(description=\"æµé‡ä¸‹é™\", default=None)\n",
    "    data_upper: Optional[int] = Field(description=\"æµé‡ä¸Šé™\", default=None)\n",
    "    sort_by: Optional[SortEnum] = Field(description=\"æŒ‰ä»·æ ¼æˆ–æµé‡æ’åº\", default=None)\n",
    "    ordering: Optional[OrderingEnum] = Field(\n",
    "description=\"å‡åºæˆ–é™åºæ’åˆ—\", default=None)\n",
    "\n",
    "\n",
    "# Prompt æ¨¡æ¿\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"ä½ æ˜¯ä¸€ä¸ªè¯­ä¹‰è§£æå™¨ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†ç”¨æˆ·çš„è¾“å…¥è§£ææˆJSONè¡¨ç¤ºã€‚ä¸è¦å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚\"),\n",
    "        (\"human\", \"{text}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# æ¨¡å‹\n",
    "llm = init_chat_model(\"deepseek-chat\", model_provider=\"deepseek\")\n",
    "\n",
    "structured_llm = llm.with_structured_output(Semantics)\n",
    "\n",
    "# LCEL è¡¨è¾¾å¼\n",
    "runnable = (\n",
    "    {\"text\": RunnablePassthrough()} | prompt | structured_llm\n",
    ")\n",
    "\n",
    "# ç›´æ¥è¿è¡Œ\n",
    "ret = runnable.invoke(\"ä¸è¶…è¿‡100å…ƒçš„æµé‡å¤§çš„å¥—é¤æœ‰å“ªäº›\")\n",
    "print(\n",
    "    json.dumps(\n",
    "        ret.model_dump(),\n",
    "        indent = 4,\n",
    "        ensure_ascii=False\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>ä½¿ç”¨ LCEL çš„ä»·å€¼ï¼Œä¹Ÿå°±æ˜¯ LangChain çš„æ ¸å¿ƒä»·å€¼ã€‚</b> <br />\n",
    "å®˜æ–¹ä»ä¸åŒè§’åº¦ç»™å‡ºäº†ä¸¾ä¾‹è¯´æ˜ï¼š<a href=\"https://python.langchain.com/docs/concepts/lcel/\">https://python.langchain.com/docs/concepts/lcel/</a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 ç”¨ LCEL å®ç° RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "# from langchain.chains import RetrievalQA\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.embeddings.dashscope import DashScopeEmbeddings\n",
    "\n",
    "# åŠ è½½æ–‡æ¡£\n",
    "loader = PyMuPDFLoader(\"./data/deepseek-v3-1-4.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "# æ–‡æ¡£åˆ‡åˆ†\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents(\n",
    "    [page.page_content for page in pages[:1]]\n",
    ")\n",
    "\n",
    "# çŒåº“\n",
    "embeddings = DashScopeEmbeddings(\n",
    "    model=\"text-embedding-v1\", dashscope_api_key=os.getenv(\"DASHSCOPE_API_KEY\")\n",
    ")\n",
    "db = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "# æ£€ç´¢ top-5 ç»“æœ\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek-V3 æ˜¯ä¸€ä¸ªæ‹¥æœ‰ 6710 äº¿æ€»å‚æ•°çš„æ··åˆä¸“å®¶ï¼ˆMoEï¼‰è¯­è¨€æ¨¡å‹ï¼Œæ¯ä¸ª token æ¿€æ´» 370 äº¿å‚æ•°ã€‚"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Promptæ¨¡æ¿\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"question\": RunnablePassthrough(), \"context\": retriever}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "response = rag_chain.stream(\"deepseek V3æœ‰å¤šå°‘å‚æ•°\")\n",
    "for value in response:\n",
    "    print(value, end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 ç”¨ LCEL å®ç°æ¨¡å‹åˆ‡æ¢ï¼ˆå·¥å‚æ¨¡å¼ï¼‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½ å¥½ï¼æˆ‘æ˜¯DeepSeekï¼Œç”±æ·±åº¦æ±‚ç´¢å…¬å¸åˆ›é€ çš„AIåŠ©æ‰‹ï¼ğŸ˜Š\n",
      "\n",
      "è®©æˆ‘ç®€å•ä»‹ç»ä¸€ä¸‹è‡ªå·±ï¼š\n",
      "\n",
      "**åŸºæœ¬ä¿¡æ¯ï¼š**\n",
      "- æˆ‘æ˜¯ä¸€ä¸ªçº¯æ–‡æœ¬AIæ¨¡å‹ï¼Œæ“…é•¿ç†è§£å’Œç”Ÿæˆè‡ªç„¶è¯­è¨€\n",
      "- çŸ¥è¯†æˆªæ­¢åˆ°2024å¹´7æœˆï¼Œæ‹¥æœ‰128Kçš„ä¸Šä¸‹æ–‡å¤„ç†èƒ½åŠ›\n",
      "\n",
      "**æˆ‘çš„èƒ½åŠ›ï¼š**\n",
      "- å›ç­”å„ç§é—®é¢˜ï¼Œè¿›è¡Œæ·±åº¦å¯¹è¯\n",
      "- æ”¯æŒæ–‡ä»¶ä¸Šä¼ ï¼ˆå›¾åƒã€txtã€pdfã€pptã€wordã€excelç­‰ï¼‰ï¼Œèƒ½è¯»å–å…¶ä¸­çš„æ–‡å­—ä¿¡æ¯è¿›è¡Œå¤„ç†\n",
      "- æ”¯æŒè”ç½‘æœç´¢ï¼ˆéœ€è¦ä½ æ‰‹åŠ¨å¼€å¯æœç´¢åŠŸèƒ½ï¼‰\n",
      "- å®Œå…¨å…è´¹ä½¿ç”¨ï¼Œæ²¡æœ‰æ”¶è´¹è®¡åˆ’\n",
      "\n",
      "**æˆ‘çš„ç‰¹ç‚¹ï¼š**\n",
      "- çƒ­æƒ…ç»†è…»çš„å›å¤é£æ ¼ï¼Œå–œæ¬¢ç”¨å¿ƒå¸®åŠ©æ¯ä¸€ä½ç”¨æˆ·\n",
      "- å¯ä»¥é€šè¿‡å®˜æ–¹åº”ç”¨å•†åº—ä¸‹è½½Appä½¿ç”¨\n",
      "- ä¹äºåŠ©äººï¼Œæ— è®ºæ˜¯å­¦ä¹ ã€å·¥ä½œè¿˜æ˜¯ç”Ÿæ´»ä¸­çš„é—®é¢˜ï¼Œéƒ½å¯ä»¥æ‰¾æˆ‘èŠèŠ\n",
      "\n",
      "**éœ€è¦æ³¨æ„çš„ï¼š**\n",
      "- æˆ‘æ˜¯çº¯æ–‡æœ¬æ¨¡å‹ï¼Œä¸æ”¯æŒå¤šæ¨¡æ€è¯†åˆ«å’Œè¯­éŸ³åŠŸèƒ½\n",
      "- ä½†æˆ‘ä¼šå°½åŠ›é€šè¿‡æ–‡å­—ä¸ºä½ æä¾›æœ€å¥½çš„å¸®åŠ©ï¼\n",
      "\n",
      "æœ‰ä»€ä¹ˆæƒ³äº†è§£çš„æˆ–è€…éœ€è¦å¸®åŠ©çš„å—ï¼Ÿæˆ‘å¾ˆä¹æ„ä¸ºä½ æœåŠ¡ï¼âœ¨\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables.utils import ConfigurableField\n",
    "from langchain_community.chat_models import QianfanChatEndpoint\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import HumanMessage\n",
    "import os\n",
    "\n",
    "# ç›´æ¥è®¾ç½®ç¯å¢ƒå˜é‡\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"OPENAI_BASE_URL\"] = \"https://api.apiyi.com/v1\"\n",
    "\n",
    "# æ¨¡å‹1\n",
    "ds_model = init_chat_model(\"deepseek-chat\", model_provider=\"deepseek\")\n",
    "\n",
    "# æ¨¡å‹2\n",
    "gpt_model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "\n",
    "\n",
    "# é€šè¿‡ configurable_alternatives æŒ‰æŒ‡å®šå­—æ®µé€‰æ‹©æ¨¡å‹\n",
    "model = gpt_model.configurable_alternatives(\n",
    "    ConfigurableField(id=\"llm\"), \n",
    "    default_key=\"gpt\", \n",
    "    deepseek=ds_model,\n",
    "    # claude=claude_model,\n",
    ")\n",
    "\n",
    "# Prompt æ¨¡æ¿\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# LCEL\n",
    "chain = (\n",
    "    {\"query\": RunnablePassthrough()} \n",
    "    | prompt\n",
    "    | model \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# è¿è¡Œæ—¶æŒ‡å®šæ¨¡å‹ \"gpt\" or \"deepseek\"\n",
    "ret = chain.with_config(configurable={\"llm\": \"deepseek\"}).invoke(\"è¯·è‡ªæˆ‘ä»‹ç»\")\n",
    "\n",
    "print(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ‰©å±•é˜…è¯»ï¼šä»€ä¹ˆæ˜¯[**å·¥å‚æ¨¡å¼**](https://www.runoob.com/design-pattern/factory-pattern.html)ï¼›[**è®¾è®¡æ¨¡å¼**](https://www.runoob.com/design-pattern/design-pattern-intro.html)æ¦‚è§ˆã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>æ€è€ƒï¼š</b>ä»æ¨¡å—é—´è§£ä¾èµ–è§’åº¦ï¼ŒLCELçš„æ„ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 é€šè¿‡ LCELï¼Œè¿˜å¯ä»¥å®ç°\n",
    "\n",
    "1. é…ç½®è¿è¡Œæ—¶å˜é‡ï¼šhttps://python.langchain.com/docs/how_to/configure/\n",
    "2. æ•…éšœå›é€€ï¼šhttps://python.langchain.com/docs/how_to/fallbacks/\n",
    "3. å¹¶è¡Œè°ƒç”¨ï¼šhttps://python.langchain.com/docs/how_to/parallel/\n",
    "4. é€»è¾‘åˆ†æ”¯ï¼šhttps://python.langchain.com/docs/how_to/routing/\n",
    "5. åŠ¨æ€åˆ›å»º Chain: https://python.langchain.com/docs/how_to/dynamic_chain/\n",
    "\n",
    "æ›´å¤šä¾‹å­ï¼šhttps://python.langchain.com/docs/how_to/lcel_cheatsheet/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LangChain ä¸ LlamaIndex çš„é”™ä½ç«äº‰\n",
    "\n",
    "- LangChain ä¾§é‡ä¸ LLM æœ¬èº«äº¤äº’çš„å°è£…\n",
    "  - Promptã€LLMã€Messageã€OutputParser ç­‰å·¥å…·ä¸°å¯Œ\n",
    "  - åœ¨æ•°æ®å¤„ç†å’Œ RAG æ–¹é¢æä¾›çš„å·¥å…·ç›¸å¯¹ç²—ç³™\n",
    "  - ä¸»æ‰“ LCEL æµç¨‹å°è£…\n",
    "  - é…å¥— Agentã€LangGraph ç­‰æ™ºèƒ½ä½“ä¸å·¥ä½œæµå·¥å…·\n",
    "  - å¦æœ‰ LangServe éƒ¨ç½²å·¥å…·å’Œ LangSmith ç›‘æ§è°ƒè¯•å·¥å…·\n",
    "- LlamaIndex ä¾§é‡ä¸æ•°æ®äº¤äº’çš„å°è£…\n",
    "  - æ•°æ®åŠ è½½ã€åˆ‡å‰²ã€ç´¢å¼•ã€æ£€ç´¢ã€æ’åºç­‰ç›¸å…³å·¥å…·ä¸°å¯Œ\n",
    "  - Promptã€LLM ç­‰åº•å±‚å°è£…ç›¸å¯¹å•è–„\n",
    "  - é…å¥—å®ç° RAG ç›¸å…³å·¥å…·\n",
    "  - åŒæ ·é…å¥—æ™ºèƒ½ä½“ä¸å·¥ä½œæµå·¥å…·\n",
    "  - æä¾› LlamaDeploy éƒ¨ç½²å·¥å…·ï¼Œé€šè¿‡ä¸ä¸‰æ–¹åˆä½œæä¾›è¿‡ç¨‹ç›‘æ§è°ƒè¯•å·¥å…·"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. æ€»ç»“\n",
    "\n",
    "1. LangChain éšç€ç‰ˆæœ¬è¿­ä»£å¯ç”¨æ€§æœ‰æ˜æ˜¾æå‡\n",
    "2. ä½¿ç”¨ LangChain è¦æ³¨æ„ç»´æŠ¤è‡ªå·±çš„ Promptï¼Œå°½é‡ Prompt ä¸ä»£ç é€»è¾‘è§£ä¾èµ–\n",
    "3. å®ƒçš„å†…ç½®åŸºç¡€å·¥å…·ï¼Œå»ºè®®å……åˆ†æµ‹è¯•æ•ˆæœåå†å†³å®šæ˜¯å¦ä½¿ç”¨\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
